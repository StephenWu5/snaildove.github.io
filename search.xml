<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[同步hexo博客]]></title>
    <url>%2F2018%2F02%2F10%2Fhexo_source-code_synchronization%2F</url>
    <content type="text"><![CDATA[旧电脑上博客根目录执行12345678git initgit check -b sourcegit check sourcegit add *git rm --cached ./node_modules/ ./source/_drafts ._config.yml ./themes/next/_config.ymlvim .gitignore #添加：./node_modules/ ./source/_drafts ._config.yml （博客根目录配置文件，防止泄露隐私）./themes/next/_config.yml (主题配置文件，防止泄露隐私，百度云盘隐藏空间备份)git commit -m "初次同步博客"git push origin source:source 新电脑上博客的新目录下执行在github的 ~.github.io.git（~代表你的github用户名） 仓库上设置 source 分支为默认分支 git 配置完成以后12345git clone https://github.com/~/~.github.io.gitcd ~.github.io.gitnpm install hexonpm installnpm install hexo-deployer-git latex公式问题按网上的教程继续修改 ./node_modules/marked/marked.js 以支持数学公式中的 \ -]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My summary of neural-networks-deep-learning]]></title>
    <url>%2F2018%2F02%2F08%2Fneural-networks-deep-learning%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal summary after studying the course neural-networks-deep-learning, which belongs to Deep Learning Specialization. and the copyright belongs to deeplearning.ai. My personal notesweek1: introduction-to-deep-learningweek2: neural-networks-basicsweek3: shallow-neural-networksweek4: deep-neural-networks My personal programming assignmentsweek 1 and week 2: logistic-regression-with-a-neural-network-mindsetweek 3: Planar data classification with a hidden layerweek 4 part 1: Building your deep neural network: Step by Stepweek 4 part 2: deep-neural-network-application]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>neural-networks-deep-learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Building your Deep Neural Network]]></title>
    <url>%2F2018%2F02%2F07%2FBuilding-your-Deep-Neural-Network_week4%2F</url>
    <content type="text"><![CDATA[NoteThese are my personal programming assignments at the 4th week after studying the course neural-networks-deep-learning and the copyright belongs to deeplearning.ai. Part 1：Building your Deep Neural Network: Step by Step1. PackagesLet’s first import all the packages that you will need during this assignment. numpy is the main package for scientific computing with Python. matplotlib is a library to plot graphs in Python. dnn_utils provides some necessary functions for this notebook. testCases provides some test cases to assess the correctness of your functions np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. Please don’t change the seed. 123456789101112131415import numpy as np;import h5py;import matplotlib.pyplot as plt;from testCases_v3 import *;from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward;%matplotlib inlineplt.rcParams['figure.figsize'] = (5.0, 4.0); # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest';plt.rcParams['image.cmap'] = 'gray';%load_ext autoreload%autoreload 2np.random.seed(1); You can get the support code from here. the sigmoid function: 12345678910111213141516def sigmoid(Z): """ Implements the sigmoid activation in numpy Arguments: Z -- numpy array of any shape Returns: A -- output of sigmoid(z), same shape as Z cache -- returns Z as well, useful during backpropagation """ A = 1 / (1 + np.exp(-Z)); cache = Z; return A, cache; the sigmoid_backward function: 1234567891011121314151617181920def sigmoid_backward(dA, cache): """ Implement the backward propagation for a single SIGMOID unit. Arguments: dA -- post-activation gradient, of any shape cache -- 'Z' where we store for computing backward propagation efficiently Returns: dZ -- Gradient of the cost with respect to Z """ Z = cache; s = 1 / (1 + np.exp(-Z)); dZ = dA * s * (1 - s); assert (dZ.shape == Z.shape); return dZ; the relu function: 123456789101112131415161718def relu(Z): """ Implement the RELU function. Arguments: Z -- Output of the linear layer, of any shape Returns: A -- Post-activation parameter, of the same shape as Z cache -- a python dictionary containing "A" ; stored for computing the backward pass efficiently """ A = np.maximum(0,Z); assert(A.shape == Z.shape); cache = Z; return A, cache; the relu_backward function： 123456789101112131415161718192021def relu_backward(dA, cache): """ Implement the backward propagation for a single RELU unit. Arguments: dA -- post-activation gradient, of any shape cache -- 'Z' where we store for computing backward propagation efficiently Returns: dZ -- Gradient of the cost with respect to Z """ Z = cache; dZ = np.array(dA, copy = True); # just converting dz to a correct object. # When z &lt;= 0, you should set dz to 0 as well. dZ[Z &lt;= 0] = 0; assert (dZ.shape == Z.shape); return dZ; 2. Outline of the AssignmentTo build your neural network, you will be implementing several “helper functions”. These helper functions will be used in the next assignment to build a two-layer neural network and an L-layer neural network. Each small helper function you will implement will have detailed instructions that will walk you through the necessary steps. Here is an outline of this assignment, you will: Initialize the parameters for a two-layer network and for an L-layer neural network. Implement the forward propagation module (shown in purple in the figure below). Complete the LINEAR part of a layer’s forward propagation step (resulting in Z[l]). We give you the ACTIVATION function (relu/sigmoid). Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] forward function. Stack the [LINEAR-&gt;RELU] forward function L-1 time (for layers 1 through L-1) and add a [LINEAR-&gt;SIGMOID] at the end (for the final layer L). This gives you a new L_model_forward function. Compute the loss. Implement the backward propagation module (denoted in red in the figure below). Complete the LINEAR part of a layer’s backward propagation step. We give you the gradient of the ACTIVATE function (relu_backward/sigmoid_backward) Combine the previous two steps into a new [LINEAR-&gt;ACTIVATION] backward function. Stack [LINEAR-&gt;RELU] backward L-1 times and add [LINEAR-&gt;SIGMOID] backward in a new L_model_backward function Finally update the parameters. Note that for every forward function, there is a corresponding backward function. That is why at every step of your forward module you will be storing some values in a cache. The cached values are useful for computing gradients. In the backpropagation module you will then use the cache to calculate the gradients. This assignment will show you exactly how to carry out each of these steps. 3. InitializationYou will write two helper functions that will initialize the parameters for your model. The first function will be used to initialize parameters for a two layer model. The second one will generalize this initialization process to L layers. 3.1 2-layer Neural NetworkExercise: Create and initialize the parameters of the 2-layer neural network. Instructions: The model’s structure is: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Use random initialization for the weight matrices. Use np.random.randn(shape)*0.01 with the correct shape. Use zero initialization for the biases. Use np.zeros(shape). 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: initialize_parametersdef initialize_parameters(n_x, n_h, n_y): """ Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: parameters -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) """ np.random.seed(1); ### START CODE HERE ### (≈ 4 lines of code) W1 = np.random.randn(n_h, n_x) * 0.01; b1 = np.zeros((n_h, 1)); W2 = np.random.randn(n_y, n_h) * 0.01; b2 = np.zeros((n_y, 1)); ### END CODE HERE ### assert(W1.shape == (n_h, n_x)); assert(b1.shape == (n_h, 1)); assert(W2.shape == (n_y, n_h)); assert(b2.shape == (n_y, 1)); parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125;; return parameters; 12345parameters = initialize_parameters(3,2,1);print("W1 = " + str(parameters["W1"]));print("b1 = " + str(parameters["b1"]));print("W2 = " + str(parameters["W2"]));print("b2 = " + str(parameters["b2"])); W1 = [[ 0.01624345 -0.00611756 -0.00528172] [-0.01072969 0.00865408 -0.02301539]] b1 = [[0.] [0.]] W2 = [[ 0.01744812 -0.00761207]] b2 = [[0.]] 3.2 L-layer Neural NetworkThe initialization for a deeper L-layer neural network is more complicated because there are many more weight matrices and bias vectors. When completing the initialize_parameters_deep, you should make sure that your dimensions match between each layer. Recall that $n^{[l]}$ is the number of units in layer $l$. Thus for example if the size of our input $X$ is $(12288,209)$ (with $m=209$ examples) then: Shape of W Shape of b Activation Shape of Activation Layer 1 (n[1],12288) (n[1],1) Z[1]=W[1]X+b[1] (n[1],209) Layer 2 (n[2],n[1]) (n[2],1) Z[2]=W[2]A[1]+b[2] (n[2],209) $\vdots$ $\vdots$ $\vdots$ $\vdots$ $\vdots$ Layer L-1 (n[L−1],n[L−2]) (n[L−1],1) Z[L−1]=W[L−1]A[L−2]+b[L−1] (n[L−1],209) Layer L (n[L],n[L−1]) (n[L],1) Z[L]=W[L]A[L−1]+b[L] (n[L],209) Remember that when we compute $WX+b$ in python, it carries out broadcasting. For example, if: $$W = \begin{bmatrix} j & k & l\\ m & n & o \\ p & q & r \end{bmatrix}\;\;\; X = \begin{bmatrix} a & b & c\\ d & e & f \\ g & h & i \end{bmatrix} \;\;\; b =\begin{bmatrix} s \\ t \\ u \end{bmatrix}\tag{1}$$ Then $WX+b$ will be: $$WX + b = \begin{bmatrix} (ja + kd + lg) + s & (jb + ke + lh) + s & (jc + kf + li)+ s\\ (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\ (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u \end{bmatrix}\tag{2}$$ Exercise: Implement initialization for an L-layer Neural Network. Instructions: The model’s structure is [LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID. I.e., it has L−1 layers using a ReLU activation function followed by an output layer with a sigmoid activation function. Use random initialization for the weight matrices. Use np.random.rand(shape) * 0.01. Use zeros initialization for the biases. Use np.zeros(shape). We will store $n^{[l]}$, the number of units in different layers, in a variable layer_dims. For example, the layer_dims for the “Planar Data classification model” from last week would have been [2,4,1]: There were two inputs, one hidden layer with 4 hidden units, and an output layer with 1 output unit. Thus means W1’s shape was (4,2), b1 was (4,1), W2 was (1,4) and b2 was (1,1). Now you will generalize this to L layers! Here is the implementation for L=1 (one layer neural network). It should inspire you to implement the general case (L-layer neural network). 123if L == 1:parameters["W" + str(L)] = np.random.randn(layer_dims[1], layer_dims[0]) * 0.01;parameters["b" + str(L)] = np.zeros((layer_dims[1], 1)); 12345678910111213141516171819202122232425262728# GRADED FUNCTION: initialize_parameters_deepdef initialize_parameters_deep(layer_dims): """ Arguments: layer_dims -- python array (list) containing the dimensions of each layer in our network Returns: parameters -- python dictionary containing your parameters "W1", "b1", ..., "WL", "bL": Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1]) bl -- bias vector of shape (layer_dims[l], 1) """ np.random.seed(3); parameters = &#123;&#125;; L = len(layer_dims); # number of layers in the network for l in range(1, L): ### START CODE HERE ### (≈ 2 lines of code) parameters["W" + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01; parameters["b" + str(l)] = np.zeros((layer_dims[l], 1)); ### END CODE HERE ### assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1])); assert(parameters['b' + str(l)].shape == (layer_dims[l], 1)); return parameters; 12345parameters = initialize_parameters_deep([5,4,3]);print("W1 = " + str(parameters["W1"]));print("b1 = " + str(parameters["b1"]));print("W2 = " + str(parameters["W2"]));print("b2 = " + str(parameters["b2"])); W1 = [[ 0.01788628 0.0043651 0.00096497 -0.01863493 -0.00277388] [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218] [-0.01313865 0.00884622 0.00881318 0.01709573 0.00050034] [-0.00404677 -0.0054536 -0.01546477 0.00982367 -0.01101068]] b1 = [[0.] [0.] [0.] [0.]] W2 = [[-0.01185047 -0.0020565 0.01486148 0.00236716] [-0.01023785 -0.00712993 0.00625245 -0.00160513] [-0.00768836 -0.00230031 0.00745056 0.01976111]] b2 = [[0.] [0.] [0.]] 4 Forward propagation module4.1 Linear ForwardNow that you have initialized your parameters, you will do the forward propagation module. You will start by implementing some basic functions that you will use later when implementing the model. You will complete three functions in this order: LINEAR LINEAR -&gt; ACTIVATION where ACTIVATION will be either ReLU or Sigmoid. [LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID (whole model) The linear forward module (vectorized over all the examples) computes the following equations:$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\tag{3}$$ where $A^{[0]}=X$. Exercise: Build the linear part of forward propagation. Reminder:The mathematical representation of this unit is $Z^{[l]}=W^{[l]}A^{[l−1]}+b^{[l]}$. You may also find np.dot() useful. If your dimensions don’t match, printing W.shape may help. 123456789101112131415161718192021222324# GRADED FUNCTION: linear_forwarddef linear_forward(A, W, b): """ Implement the linear part of a layer's forward propagation. Arguments: A -- activations from previous layer (or input data): (size of previous layer, number of examples) W -- weights matrix: numpy array of shape (size of current layer, size of previous layer) b -- bias vector, numpy array of shape (size of the current layer, 1) Returns: Z -- the input of the activation function, also called pre-activation parameter cache -- a python dictionary containing "A", "W" and "b" ; stored for computing the backward pass efficiently """ ### START CODE HERE ### (≈ 1 line of code) Z = np.dot(W, A) + b; ### END CODE HERE ### assert(Z.shape == (W.shape[0], A.shape[1])); cache = (A, W, b); return Z, cache; 123A, W, b = linear_forward_test_case();Z, linear_cache = linear_forward(A, W, b);print("Z = " + str(Z)); Z = [[ 3.26295337 -1.23429987]] linear_forward_test_case: 123456def linear_forward_test_case(): np.random.seed(1); A = np.random.randn(3,2); W = np.random.randn(1,3); b = np.random.randn(1,1); return A, W, b; 4.2 Linear-Activation ForwardIn this notebook, you will use two activation functions: Sigmoid: $\sigma(Z) = \sigma(W A + b) = \frac{1}{ 1 + e^{-(W A + b)} }$ We have provided you with the sigmoid function. This function returns two items: the activation value “a” and a “cache” that contains “Z” (it’s what we will feed in to the corresponding backward function). To use it you could just call: 1A, activation_cache = sigmoid(Z); ReLU: The mathematical formula for ReLu is A=RELU(Z)=max(0,Z). We have provided you with the relu function. This function returns two items: the activation value “A” and a “cache” that contains “Z” (it’s what we will feed in to the corresponding backward function). To use it you could just call: 1A, activation_cache = relu(Z); For more convenience, you are going to group two functions (Linear and Activation) into one function (LINEAR-&gt;ACTIVATION). Hence, you will implement a function that does the LINEAR forward step followed by an ACTIVATION forward step. Exercise: Implement the forward propagation of the LINEAR-&gt;ACTIVATION layer. Mathematical relation is:$A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ where the activation “g” can be sigmoid() or relu(). Use linear_forward() and the correct activation function. 123456789101112131415161718192021222324252627282930313233343536# GRADED FUNCTION: linear_activation_forwarddef linear_activation_forward(A_prev, W, b, activation): """ Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer Arguments: A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples) W -- weights matrix: numpy array of shape (size of current layer, size of previous layer) b -- bias vector, numpy array of shape (size of the current layer, 1) activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu" Returns: A -- the output of the activation function, also called the post-activation value cache -- a python dictionary containing "linear_cache" and "activation_cache"; stored for computing the backward pass efficiently """ if activation == "sigmoid": # Inputs: "A_prev, W, b". Outputs: "A, activation_cache". ### START CODE HERE ### (≈ 2 lines of code) Z, linear_cache = linear_forward(A_prev, W, b); # Z, (W, A_prev, B) A, activation_cache = sigmoid(Z); # A, (Z) ### END CODE HERE ### elif activation == "relu": # Inputs: "A_prev, W, b". Outputs: "A, activation_cache". ### START CODE HERE ### (≈ 2 lines of code) Z, linear_cache = linear_forward(A_prev, W, b); A, activation_cache = relu(Z); ### END CODE HERE ### assert (A.shape == (W.shape[0], A_prev.shape[1])); cache = (linear_cache, activation_cache); #, ((W, A_prev, B) ,(Z)) return A, cache; 1234567A_prev, W, b = linear_activation_forward_test_case();A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = "sigmoid");print("With sigmoid: A = " + str(A));A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = "relu");print("With ReLU: A = " + str(A)); With sigmoid: A = [[0.96890023 0.11013289]] With ReLU: A = [[3.43896131 0. ]] linear_activation_forward_test_case function:123456def linear_activation_forward_test_case(): np.random.seed(2) A_prev = np.random.randn(3,2) W = np.random.randn(1,3) b = np.random.randn(1,1) return A_prev, W, b Note: In deep learning, the “[LINEAR-&gt;ACTIVATION]” computation is counted as a single layer in the neural network, not two layers. 4.3 L-Layer ModelFor even more convenience when implementing the L-layer Neural Net, you will need a function that replicates the previous one (linear_activation_forward with RELU) L−1 times, then follows that with one linear_activation_forward with SIGMOID. Exercise: Implement the forward propagation of the above model. Instruction: In the code below, the variable AL will denote $A^{[L]} = \sigma(Z^{[L]}) = \sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (This is sometimes also called Yhat, i.e., this is $\hat{Y}$.) Tips: Use the functions you had previously written Use a for loop to replicate [LINEAR-&gt;RELU] (L-1) times Don’t forget to keep track of the caches in the “caches” list. To add a new value c to a list, you can use list.append(c). 1234567891011121314151617181920212223242526272829303132333435363738# GRADED FUNCTION: L_model_forwarddef L_model_forward(X, parameters): """ Implement forward propagation for the [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID computation Arguments: X -- data, numpy array of shape (input size, number of examples) parameters -- output of initialize_parameters_deep() Returns: AL -- last post-activation value caches -- list of caches containing: every cache of linear_relu_forward() (there are L-1 of them, indexed from 0 to L-2) the cache of linear_sigmoid_forward() (there is one, indexed L-1) """ caches = [] A = X L = len(parameters) // 2 # number of layers in the neural network # Implement [LINEAR -&gt; RELU]*(L-1). Add "cache" to the "caches" list. for l in range(1, L): A_prev = A ### START CODE HERE ### (≈ 2 lines of code) A, linear_activation_cache = linear_activation_forward(A_prev, parameters["W" + str(l)], parameters["b" + str(l)], "relu"); caches.append(linear_activation_cache); ### END CODE HERE ### # Implement LINEAR -&gt; SIGMOID. Add "cache" to the "caches" list. ### START CODE HERE ### (≈ 2 lines of code) AL, linear_activation_cache = linear_activation_forward(A, parameters["W" + str(L)], parameters["b" + str(L)], "sigmoid"); caches.append(linear_activation_cache); ### END CODE HERE ### assert(AL.shape == (1,X.shape[1])); return AL, caches; 1234X, parameters = L_model_forward_test_case_2hidden();AL, caches = L_model_forward(X, parameters);print("AL = " + str(AL));print("Length of caches list = " + str(len(caches))); AL = [[0.03921668 0.70498921 0.19734387 0.04728177]] Length of caches list = 3 L_model_forward_test_case function:12345678910111213def L_model_forward_test_case(): np.random.seed(1); X = np.random.randn(4,2); W1 = np.random.randn(3,4); b1 = np.random.randn(3,1); W2 = np.random.randn(1,3); b2 = np.random.randn(1,1); parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125;; return X, parameters; Great! Now you have a full forward propagation that takes the input $X$ and outputs a row vector $A^{[L]}$ containing your predictions. It also records all intermediate values in “caches”. Using $A^{[L]}$, you can compute the cost of your predictions. 5. Cost functionNow you will implement forward and backward propagation. You need to compute the cost, because you want to check if your model is actually learning. Exercise: Compute the cross-entropy cost $J$, using the following formula:$$-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{L}\right)) \tag{4}$$ 1234567891011121314151617181920212223242526# GRADED FUNCTION: compute_costdef compute_cost(AL, Y): """ Implement the cost function defined by equation (7). Arguments: AL -- probability vector corresponding to your label predictions, shape (1, number of examples) Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples) Returns: cost -- cross-entropy cost """ m = Y.shape[1]; # Compute loss from aL and y. ### START CODE HERE ### (≈ 1 lines of code) cost = -1 / m * (np.dot(Y, np.log(AL).T) + np.dot(1 - Y, np.log(1 - AL).T)); ### END CODE HERE ### cost = np.squeeze(cost); # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17). #assert(isinstance(cost, float)); assert(cost.shape == ()); return cost; 12Y, AL = compute_cost_test_case();print("cost = " + str(compute_cost(AL, Y))); cost = 0.41493159961539694 compute_cost_test_case function:1234def compute_cost_test_case(): Y = np.asarray([[1, 1, 1]]); aL = np.array([[.8,.9,0.4]]); return Y, aL; 6. Backward propagation moduleJust like with forward propagation, you will implement helper functions for backpropagation. Remember that back propagation is used to calculate the gradient of the loss function with respect to the parameters. Reminder: Figure 3 : Forward and Backward propagation for LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID The purple blocks represent the forward propagation, and the red blocks represent the backward propagation. $$\frac{d \mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \frac{d\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\frac{{da^{[2]}}}{{dz^{[2]}}}\frac{{dz^{[2]}}}{{da^{[1]}}}\frac{{da^{[1]}}}{{dz^{[1]}}} \tag{5}$$ In order to calculate the gradient $dW^{[1]} = \frac{\partial L}{\partial W^{[1]}}$, you use the previous chain rule and you do $dW^{[1]} = dz^{[1]} \times \frac{\partial z^{[1]} }{\partial W^{[1]}}$, . During the backpropagation, at each step you multiply your current gradient by the gradient corresponding to the specific layer to get the gradient you wanted. Equivalently, in order to calculate the gradient $db^{[1]} = \frac{\partial L}{\partial b^{[1]}}$, you use the previous chain rule and you do $db^{[1]} = dz^{[1]} \times \frac{\partial z^{[1]} }{\partial b^{[1]}}$. This is why we talk about backpropagation. Now, similar to forward propagation, you are going to build the backward propagation in three steps: LINEAR backward LINEAR -&gt; ACTIVATION backward where ACTIVATION computes the derivative of either the ReLU or sigmoid activation [LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID backward (whole model) 6.1 Linear backwardFor layer l, the linear part is: $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$, (followed by an activation).Suppose you have already calculated the derivative $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$. You want to get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$. The three outputs $(dW^{[l]}, db^{[l]}, dA^{[l]})$, are computed using the input $dZ^{[l]}$. Here are the formulas you need:$$dW^{[l]} = \frac{\partial \mathcal{L} }{\partial W^{[l]}} = \frac{1}{m} dZ^{[l]} A^{[l-1] T} \tag{5}$$ $$db^{[l]} = \frac{\partial \mathcal{L} }{\partial b^{[l]}} = \frac{1}{m} \sum_{i = 1}^{m} dZ^{l}\tag{6}$$ $$dA^{[l-1]} = \frac{\partial \mathcal{L} }{\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \tag{7}$$ Exercise: Use the 3 formulas above to implement linear_backward(). 1234567891011121314151617181920212223242526272829# GRADED FUNCTION: linear_backwarddef linear_backward(dZ, cache): """ Implement the linear portion of backward propagation for a single layer (layer l) Arguments: dZ -- Gradient of the cost with respect to the linear output (of current layer l) cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer Returns: dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev dW -- Gradient of the cost with respect to W (current layer l), same shape as W db -- Gradient of the cost with respect to b (current layer l), same shape as b """ A_prev, W, b = cache; m = A_prev.shape[1]; ### START CODE HERE ### (≈ 3 lines of code) dW = 1 / m * np.dot(dZ, A_prev.T); db = 1 / m * np.sum(dZ, axis = 1, keepdims = True); dA_prev = np.dot(W.T, dZ); ### END CODE HERE ### assert (dA_prev.shape == A_prev.shape); assert (dW.shape == W.shape); assert (db.shape == b.shape); return dA_prev, dW, db; 123456# Set up some test inputsdZ, linear_cache = linear_backward_test_case();dA_prev, dW, db = linear_backward(dZ, linear_cache);print ("dA_prev = "+ str(dA_prev));print ("dW = " + str(dW));print ("db = " + str(db)); dA_prev = [[ 0.51822968 -0.19517421] [-0.40506361 0.15255393] [ 2.37496825 -0.89445391]] dW = [[-0.10076895 1.40685096 1.64992505]] db = [[0.50629448]] linear_backward_test_case function:12345678def linear_backward_test_case(): np.random.seed(1); dZ = np.random.randn(1,2); A = np.random.randn(3,2); W = np.random.randn(1,3); b = np.random.randn(1,1); linear_cache = (A, W, b); return dZ, linear_cache; 6.2 Linear-Activation backwardNext, you will create a function that merges the two helper functions: linear_backward and the backward step for the activation linear_activation_backward. To help you implement linear_activation_backward, we provided two backward functions: sigmoid_backward: Implements the backward propagation for SIGMOID unit. You can call it as follows: 1dZ = sigmoid_backward(dA, activation_cache) relu_backward: Implements the backward propagation for RELU unit. You can call it as follows: 1dZ = relu_backward(dA, activation_cache) If g(.) is the activation function,sigmoid_backward and relu_backward compute: $$dZ^{[l]} = dA^{[l]} * g’(Z^{[l]}) \tag{8}$$ Exercise: Implement the backpropagation for the LINEAR-&gt;ACTIVATION layer. 123456789101112131415161718192021222324252627282930313233# GRADED FUNCTION: linear_activation_backwarddef linear_activation_backward(dA, cache, activation): """ Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer. Arguments: dA -- post-activation gradient for current layer l cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu" Returns: dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev dW -- Gradient of the cost with respect to W (current layer l), same shape as W db -- Gradient of the cost with respect to b (current layer l), same shape as b """ linear_cache, activation_cache = cache if activation == "relu": ### START CODE HERE ### (≈ 2 lines of code) dZ = relu_backward(dA, activation_cache); dA_prev, dW, db = linear_backward(dZ, linear_cache); ### END CODE HERE ### elif activation == "sigmoid": ### START CODE HERE ### (≈ 2 lines of code) dZ = sigmoid_backward(dA, activation_cache); dA_prev, dW, db = linear_backward(dZ, linear_cache); ### END CODE HERE ### return dA_prev, dW, db; 12345678910111213AL, linear_activation_cache = linear_activation_backward_test_case();dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = "sigmoid");print ("sigmoid:");print ("dA_prev = "+ str(dA_prev));print ("dW = " + str(dW));print ("db = " + str(db) + "\n");dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = "relu");print ("relu:");print ("dA_prev = "+ str(dA_prev));print ("dW = " + str(dW));print ("db = " + str(db)); sigmoid: dA_prev = [[ 0.11017994 0.01105339] [ 0.09466817 0.00949723] [-0.05743092 -0.00576154]] dW = [[ 0.10266786 0.09778551 -0.01968084]] db = [[-0.05729622]] relu: dA_prev = [[ 0.44090989 0. ] [ 0.37883606 0. ] [-0.2298228 0. ]] dW = [[ 0.44513824 0.37371418 -0.10478989]] db = [[-0.20837892]] linear_activation_backward_test_case function:123456789101112def linear_activation_backward_test_case(): np.random.seed(2); dA = np.random.randn(1,2); A = np.random.randn(3,2); W = np.random.randn(1,3); b = np.random.randn(1,1); Z = np.random.randn(1,2); linear_cache = (A, W, b); activation_cache = Z; linear_activation_cache = (linear_cache, activation_cache); return dA, linear_activation_cache; 6.3 L-Model BackwardNow you will implement the backward function for the whole network. Recall that when you implemented the L_model_forward function, at each iteration, you stored a cache which contains (X,W,b, and z). In the back propagation module, you will use those variables to compute the gradients. Therefore, in the L_model_backward function, you will iterate through all the hidden layers backward, starting from layer L. On each step, you will use the cached values for layer l to backpropagate through layer l. Figure 5 below shows the backward pass. Initializing backpropagation: To backpropagate through this network, we know that the output is, $A^{[L]} = \sigma(Z^{[L]})$ . Your code thus needs to compute $= \frac{\partial \mathcal{L}}{\partial A^{[L]}}$.To do so, use this formula (derived using calculus which you don’t need in-depth knowledge of):1dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL You can then use this post-activation gradient dAL to keep going backward. As seen in Figure 5, you can now feed in dAL into the LINEAR-&gt;SIGMOID backward function you implemented (which will use the cached values stored by the L_model_forward function). After that, you will have to use a for loop to iterate through all the other layers using the LINEAR-&gt;RELU backward function. You should store each dA, dW, and db in the grads dictionary. To do so, use this formula :$$grads[“dW” + str(l)] = dW^{[l]}\tag{9}$$ For example, for l=3 this would store $dW^{[l]}$ in grads[&quot;dW3&quot;]. Exercise: Implement backpropagation for the [LINEAR-&gt;RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID model. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# GRADED FUNCTION: L_model_backwarddef L_model_backward(AL, Y, caches): """ Implement the backward propagation for the [LINEAR-&gt;RELU] * (L-1) -&gt; LINEAR -&gt; SIGMOID group Arguments: AL -- probability vector, output of the forward propagation (L_model_forward()) Y -- true "label" vector (containing 0 if non-cat, 1 if cat) caches -- list of caches containing: every cache of linear_activation_forward() with "relu" (it's caches[l], for l in range(L-1) i.e l = 0...L-2) the cache of linear_activation_forward() with "sigmoid" (it's caches[L-1]) Returns: grads -- A dictionary with the gradients grads["dA" + str(l)] = ... grads["dW" + str(l)] = ... grads["db" + str(l)] = ... """ grads = &#123;&#125;; L = len(caches); # the number of layers m = AL.shape[1]; Y = Y.reshape(AL.shape); # after this line, Y is the same shape as AL # Initializing the backpropagation ### START CODE HERE ### (1 line of code) dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)); ### END CODE HERE ### # Lth layer (SIGMOID -&gt; LINEAR) gradients. Inputs: "AL, Y, caches". Outputs: "grads["dAL"], grads["dWL"], grads["dbL"] ### START CODE HERE ### (approx. 2 lines) dA_prev, dW, db = linear_activation_backward(dAL, caches[L - 1], "sigmoid"); grads["dA" + str(L)], grads["dW" + str(L)], grads["db" + str(L)] = dA_prev, dW, db; ### END CODE HERE ### for l in reversed(range(L-1)): # lth layer: (RELU -&gt; LINEAR) gradients. # Inputs: "grads["dA" + str(l + 2)], caches". Outputs: "grads["dA" + str(l + 1)] , grads["dW" + str(l + 1)] , grads["db" + str(l + 1)] ### START CODE HERE ### (approx. 5 lines) dA = dA_prev; dA_prev, dW, db = linear_activation_backward(dA, caches[l], "relu"); grads["dA" + str(l + 1)] = dA_prev; grads["dW" + str(l + 1)] = dW; grads["db" + str(l + 1)] = db; ### END CODE HERE ### return grads; 123AL, Y_assess, caches = L_model_backward_test_case();grads = L_model_backward(AL, Y_assess, caches);print_grads(grads); dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167] [0. 0. 0. 0. ] [0.05283652 0.01005865 0.01777766 0.0135308 ]] db1 = [[-0.22007063] [ 0. ] [-0.02835349]] dA1 = [[ 0.12913162 -0.44014127] [-0.14175655 0.48317296] [ 0.01663708 -0.05670698]] L_model_backward_test_case function in testCases_v3.py:1234567891011121314151617181920212223242526272829303132def L_model_backward_test_case(): """ X = np.random.rand(3,2) Y = np.array([[1, 1]]) parameters = &#123;'W1': np.array([[ 1.78862847, 0.43650985, 0.09649747]]), 'b1': np.array([[ 0.]])&#125; aL, caches = (np.array([[ 0.60298372, 0.87182628]]), [((np.array([[ 0.20445225, 0.87811744], [ 0.02738759, 0.67046751], [ 0.4173048 , 0.55868983]]), np.array([[ 1.78862847, 0.43650985, 0.09649747]]), np.array([[ 0.]])), np.array([[ 0.41791293, 1.91720367]]))]) """ np.random.seed(3) AL = np.random.randn(1, 2) Y = np.array([[1, 0]]) A1 = np.random.randn(4,2) W1 = np.random.randn(3,4) b1 = np.random.randn(3,1) Z1 = np.random.randn(3,2) linear_cache_activation_1 = ((A1, W1, b1), Z1) A2 = np.random.randn(3,2) W2 = np.random.randn(1,3) b2 = np.random.randn(1,1) Z2 = np.random.randn(1,2) linear_cache_activation_2 = ((A2, W2, b2), Z2) caches = (linear_cache_activation_1, linear_cache_activation_2) return AL, Y, caches 6.4 Update ParametersIn this section you will update the parameters of the model, using gradient descent:$$W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]} \tag{10}$$ $$b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} \tag{11}$$ where $α$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. Exercise: Implement update_parameters() to update your parameters using gradient descent. Instructions:Update parameters using gradient descent on every $W^{[l]}$ and $b^{[l]}$ for $l=1,2,…,L$. 12345678910111213141516171819202122232425# GRADED FUNCTION: update_parametersdef update_parameters(parameters, grads, learning_rate): """ Update parameters using gradient descent Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients, output of L_model_backward Returns: parameters -- python dictionary containing your updated parameters parameters["W" + str(l)] = ... parameters["b" + str(l)] = ... """ L = len(parameters) // 2 # number of layers in the neural network # Update rule for each parameter. Use a for loop. ### START CODE HERE ### (≈ 3 lines of code) for l in range(L): parameters["W" + str(l + 1)] -= learning_rate * grads["dW" + str(l + 1)]; parameters["b" + str(l + 1)] -= learning_rate * grads["db" + str(l + 1)]; ### END CODE HERE ### return parameters; 1234567parameters, grads = update_parameters_test_case();parameters = update_parameters(parameters, grads, 0.1);print ("W1 = "+ str(parameters["W1"]));print ("b1 = "+ str(parameters["b1"]));print ("W2 = "+ str(parameters["W2"]));print ("b2 = "+ str(parameters["b2"])); W1 = [[-0.59562069 -0.09991781 -2.14584584 1.82662008] [-1.76569676 -0.80627147 0.51115557 -1.18258802] [-1.0535704 -0.86128581 0.68284052 2.20374577]] b1 = [[-0.04659241] [-1.28888275] [ 0.53405496]] W2 = [[-0.55569196 0.0354055 1.32964895]] b2 = [[-0.84610769]] update_parameters_test_case function in testCases_v3.py:123456789101112131415161718192021def update_parameters_test_case(): np.random.seed(2) W1 = np.random.randn(3,4) b1 = np.random.randn(3,1) W2 = np.random.randn(1,3) b2 = np.random.randn(1,1) parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125; np.random.seed(3) dW1 = np.random.randn(3,4) db1 = np.random.randn(3,1) dW2 = np.random.randn(1,3) db2 = np.random.randn(1,1) grads = &#123;"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2&#125; return parameters, grads 7. ConclusionCongrats on implementing all the functions required for building a deep neural network! We know it was a long assignment but going forward it will only get better. The next part of the assignment is easier. In the next assignment you will put all these together to build two models: A two-layer neural network An L-layer neural network You will in fact use these models to classify cat vs non-cat images! Part 2：Deep Neural Network for Image Classification: Application1. PackagesLet’s first import all the packages that you will need during this assignment. numpy is the fundamental package for scientific computing with Python. matplotlib is a library to plot graphs in Python. h5py is a common package to interact with a dataset that is stored on an H5 file. PIL and scipy are used here to test your model with your own picture at the end. dnn_app_utils provides the functions implemented in the “Building your Deep Neural Network: Step by Step” assignment to this notebook. np.random.seed(1) is used to keep all the random function calls consistent. It will help us grade your work. 123456789101112131415161718import timeimport numpy as npimport h5pyimport matplotlib.pyplot as pltimport scipyfrom PIL import Imagefrom scipy import ndimagefrom dnn_app_utils_v2 import *%matplotlib inlineplt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plotsplt.rcParams['image.interpolation'] = 'nearest'plt.rcParams['image.cmap'] = 'gray'%load_ext autoreload%autoreload 2np.random.seed(1) The autoreload extension is already loaded. To reload it, use: %reload_ext autoreload 2. DatasetYou will use the same “Cat vs non-Cat” dataset as in “Logistic Regression as a Neural Network” (Assignment 2). The model you had built had 70% test accuracy on classifying cats vs non-cats images. Hopefully, your new model will perform a better! Problem Statement: You are given a dataset (“data.h5”) containing: a training set of m_train images labelled as cat (1) or non-cat (0) a test set of m_test images labelled as cat and non-cat each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Let’s get more familiar with the dataset. Load the data by running the cell below. 1train_x_orig, train_y, test_x_orig, test_y, classes = load_data(); The following code will show you an image in the dataset. Feel free to change the index and re-run the cell multiple times to see other images. 1234# Example of a pictureindex = 10;plt.imshow(train_x_orig[index]);print ("y = " + str(train_y[0,index]) + ". It's a " + classes[train_y[0,index]].decode("utf-8") + " picture."); y = 0. It&apos;s a non-cat picture. 123456789101112# Explore your dataset m_train = train_x_orig.shape[0];num_px = train_x_orig.shape[1];m_test = test_x_orig.shape[0];print ("Number of training examples: " + str(m_train));print ("Number of testing examples: " + str(m_test));print ("Each image is of size: (" + str(num_px) + ", " + str(num_px) + ", 3)");print ("train_x_orig shape: " + str(train_x_orig.shape));print ("train_y shape: " + str(train_y.shape));print ("test_x_orig shape: " + str(test_x_orig.shape));print ("test_y shape: " + str(test_y.shape)); Number of training examples: 209 Number of testing examples: 50 Each image is of size: (64, 64, 3) train_x_orig shape: (209, 64, 64, 3) train_y shape: (1, 209) test_x_orig shape: (50, 64, 64, 3) test_y shape: (1, 50) As usual, you reshape and standardize the images before feeding them to the network. The code is given in the cell below. 12345678910# Reshape the training and test examples train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T; # The "-1" makes reshape flatten the remaining dimensionstest_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T;# Standardize data to have feature values between 0 and 1.train_x = train_x_flatten / 255.;test_x = test_x_flatten / 255.;print ("train_x's shape: " + str(train_x.shape));print ("test_x's shape: " + str(test_x.shape)); train_x&apos;s shape: (12288, 209) test_x&apos;s shape: (12288, 50) $12288$ equals $64×64×3$ which is the size of one reshaped image vector. 3. Architecture of your modelNow that you are familiar with the dataset, it is time to build a deep neural network to distinguish cat images from non-cat images. You will build two different models: A 2-layer neural network An L-layer deep neural network You will then compare the performance of these models, and also try out different values for L. Let’s look at the two architectures. 3.1 2-layer neural networkThe model can be summarized as: INPUT -&gt; LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID -&gt; OUTPUT Detailed Architecture of figure 2: The input is a $(64,64,3)$ image which is flattened to a vector of size $(12288,1)$. The corresponding vector: $[x_0,x_1,…,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ of size $(n^{[1]},12288)$. You then add a bias term and take its relu to get the following vector: $[a^{[1]}_0,a^{[1]}_1,…,a^{[1]}_{n^{[1]}−1}]^T$. You then repeat the same process. You multiply the resulting vector by $W^{[2]}$ and add your intercept (bias). Finally, you take the sigmoid of the result. If it is greater than $0.5$, you classify it to be a cat. 3.2 L-layer deep neural networkIt is hard to represent an L-layer deep neural network with the above representation. However, here is a simplified network representation:The model can be summarized as: [LINEAR -&gt; RELU] × (L-1) -&gt; LINEAR -&gt; SIGMOID Detailed Architecture of figure 3: The input is a $(64,64,3)$ image which is flattened to a vector of size $(12288,1)$. The corresponding vector: $[x_0,x_1,…,x_{12287}]^T$ is then multiplied by the weight matrix $W^{[1]}$ and then you add the intercept $b^{[l]}$. The result is called the linear unit. Next, you take the relu of the linear unit. This process could be repeated several times for each $(W^{[l]},b^{[l]})$ depending on the model architecture. Finally, you take the sigmoid of the final linear unit. If it is greater than $0.5$, you classify it to be a cat. 3.3 General methodologyAs usual you will follow the Deep Learning methodology to build the model: Initialize parameters / Define hyperparameters Loop for num_iterations: Forward propagation Compute cost function Backward propagation Update parameters (using parameters, and grads from backprop) Use trained parameters to predict labels Let’s now implement those two models! 4. Two-layer neural networkQuestion: Use the helper functions you have implemented in the previous assignment to build a 2-layer neural network with the following structure: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. The functions you may need and their inputs are: 123456789101112131415def initialize_parameters(n_x, n_h, n_y): ... return parameters def linear_activation_forward(A_prev, W, b, activation): ... return A, cachedef compute_cost(AL, Y): ... return costdef linear_activation_backward(dA, cache, activation): ... return dA_prev, dW, dbdef update_parameters(parameters, grads, learning_rate): ... return parameters 12345### CONSTANTS DEFINING THE MODEL ####n_x = 12288; # num_px * num_px * 3n_h = 7;n_y = 1;layers_dims = (n_x, n_h, n_y); 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091#GRADED FUNCTION: two_layer_modeldef two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False): """ Implements a two-layer neural network: LINEAR-&gt;RELU-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- input data, of shape (n_x, number of examples) Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) layers_dims -- dimensions of the layers (n_x, n_h, n_y) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- If set to True, this will print the cost every 100 iterations Returns: parameters -- a dictionary containing W1, W2, b1, and b2 """ np.random.seed(1); grads = &#123;&#125;; costs = []; # to keep track of the cost m = X.shape[1]; # number of examples (n_x, n_h, n_y) = layers_dims; # Initialize parameters dictionary, by calling one of the functions you'd previously implemented ### START CODE HERE ### (≈ 1 line of code) parameters = initialize_parameters(n_x, n_h, n_y); ### END CODE HERE ### # Get W1, b1, W2 and b2 from the dictionary parameters. W1 = parameters["W1"]; b1 = parameters["b1"]; W2 = parameters["W2"]; b2 = parameters["b2"]; # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: LINEAR -&gt; RELU -&gt; LINEAR -&gt; SIGMOID. Inputs: "X, W1, b1". Output: "A1, cache1, A2, cache2". ### START CODE HERE ### (≈ 2 lines of code) A1, cache1 = linear_activation_forward(X, W1, b1, "relu"); A2, cache2 = linear_activation_forward(A1, W2, b2, "sigmoid"); ### END CODE HERE ### # Compute cost ### START CODE HERE ### (≈ 1 line of code) cost = compute_cost(A2, Y); ### END CODE HERE ### # Initializing backward propagation dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2)); # Backward propagation. Inputs: "dA2, cache2, cache1". Outputs: "dA1, dW2, db2; also dA0 (not used), dW1, db1". ### START CODE HERE ### (≈ 2 lines of code) dA1, dW2, db2 = linear_activation_backward(dA2, cache2, "sigmoid"); dA0, dW1, db1 = linear_activation_backward(dA1, cache1, "relu"); ### END CODE HERE ### # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2 grads['dW1'] = dW1; grads['db1'] = db1; grads['dW2'] = dW2; grads['db2'] = db2; # Update parameters. ### START CODE HERE ### (approx. 1 line of code) parameters = update_parameters(parameters, grads, learning_rate); ### END CODE HERE ### # Retrieve W1, b1, W2, b2 from parameters W1 = parameters["W1"]; b1 = parameters["b1"]; W2 = parameters["W2"]; b2 = parameters["b2"]; # Print the cost every 100 training example if print_cost and i % 100 == 0: print("Cost after iteration &#123;&#125;: &#123;&#125;".format(i, np.squeeze(cost))); if print_cost and i % 100 == 0: costs.append(cost); # plot the cost plt.plot(np.squeeze(costs)); plt.ylabel('cost'); plt.xlabel('iterations (per tens)'); plt.title("Learning rate =" + str(learning_rate)); plt.show(); return parameters; Run the cell below to train your parameters. See if your model runs. The cost should be decreasing. It may take up to 5 minutes to run 2500 iterations. Check if the “Cost after iteration 0” matches the expected output below, if not click on the black square button on the upper bar of the notebook to stop the cell and try to find your error. 1parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost = True); Cost after iteration 0: 0.693049735659989 Cost after iteration 100: 0.6464320953428849 Cost after iteration 200: 0.6325140647912678 Cost after iteration 300: 0.6015024920354665 Cost after iteration 400: 0.5601966311605748 Cost after iteration 500: 0.5158304772764729 Cost after iteration 600: 0.4754901313943325 Cost after iteration 700: 0.43391631512257495 Cost after iteration 800: 0.4007977536203886 Cost after iteration 900: 0.35807050113237976 Cost after iteration 1000: 0.33942815383664127 Cost after iteration 1100: 0.3052753636196264 Cost after iteration 1200: 0.2749137728213016 Cost after iteration 1300: 0.2468176821061484 Cost after iteration 1400: 0.19850735037466102 Cost after iteration 1500: 0.1744831811255665 Cost after iteration 1600: 0.17080762978096942 Cost after iteration 1700: 0.11306524562164715 Cost after iteration 1800: 0.09629426845937152 Cost after iteration 1900: 0.0834261795972687 Cost after iteration 2000: 0.07439078704319087 Cost after iteration 2100: 0.06630748132267934 Cost after iteration 2200: 0.05919329501038172 Cost after iteration 2300: 0.053361403485605585 Cost after iteration 2400: 0.04855478562877019 Good thing you built a vectorized implementation! Otherwise it might have taken 10 times longer to train this. Now, you can use the trained parameters to classify images from the dataset. To see your predictions on the training and test sets, run the cell below. 1predictions_train = predict(train_x, train_y, parameters); Accuracy: 0.9999999999999998 1predictions_test = predict(test_x, test_y, parameters); Accuracy: 0.72 the prediction function:123456789101112131415161718192021222324252627282930def predict(X, y, parameters): """ This function is used to predict the results of a L-layer neural network. Arguments: X -- data set of examples you would like to label parameters -- parameters of the trained model Returns: p -- predictions for the given dataset X """ m = X.shape[1] n = len(parameters) // 2 # number of layers in the neural network p = np.zeros((1,m)) # Forward propagation probas, caches = L_model_forward(X, parameters) # convert probas to 0/1 predictions for i in range(0, probas.shape[1]): if probas[0,i] &gt; 0.5: p[0,i] = 1 else: p[0,i] = 0 print("Accuracy: " + str(np.sum((p == y)/m))) return p Note: You may notice that running the model on fewer iterations (say 1500) gives better accuracy on the test set. This is called “early stopping” and we will talk about it in the next course. Early stopping is a way to prevent overfitting. Congratulations! It seems that your 2-layer neural network has better performance (72%) than the logistic regression implementation (70%, assignment week 2). Let’s see if you can do even better with an L-layer model. 5. L-layer Neural NetworkQuestion: Use the helper functions you have implemented previously to build an L-layer neural network with the following structure: [LINEAR -&gt; RELU]×(L-1) -&gt; LINEAR -&gt; SIGMOID. The functions you may need and their inputs are:123456789101112131415def initialize_parameters_deep(layer_dims): ... return parameters def L_model_forward(X, parameters): ... return AL, cachesdef compute_cost(AL, Y): ... return costdef L_model_backward(AL, Y, caches): ... return gradsdef update_parameters(parameters, grads, learning_rate): ... return parameters 12### CONSTANTS ###layers_dims = [12288, 20, 7, 5, 1] # 5-layer model 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# GRADED FUNCTION: L_layer_modeldef L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009 """ Implements a L-layer neural network: [LINEAR-&gt;RELU]*(L-1)-&gt;LINEAR-&gt;SIGMOID. Arguments: X -- data, numpy array of shape (number of examples, num_px * num_px * 3) Y -- true "label" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples) layers_dims -- list containing the input size and each layer size, of length (number of layers + 1). learning_rate -- learning rate of the gradient descent update rule num_iterations -- number of iterations of the optimization loop print_cost -- if True, it prints the cost every 100 steps Returns: parameters -- parameters learnt by the model. They can then be used to predict. """ np.random.seed(1) costs = [] # keep track of cost # Parameters initialization. ### START CODE HERE ### parameters = initialize_parameters_deep(layers_dims); ### END CODE HERE ### # Loop (gradient descent) for i in range(0, num_iterations): # Forward propagation: [LINEAR -&gt; RELU]*(L-1) -&gt; LINEAR -&gt; SIGMOID. ### START CODE HERE ### (≈ 1 line of code) AL, caches =L_model_forward(X, parameters); ### END CODE HERE ### # Compute cost. ### START CODE HERE ### (≈ 1 line of code) cost = compute_cost(AL, Y); ### END CODE HERE ### # Backward propagation. ### START CODE HERE ### (≈ 1 line of code) grads = L_model_backward(AL, Y, caches); ### END CODE HERE ### # Update parameters. ### START CODE HERE ### (≈ 1 line of code) parameters = update_parameters(parameters, grads, learning_rate); ### END CODE HERE ### # Print the cost every 100 training example if print_cost and i % 100 == 0: print ("Cost after iteration %i: %f" %(i, cost)); if print_cost and i % 100 == 0: costs.append(cost); # plot the cost plt.plot(np.squeeze(costs)); plt.ylabel('cost'); plt.xlabel('iterations (per tens)'); plt.title("Learning rate =" + str(learning_rate)); plt.show(); return parameters; You will now train the model as a 5-layer neural network. Run the cell below to train your model. The cost should decrease on every iteration. It may take up to 5 minutes to run 2500 iterations. Check if the “Cost after iteration 0” matches the expected output below, if not click on the black square button on the upper bar of the notebook to stop the cell and try to find your error. 1parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 2500, print_cost = True); Cost after iteration 0: 0.771749 Cost after iteration 100: 0.672053 Cost after iteration 200: 0.648263 Cost after iteration 300: 0.611507 Cost after iteration 400: 0.567047 Cost after iteration 500: 0.540138 Cost after iteration 600: 0.527930 Cost after iteration 700: 0.465477 Cost after iteration 800: 0.369126 Cost after iteration 900: 0.391747 Cost after iteration 1000: 0.315187 Cost after iteration 1100: 0.272700 Cost after iteration 1200: 0.237419 Cost after iteration 1300: 0.199601 Cost after iteration 1400: 0.189263 Cost after iteration 1500: 0.161189 Cost after iteration 1600: 0.148214 Cost after iteration 1700: 0.137775 Cost after iteration 1800: 0.129740 Cost after iteration 1900: 0.121225 Cost after iteration 2000: 0.113821 Cost after iteration 2100: 0.107839 Cost after iteration 2200: 0.102855 Cost after iteration 2300: 0.100897 Cost after iteration 2400: 0.092878 1pred_train = predict(train_x, train_y, parameters); Accuracy: 0.9856459330143539 1pred_test = predict(test_x, test_y, parameters); Accuracy: 0.8 Congrats! It seems that your 5-layer neural network has better performance $(80%) $than your 2-layer neural network $(72%)$ on the same test set. This is good performance for this task. Nice job! Though in the next course on “Improving deep neural networks” you will learn how to obtain even higher accuracy by systematically searching for better hyperparameters (learning_rate, layers_dims, num_iterations, and others you’ll also learn in the next course). 6. Results AnalysisFirst, let’s take a look at some images the L-layer model labeled incorrectly. This will show a few mislabeled images. 1print_mislabeled_images(classes, test_x, test_y, pred_test); A few type of images the model tends to do poorly on include: Cat body in an unusual position Cat appears against a background of a similar color Unusual cat color and species Camera Angle Brightness of the picture Scale variation (cat is very large or small in image) 7. Test with your own image (optional/ungraded exercise)Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that: Click on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub. Add your image to this Jupyter Notebook’s directory, in the “images” folder Change your image’s name in the following code Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)! 123456789101112## START CODE HERE ##my_image = "1.png"; # change this to the name of your image file my_label_y = [1]; # the true class of your image (1 -&gt; cat, 0 -&gt; non-cat)## END CODE HERE ##fname = "images/" + my_image;image = np.array(ndimage.imread(fname, flatten=False));my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px * num_px * 3,1));my_predicted_image = predict(my_image, my_label_y, parameters);plt.imshow(image);print ("y = " + str(np.squeeze(my_predicted_image)) + ", your L-layer model predicts a \"" + classes[int(np.squeeze(my_predicted_image)),].decode("utf-8") + "\" picture."); Accuracy: 1.0 y = 1.0, your L-layer model predicts a &quot;cat&quot; picture.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>neural-networks-deep-learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[planar data classification with one-hidden layer]]></title>
    <url>%2F2018%2F02%2F06%2Fplanar-data-classification-with-one-hidden%20layer_week3%2F</url>
    <content type="text"><![CDATA[NoteThese are my personal programming assignments at the third week after studying the course neural-networks-deep-learning and the copyright belongs to deeplearning.ai. planar data classification with one hidden layer1 PackagesLet’s first import all the packages that you will need during this assignment. numpy is the fundamental package for scientific computing with Python. sklearn provides simple and efficient tools for data mining and data analysis. matplotlib is a library for plotting graphs in Python. testCases_v2 provides some test examples to assess the correctness of your functions planar_utils provide various useful functions used in this assignment 123456789101112# Package importsimport numpy as np;import matplotlib.pyplot as plt;import sklearn;import sklearn.datasets;import sklearn.linear_model;from testCases_v2 import *;from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets;%matplotlib inlinenp.random.seed(1); # set a seed so that the results are consistent You can get the support code from here. 2 DatasetFirst, let’s get the dataset you will work on. The following code will load a “flower” 2-class dataset into variables X and Y. 1234567891011121314151617def load_planar_dataset(): #generate two random array X and Y np.random.seed(1) m=400 #样本的数量 N=int(m/2) #每一类的数量，共有俩类数据 D=2 #维数，二维数据 X=np.zeros((m,D)) # 生成（m，2）独立的样本 Y=np.zeros((m,1),dtype='uint8') #生成（m，1）矩阵的样本 a=4 #maximum ray of the flower for j in range(2): ix=range(N*j,N*(j+1)) #范围在[N*j,N*(j+1)]之间 t=np.linspace(j*3.12,(j+1)*3.12,N)+np.random.randn(N)*0.2 #theta r=a*np.sin(4*t)+np.random.randn(N)*0.2 #radius X[ix]=np.c_[r*np.sin(t),r*np.cos(t)] # (m,2),使用np.c_是为了形成（m，2）矩阵 Y[ix]=j X=X.T #（2，m） Y=Y.T # (1,m) return X,Y Visualize the dataset using matplotlib. The data looks like a “flower” with some red (label y=0) and some blue (y=1) points. Your goal is to build a model to fit this data. 123X,Y = load_planar_dataset();plt.scatter(X[0,:], X[1,:], c=np.squeeze(Y),s=40,cmap=plt.cm.Spectral);plt.show(); You have: a numpy-array (matrix) X that contains your features (x1, x2) a numpy-array (vector) Y that contains your labels (red:0, blue:1). Lets first get a better sense of what our data is like. Exercise: How many training examples do you have? In addition, what is the shape of the variables X and Y? Hint: How do you get the shape of a numpy array? (help) 123456789### START CODE HERE ### (≈ 3 lines of code)shape_X = X.shapeshape_Y = Y.shapem = X.shape[1] # training set size### END CODE HERE ###print ('The shape of X is: ' + str(shape_X))print ('The shape of Y is: ' + str(shape_Y))print ('I have m = %d training examples!' % (m)) The shape of X is: (2, 400) The shape of Y is: (1, 400) I have m = 400 training examples! 3 Simple Logistic RegressionBefore building a full neural network, lets first see how logistic regression performs on this problem. You can use sklearn’s built-in functions to do that. Run the code below to train a logistic regression classifier on the dataset. 123# Train the logistic regression classifierclf = sklearn.linear_model.LogisticRegressionCV();clf.fit(X.T, np.squeeze(Y.T)); You can now plot the decision boundary of these models. Run the code below. 12345678# Plot the decision boundary for logistic regressionplot_decision_boundary(lambda x: clf.predict(x), X, Y)plt.title("Logistic Regression")# Print accuracyLR_predictions = clf.predict(X.T)print ('Accuracy of logistic regression: %d ' % float((np.dot(Y,LR_predictions) + np.dot(1-Y,1-LR_predictions))/float(Y.size)*100) + '% ' + "(percentage of correctly labelled datapoints)") Accuracy of logistic regression: 47 % (percentage of correctly labelled datapoints) plot_decision_boundary: 123456789101112131415def plot_decision_boundary(model, X, y): # Set min and max values and give it some padding x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1 y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1 h = 0.01 # Generate a grid of points with distance h between them xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # Predict the function value for the whole grid Z = model(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) # Plot the contour and training examples plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral) plt.ylabel('x2') plt.xlabel('x1') plt.scatter(X[0, :], X[1, :], c=np.squeeze(y), cmap=plt.cm.Spectral) 4 Neural Network modelLogistic regression did not work well on the “flower dataset”. You are going to train a Neural Network with a single hidden layer. Here is our model: Mathematically: For one example $x^{(i)}$:$$z^{[1] (i)} = W^{[1]} x^{(i)} + b^{[1] (i)}\tag{1}$$ $$a^{[1] (i)} = \tanh(z^{[1] (i)})\tag{2}$$ $$z^{[2] (i)} = W^{[2]} a^{[1] (i)} + b^{[2] (i)}\tag{3}$$ $$\hat{y}^{(i)} = a^{[2] (i)} = \sigma(z^{ [2] (i)})\tag{4}$$ $$y^{(i)}_{prediction} = \begin{cases} 1 &amp; \mbox{if } a^{2} &gt; 0.5 \\ 0 &amp; \mbox{otherwise } \end{cases}\tag{5}$$ Given the predictions on all the examples, you can also compute the cost $J$ as follows: $$J = - \frac{1}{m} \sum\limits_{i = 0}^{m} \large\left(\small y^{(i)}\log\left(a^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[2] (i)}\right) \large \right) \small \tag{6}$$ Reminder: The general methodology to build a Neural Network is to: Define the neural network structure ( # of input units, # of hidden units, etc). Initialize the model’s parameters Loop: Implement forward propagation Compute loss Implement backward propagation to get the gradients Update parameters (gradient descent) You often build helper functions to compute steps 1-3 and then merge them into one function we call nn_model(). Once you’ve built nn_model() and learnt the right parameters, you can make predictions on new data. 4.1 Defining the neural network structureExercise: Define three variables: $n_x$ : the size of the input layer $n_h$ : the size of the hidden layer (set this to 4) $n_y$ : the size of the output layer Hint: Use shapes of $X$ and $Y$ to find $n_x$ and $n_y$. Also, hard code the hidden layer size to be 4. 12345678910111213141516171819# GRADED FUNCTION: layer_sizesdef layer_sizes(X, Y): """ Arguments: X -- input dataset of shape (input size, number of examples) Y -- labels of shape (output size, number of examples) Returns: n_x -- the size of the input layer n_h -- the size of the hidden layer n_y -- the size of the output layer """ ### START CODE HERE ### (≈ 3 lines of code) n_x = X.shape[0]; # size of input layer n_h = 4; n_y = Y.shape[0];# size of output layer ### END CODE HERE ### return (n_x, n_h, n_y); 12345X_assess, Y_assess = layer_sizes_test_case();(n_x, n_h, n_y) = layer_sizes(X_assess, Y_assess);print("The size of the input layer is: n_x = " + str(n_x));print("The size of the hidden layer is: n_h = " + str(n_h));print("The size of the output layer is: n_y = " + str(n_y)); The size of the input layer is: n_x = 5 The size of the hidden layer is: n_h = 4 The size of the output layer is: n_y = 2 4.2 Initialize the model’s parametersExercise: Implement the function initialize_parameters(). Instructions: Make sure your parameters’ sizes are right. Refer to the neural network figure above if needed. You will initialize the weights matrices with random values. Use: np.random.randn(a,b) * 0.01 to randomly initialize a matrix of shape (a,b). You will initialize the bias vectors as zeros. Use: np.zeros((a,b)) to initialize a matrix of shape (a,b) with zeros. 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: initialize_parametersdef initialize_parameters(n_x, n_h, n_y): """ Argument: n_x -- size of the input layer n_h -- size of the hidden layer n_y -- size of the output layer Returns: params -- python dictionary containing your parameters: W1 -- weight matrix of shape (n_h, n_x) b1 -- bias vector of shape (n_h, 1) W2 -- weight matrix of shape (n_y, n_h) b2 -- bias vector of shape (n_y, 1) """ np.random.seed(2) # we set up a seed so that your output matches ours although the initialization is random. ### START CODE HERE ### (≈ 4 lines of code) W1 = np.random.rand(n_h, n_x) * 0.01; b1 = np.zeros((n_h, 1)); W2 = np.random.rand(n_y, n_h) * 0.01; b2 = np.zeros((n_y, 1)); ### END CODE HERE ### assert (W1.shape == (n_h, n_x)); assert (b1.shape == (n_h, 1)); assert (W2.shape == (n_y, n_h)); assert (b2.shape == (n_y, 1)); parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125;; return parameters; 123456n_x, n_h, n_y = initialize_parameters_test_case();parameters = initialize_parameters(n_x, n_h, n_y);print("W1 = " + str(parameters["W1"]));print("b1 = " + str(parameters["b1"]));print("W2 = " + str(parameters["W2"]));print("b2 = " + str(parameters["b2"])); W1 = [[0.00435995 0.00025926] [0.00549662 0.00435322] [0.00420368 0.00330335] [0.00204649 0.00619271]] b1 = [[0.] [0.] [0.] [0.]] W2 = [[0.00299655 0.00266827 0.00621134 0.00529142]] b2 = [[0.]] 4.3 The LoopQuestion: Implement forward_propagation(). Instructions: Look above at the mathematical representation of your classifier. You can use the function sigmoid(). It is built-in (imported) in the notebook. You can use the function np.tanh(). It is part of the numpy library. The steps you have to implement are: Retrieve each parameter from the dictionary “parameters” (which is the output of initialize_parameters() ) by using parameters[&quot;..&quot;]. Implement Forward Propagation. Compute $Z^{[1]},A^{[1]},Z^{[2]}$ and $A^{[2]}$ (the vector of all your predictions on all the examples in the training set). Values needed in the backpropagation are stored in “cache“. The cache will be given as an input to the backpropagation function. 123456789101112131415161718192021222324252627282930313233343536# GRADED FUNCTION: forward_propagationdef forward_propagation(X, parameters): """ Argument: X -- input data of size (n_x, m) parameters -- python dictionary containing your parameters (output of initialization function) Returns: A2 -- The sigmoid output of the second activation cache -- a dictionary containing "Z1", "A1", "Z2" and "A2" """ # Retrieve each parameter from the dictionary "parameters" ### START CODE HERE ### (≈ 4 lines of code) W1 = parameters["W1"]; b1 = parameters["b1"]; W2 = parameters["W2"]; b2 = parameters["b2"]; ### END CODE HERE ### # Implement Forward Propagation to calculate A2 (probabilities) ### START CODE HERE ### (≈ 4 lines of code) Z1 = np.dot(W1, X) + b1; A1 = np.tanh(Z1); Z2 = np.dot(W2, A1) + b2; A2 = sigmoid(Z2); ### END CODE HERE ### assert(A2.shape == (1, X.shape[1])) cache = &#123;"Z1": Z1, "A1": A1, "Z2": Z2, "A2": A2&#125;; return A2, cache; 1234X_assess, parameters = forward_propagation_test_case();A2, cache = forward_propagation(X_assess, parameters);# Note: we use the mean here just to make sure that your output matches ours. print(np.mean(cache['Z1']) ,np.mean(cache['A1']),np.mean(cache['Z2']),np.mean(cache['A2'])); 0.26281864019752443 0.09199904522700109 -1.3076660128732143 0.21287768171914198 Exercise: Implement compute_cost() to compute the value of the cost $J$. Instructions: There are many ways to implement the cross-entropy loss. To help you, we give you how we would have implemented $\sum\limits_{i=0}^{m} y^{(i)}\log(a^{2})$ 12logprobs = np.multiply(np.log(A2),Y);cost = - np.sum(logprobs); # no need to use a for loop! (you can use either np.multiply() and then np.sum() or directly np.dot()). 123456789101112131415161718192021222324252627282930# GRADED FUNCTION: compute_costdef compute_cost(A2, Y, parameters): """ Computes the cross-entropy cost given in equation (13) Arguments: A2 -- The sigmoid output of the second activation, of shape (1, number of examples) Y -- "true" labels vector of shape (1, number of examples) parameters -- python dictionary containing your parameters W1, b1, W2 and b2 Returns: cost -- cross-entropy cost given equation (13) """ m = Y.shape[1] # number of example # Compute the cross-entropy cost ### START CODE HERE ### (≈ 2 lines of code) #logprobs = np.multiply(np.log(A2), Y) + np.multiply(np.log(1-A2), (1-Y)) logprobs = np.dot(np.log(A2), Y.T) + np.dot(np.log(1 - A2), (1 - Y).T); cost = - 1.0 / m * logprobs[0][0]; ### END CODE HERE ### cost = np.squeeze(cost); # makes sure cost is the dimension we expect. # E.g., turns [[17]] into 17 assert(isinstance(cost, float)); return cost; 12A2, Y_assess, parameters = compute_cost_test_case();print("cost = " + str(compute_cost(A2, Y_assess, parameters))); cost = 0.6930587610394646 Using the cache computed during forward propagation, you can now implement backward propagation. Question: Implement the function backward_propagation(). Instructions:Backpropagation is usually the hardest (most mathematical) part in deep learning. To help you, here again is the slide from the lecture on backpropagation. You’ll want to use the six equations on the right of this slide, since you are building a vectorized implementation. Tips:To compute $dZ_1$ you’ll need to compute $g’^{[1]}(Z^{[1]})$. Since $g1$ is the tanh activation function, if $a=g^{[1]}(z)$ then $g’^{[1]}(z)=1−a^2$. So you can compute $g’^{[1]}(Z^{[1]})$ using (1 - np.power(A1, 2)). 123456789101112131415161718192021222324252627282930313233343536373839404142434445# GRADED FUNCTION: backward_propagationdef backward_propagation(parameters, cache, X, Y): """ Implement the backward propagation using the instructions above. Arguments: parameters -- python dictionary containing our parameters cache -- a dictionary containing "Z1", "A1", "Z2" and "A2". X -- input data of shape (2, number of examples) Y -- "true" labels vector of shape (1, number of examples) Returns: grads -- python dictionary containing your gradients with respect to different parameters """ m = X.shape[1]; # First, retrieve W1 and W2 from the dictionary "parameters". ### START CODE HERE ### (≈ 2 lines of code) W1 = parameters["W1"]; W2 = parameters["W2"]; ### END CODE HERE ### # Retrieve also A1 and A2 from dictionary "cache". ### START CODE HERE ### (≈ 2 lines of code) A1 = cache["A1"]; A2 = cache["A2"]; ### END CODE HERE ### # Backward propagation: calculate dW1, db1, dW2, db2. ### START CODE HERE ### (≈ 6 lines of code, corresponding to 6 equations on slide above) dZ2 = A2 - Y; dW2 = 1 / m * np.dot(dZ2, A1.T); db2 = 1 / m * np.sum(dZ2, axis = 1, keepdims = True); dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2)); dW1 = 1 / m * np.dot(dZ1, X.T); db1 = 1 / m * np.sum(dZ1, axis = 1, keepdims = True); ### END CODE HERE ### grads = &#123;"dW1": dW1, "db1": db1, "dW2": dW2, "db2": db2&#125;; return grads; 1234567parameters, cache, X_assess, Y_assess = backward_propagation_test_case()grads = backward_propagation(parameters, cache, X_assess, Y_assess)print ("dW1 = "+ str(grads["dW1"]))print ("db1 = "+ str(grads["db1"]))print ("dW2 = "+ str(grads["dW2"]))print ("db2 = "+ str(grads["db2"])) dW1 = [[ 0.00301023 -0.00747267] [ 0.00257968 -0.00641288] [-0.00156892 0.003893 ] [-0.00652037 0.01618243]] db1 = [[ 0.00176201] [ 0.00150995] [-0.00091736] [-0.00381422]] dW2 = [[ 0.00078841 0.01765429 -0.00084166 -0.01022527]] db2 = [[-0.16655712]] Question: Implement the update rule. Use gradient descent. You have to use (dW1, db1, dW2, db2) in order to update (W1, b1, W2, b2). General gradient descent rule: $θ=θ−α\frac{∂J}{∂θ}$ where $α$ is the learning rate and $θ$ represents a parameter. Illustration: The gradient descent algorithm with a good learning rate (converging) and a bad learning rate (diverging). Images courtesy of Adam Harley. 12345678910111213141516171819202122232425262728293031323334353637383940414243# GRADED FUNCTION: update_parametersdef update_parameters(parameters, grads, learning_rate = 1.2): """ Updates parameters using the gradient descent update rule given above Arguments: parameters -- python dictionary containing your parameters grads -- python dictionary containing your gradients Returns: parameters -- python dictionary containing your updated parameters """ # Retrieve each parameter from the dictionary "parameters" ### START CODE HERE ### (≈ 4 lines of code) W1 = parameters["W1"]; b1 = parameters["b1"]; W2 = parameters["W2"]; b2 = parameters["b2"]; ### END CODE HERE ### # Retrieve each gradient from the dictionary "grads" ### START CODE HERE ### (≈ 4 lines of code) dW1 = grads["dW1"]; db1 = grads["db1"]; dW2 = grads["dW2"]; db2 = grads["db2"]; ## END CODE HERE ### # Update rule for each parameter ### START CODE HERE ### (≈ 4 lines of code) W1 = W1 - learning_rate * dW1; b1 = b1 - learning_rate * db1; W2 = W2 - learning_rate * dW2; b2 = b2 - learning_rate * db2; ### END CODE HERE ### parameters = &#123;"W1": W1, "b1": b1, "W2": W2, "b2": b2&#125;; return parameters; 1234567parameters, grads = update_parameters_test_case();parameters = update_parameters(parameters, grads);print("W1 = " + str(parameters["W1"]));print("b1 = " + str(parameters["b1"]));print("W2 = " + str(parameters["W2"]));print("b2 = " + str(parameters["b2"])); W1 = [[-0.00643025 0.01936718] [-0.02410458 0.03978052] [-0.01653973 -0.02096177] [ 0.01046864 -0.05990141]] b1 = [[-1.02420756e-06] [ 1.27373948e-05] [ 8.32996807e-07] [-3.20136836e-06]] W2 = [[-0.01041081 -0.04463285 0.01758031 0.04747113]] b2 = [[0.00010457]] 4.4 Integrate parts 4.1, 4.2 and 4.3 in nn_model()Question: Build your neural network model in nn_model(). Instructions: The neural network model has to use the previous functions in the right order. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152# GRADED FUNCTION: nn_modeldef nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False): """ Arguments: X -- dataset of shape (2, number of examples) Y -- labels of shape (1, number of examples) n_h -- size of the hidden layer num_iterations -- Number of iterations in gradient descent loop print_cost -- if True, print the cost every 1000 iterations Returns: parameters -- parameters learnt by the model. They can then be used to predict. """ np.random.seed(3) n_x = layer_sizes(X, Y)[0] n_y = layer_sizes(X, Y)[2] # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: "n_x, n_h, n_y". Outputs = "W1, b1, W2, b2, parameters". ### START CODE HERE ### (≈ 5 lines of code) parameters = initialize_parameters(n_x, n_h, n_y) W1 = parameters["W1"] b1 = parameters["b1"] W2 = parameters["W2"] b2 = parameters["b2"] ### END CODE HERE ### # Loop (gradient descent) for i in range(0, num_iterations): ### START CODE HERE ### (≈ 4 lines of code) # Forward propagation. Inputs: "X, parameters". Outputs: "A2, cache". A2, cache = forward_propagation(X, parameters) # Cost function. Inputs: "A2, Y, parameters". Outputs: "cost". cost = compute_cost(A2, Y, parameters) # Backpropagation. Inputs: "parameters, cache, X, Y". Outputs: "grads". grads = backward_propagation(parameters, cache, X, Y) # Gradient descent parameter update. Inputs: "parameters, grads". Outputs: "parameters". parameters = update_parameters(parameters, grads, learning_rate = 1.2) ### END CODE HERE ### # Print the cost every 1000 iterations if print_cost and i % 1000 == 0: print ("Cost after iteration %i: %f" %(i, cost)); return parameters; 123456X_assess, Y_assess = nn_model_test_case();parameters = nn_model(X_assess, Y_assess, 4, num_iterations=10000, print_cost=True);print("W1 = " + str(parameters["W1"]));print("b1 = " + str(parameters["b1"]));print("W2 = " + str(parameters["W2"]));print("b2 = " + str(parameters["b2"])); Cost after iteration 0: 0.693175 Cost after iteration 1000: 0.000224 Cost after iteration 2000: 0.000109 Cost after iteration 3000: 0.000072 Cost after iteration 4000: 0.000054 Cost after iteration 5000: 0.000043 Cost after iteration 6000: 0.000036 Cost after iteration 7000: 0.000031 Cost after iteration 8000: 0.000027 Cost after iteration 9000: 0.000024 W1 = [[ 0.78668574 -1.44596408] [ 0.61841465 -1.14797067] [ 0.7941403 -1.45820079] [ 0.54249425 -1.01738417]] b1 = [[-0.38092208] [-0.26640968] [-0.38509848] [-0.21499134]] W2 = [[3.55445121 2.18356796 3.62513709 1.74485812]] b2 = [[0.21512924]] 4.5 PredictionsQuestion: Use your model to predict by building predict(). Use forward propagation to predict results. Reminder: predictions$$ y_{prediction} =\begin{equation}\begin{cases}1 &amp; \text{ if activation &gt; 0.5 } \\0 &amp; \text{ otherwise }\end{cases}\end{equation}$$ As an example, if you would like to set the entries of a matrix X to 0 and 1 based on a threshold you would do: X_new = (X &gt; threshold) 1234567891011121314151617181920212223# GRADED FUNCTION: predictdef predict(parameters, X): """ Using the learned parameters, predicts a class for each example in X Arguments: parameters -- python dictionary containing your parameters X -- input data of size (n_x, m) Returns predictions -- vector of predictions of our model (red: 0 / blue: 1) """ # Computes probabilities using forward propagation, and classifies to 0/1 using 0.5 as the threshold. ### START CODE HERE ### (≈ 2 lines of code) A2, cache = forward_propagation(X, parameters); predictions = (A2 &gt; 0.5); ### END CODE HERE ### return predictions; 123parameters, X_assess = predict_test_case();predictions = predict(parameters, X_assess);print("predictions mean = " + str(np.mean(predictions))); predictions mean = 0.6666666666666666 It is time to run the model and see how it performs on a planar dataset. Run the following code to test your model with a single hidden layer of n_h hidden units. 12345# Build a model with a n_h-dimensional hidden layerparameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True);# Plot the decision boundaryplot_decision_boundary(lambda x: predict(parameters, x.T), X, Y);plt.title("Decision Boundary for hidden layer size " + str(4)); Cost after iteration 0: 0.693159 Cost after iteration 1000: 0.289308 Cost after iteration 2000: 0.273860 Cost after iteration 3000: 0.238116 Cost after iteration 4000: 0.228102 Cost after iteration 5000: 0.223318 Cost after iteration 6000: 0.220193 Cost after iteration 7000: 0.217870 Cost after iteration 8000: 0.216036 Cost after iteration 9000: 0.218642 123# Print accuracypredictions = predict(parameters, X);print ( ('Accuracy: %d ' %(np.mean(Y == predictions) * 100)) + '%'); Accuracy: 90 % Accuracy is really high compared to Logistic Regression. The model has learnt the leaf patterns of the flower! Neural networks are able to learn even highly non-linear decision boundaries, unlike logistic regression. Now, let’s try out several hidden layer sizes. 4.6 Tuning hidden layer size (optional/ungraded exercise)Run the following code. It may take 1-2 minutes. You will observe different behaviors of the model for various hidden layer sizes. 123456789101112# This may take about 2 minutes to runplt.figure(figsize=(16, 32));hidden_layer_sizes = [1, 2, 3, 4, 5, 20, 50];for i, n_h in enumerate(hidden_layer_sizes): plt.subplot(5, 2, i+1); plt.title('Hidden Layer of size %d' % n_h) parameters = nn_model(X, Y, n_h, num_iterations = 5000); plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y); predictions = predict(parameters, X); accuracy = float((np.dot(Y,predictions.T) + np.dot(1-Y,1-predictions.T))/float(Y.size)*100); print ("Accuracy for &#123;&#125; hidden units: &#123;&#125; %".format(n_h, accuracy)); Accuracy for 1 hidden units: 67.5 % Accuracy for 2 hidden units: 67.25 % Accuracy for 3 hidden units: 90.75 % Accuracy for 4 hidden units: 90.75 % Accuracy for 5 hidden units: 91.25 % Accuracy for 20 hidden units: 90.25 % Accuracy for 50 hidden units: 91.0 % Interpretation: The larger models (with more hidden units) are able to fit the training set better, until eventually the largest models overfit the data. The best hidden layer size seems to be around n_h = 5. Indeed, a value around here seems to fits the data well without also incurring noticable overfitting. You will also learn later about regularization, which lets you use very large models (such as n_h = 50) without much overfitting. Optional questions: Note: Remember to submit the assignment but clicking the blue “Submit Assignment” button at the upper-right. Some optional/ungraded questions that you can explore if you wish: What happens when you change the tanh activation for a sigmoid activation or a ReLU activation? Play with the learning_rate. What happens? What if we change the dataset? (See part 5 below!) You’ve learnt to: Build a complete neural network with a hidden layer Make a good use of a non-linear unit Implemented forward propagation and backpropagation, and trained a neural network See the impact of varying the hidden layer size, including overfitting. Nice work! 5 Performance on other datasetsIf you want, you can rerun the whole notebook (minus the dataset part) for each of the following datasets. 123456789def load_extra_datasets(): N = 200 noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3); noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2); blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6); gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None); no_structure = np.random.rand(N, 2), np.random.rand(N, 2); return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure; 123456789101112131415161718192021# Datasetsnoisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()datasets = &#123;"noisy_circles": noisy_circles, "noisy_moons": noisy_moons, "blobs": blobs, "gaussian_quantiles": gaussian_quantiles&#125;;### START CODE HERE ### (choose your dataset)dataset = "noisy_moons";### END CODE HERE ###X, Y = datasets[dataset];X, Y = X.T, Y.reshape(1, Y.shape[0]);# make blobs binaryif dataset == "blobs": Y = Y%2;# Visualize the dataplt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral); 1234567891011121314### START CODE HERE ### (choose your dataset)dataset = "noisy_circles";### END CODE HERE ###X, Y = datasets[dataset];X, Y = X.T, Y.reshape(1, Y.shape[0]);# make blobs binaryif dataset == "blobs": Y = Y%2;# Visualize the dataplt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral); 12345678910111213### START CODE HERE ### (choose your dataset)dataset = "blobs";### END CODE HERE ###X, Y = datasets[dataset];X, Y = X.T, Y.reshape(1, Y.shape[0]);# make blobs binaryif dataset == "blobs": Y = Y%2;# Visualize the dataplt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral); 12345678910111213### START CODE HERE ### (choose your dataset)dataset = "gaussian_quantiles";### END CODE HERE ###X, Y = datasets[dataset];X, Y = X.T, Y.reshape(1, Y.shape[0]);# make blobs binaryif dataset == "blobs": Y = Y%2;# Visualize the dataplt.scatter(X[0, :], X[1, :], c=np.squeeze(Y), s=40, cmap=plt.cm.Spectral);]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>neural-networks-deep-learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[logistic-regression-with-a-neural-network-mindset]]></title>
    <url>%2F2018%2F02%2F05%2Flogistic-regression-with-a-neural-network-mindset_week1_and_week2%2F</url>
    <content type="text"><![CDATA[NoteThese are my personal programming assignments at the first and second week after studying the course neural-networks-deep-learning and the copyright belongs to deeplearning.ai. Part 1：Python Basics with Numpy (optional assignment)1. Building basic functions with numpyNumpy is the main package for scientific computing in Python. It is maintained by a large community (www.numpy.org). In this exercise you will learn several key numpy functions such as np.exp, np.log, and np.reshape. You will need to know how to use these functions for future assignments. 1.1 sigmoid function, np.exp()Exercise: Build a function that returns the sigmoid of a real number $x$. Use math.exp(x) for the exponential function. Reminder:$sigmoid(x)=\frac{1}{1+e^{-x}}$ is sometimes also known as the logistic function. It is a non-linear function used not only in Machine Learning (Logistic Regression), but also in Deep Learning. To refer to a function belonging to a specific package you could call it using package_name.function(). Run the code below to see an example with math.exp(). 1234567891011121314151617181920# GRADED FUNCTION: basic_sigmoidimport mathdef basic_sigmoid(x): """ Compute sigmoid of x. Arguments: x -- A scalar Return: s -- sigmoid(x) """ ### START CODE HERE ### (≈ 1 line of code) s = 1 / (1 + math.exp(-x)); ### END CODE HERE ### return s; 1basic_sigmoid(3) 0.9525741268224334 Actually, we rarely use the “math” library in deep learning because the inputs of the functions are real numbers. In deep learning we mostly use matrices and vectors. This is why numpy is more useful. 123### One reason why we use "numpy" instead of "math" in Deep Learning ###x = [1, 2, 3]basic_sigmoid(x) # you will see this give an error when you run it, because x is a vector. In fact, if $x=(x_1,x_2,…,x_n)$ is a row vector then np.exp(x) will apply the exponential function to every element of $x$. The output will thus be: $np.exp(x)=(e^{x_1},e^{x_2},…,e^{x_n})$ 12345import numpy as np# example of np.expx = np.array([1, 2, 3])print(np.exp(x)) # result is (exp(1), exp(2), exp(3)) [ 2.71828183 7.3890561 20.08553692] Furthermore, if $x$ is a vector, then a Python operation such as $s=x+3$ or $s=\frac{1}{x}$ will output s as a vector of the same size as $x$. Exercise: Implement the sigmoid function using numpy. Instructions: x could now be either a real number, a vector, or a matrix. The data structures we use in numpy to represent these shapes (vectors, matrices…) are called numpy arrays. You don’t need to know more for now. $$\text{For } x \in \mathbb{R}^n \text{, } sigmoid(x) = sigmoid\begin{pmatrix} x_1 \\ x_2 \\ … \\ x_n \\\end{pmatrix} = \begin{pmatrix} \frac{1}{1+e^{-x_1}} \\ \frac{1}{1+e^{-x_2}} \\ … \\ \frac{1}{1+e^{-x_n}} \\\end{pmatrix}\tag{1}$$ 1234567891011121314151617181920# GRADED FUNCTION: sigmoidimport numpy as np # this means you can access numpy functions by writing np.function() instead of numpy.function()def sigmoid(x): """ Compute the sigmoid of x Arguments: x -- A scalar or numpy array of any size Return: s -- sigmoid(x) """ ### START CODE HERE ### (≈ 1 line of code) s = 1 / (1 + np.exp(-x)); ### END CODE HERE ### return s; 12x = np.array([1, 2, 3]);sigmoid(x) array([0.73105858, 0.88079708, 0.95257413]) 1.2 Sigmoid gradientExercise: Implement the function sigmoid_grad() to compute the gradient of the sigmoid function with respect to its input $x$. The formula is:$$sigmoid_derivative(x) = \sigma’(x) = \sigma(x) (1 - \sigma(x))\tag{2}$$You often code this function in two steps: Set s to be the sigmoid of x. You might find your sigmoid(x) function useful. Compute $\sigma’(x) = s(1-s)$ 1234567891011121314151617181920# GRADED FUNCTION: sigmoid_derivativeimport numpy as np; # this means you can access numpy functions by writing np.function() instead of numpy.function()def sigmoid_derivative(x): """ Compute the gradient (also called the slope or derivative) of the sigmoid function with respect to its input x. You can store the output of the sigmoid function into variables and then use it to calculate the gradient. Arguments: x -- A scalar or numpy array Return: ds -- Your computed gradient. """ ### START CODE HERE ### (≈ 2 lines of code) s = 1 / (1 + np.exp(-x)); ds = s * (1 - s); ### END CODE HERE ### return ds; 12x = np.array([1, 2, 3])print ("sigmoid_derivative(x) = " + str(sigmoid_derivative(x))) sigmoid_derivative(x) = [0.19661193 0.10499359 0.04517666] 1.3 Reshaping arraysTwo common numpy functions used in deep learning are np.shape and np.reshape(). X.shape is used to get the shape (dimension) of a matrix/vector X. X.reshape() is used to reshape X into some other dimension. For example, in computer science, an image is represented by a 3D array of shape (length,height,depth=3). However, when you read an image as the input of an algorithm you convert it to a vector of shape (length∗height∗3,1). In other words, you “unroll”, or reshape, the 3D array into a 1D vector. Exercise: Implement image2vector() that takes an input of shape(length, height, 3) and returns a vector of shape(length * height * 3, 1). For example, if you would like to reshape an array v of shape (a, b, c) into a vector of shape (a*b,c) you would do:1v = v.reshape(v.shape[0] * v.shape[1], v.shape[2]) # v.shape[0] = a ; v.shape[1] = b ; v.shape[2] = c; Please don’t hardcode the dimensions of image as a constant. Instead look up the quantities you need with image.shape[0], etc. 123456789101112131415# GRADED FUNCTION: image2vectordef image2vector(image): """ Argument: image -- a numpy array of shape (length, height, depth) Returns: v -- a vector of shape (length*height*depth, 1) """ ### START CODE HERE ### (≈ 1 line of code) v = image.reshape(image.shape[0] * image.shape[1] * image.shape[2], 1); ### END CODE HERE ### return v; 123import numpy as np;image = np.random.rand(3,3,3);image2vector(image) array([[0.51571749], [0.44538647], [0.53561213], [0.1172449 ], [0.89271698], [0.30177735], [0.61210542], [0.5702647 ], [0.14097692], [0.30515161], [0.28477894], [0.69207277], [0.74081467], [0.36062328], [0.3069694 ], [0.90502389], [0.21609838], [0.92749893], [0.80694438], [0.98316829], [0.87806386], [0.41072457], [0.74295058], [0.30800667], [0.85316743], [0.46848715], [0.56193027]]) 1.4 Normalizing rowsAnother common technique we use in Machine Learning and Deep Learning is to normalize our data. It often leads to a better performance because gradient descent converges faster after normalization. Here, by normalization we mean changing x to ${x\over||x||}$ (dividing each row vector of x by its norm). For example, if 123import numpy as np;x = np.random.randint(1,10,(2,3));print(x); [[9 5 8] [5 3 6]] then$$| x| = np.linalg.norm(x, axis = 0, keepdims = True) \tag{3}$$ and$$x_normalized = \frac{x}{| x|} \tag{4}$$ 1234567import numpy as np;x = np.random.randint(1,10,(2,3));print(x);x_norm = np.linalg.norm(x, axis = 0, keepdims = True);print(x_norm);x_normalized = x / x_norm;print(x_normalized); [[6 4 3] [8 2 1]] [[10. 4.47213595 3.16227766]] [[0.6 0.89442719 0.9486833 ] [0.8 0.4472136 0.31622777]] Note that you can divide matrices of different sizes and it works fine: this is called broadcasting and you’re going to learn about it in part 5. Exercise: Implement normalizeRows() to normalize the rows of a matrix. After applying this function to an input matrix x, each row of x should be a vector of unit length (meaning length 1). 12345678910111213141516171819202122# GRADED FUNCTION: normalizeRowsdef normalizeRows(x): """ Implement a function that normalizes each row of the matrix x (to have unit length). Argument: x -- A numpy matrix of shape (n, m) Returns: x -- The normalized (by row) numpy matrix. You are allowed to modify x. """ ### START CODE HERE ### (≈ 2 lines of code) # Compute x_norm as the norm 2 of x. Use np.linalg.norm(..., ord = 2, axis = ..., keepdims = True) x_norm = np.linalg.norm(x, axis=1, keepdims = True); # Divide x by its norm. x = x / x_norm; ### END CODE HERE ### return x; 123456import numpy as np;x = np.array([ [0, 3, 4], [9, 0, 16]])print(x);print(normalizeRows(x)); [[ 0 3 4] [ 9 0 16]] [[0. 0.6 0.8 ] [0.49026124 0. 0.87157554]] Note:In normalizeRows(), you can try to print the shapes of x_norm and x, and then rerun the assessment. You’ll find out that they have different shapes. This is normal given that x_norm takes the norm of each row of x. So x_norm has the same number of rows but only 1 column. So how did it work when you divided x by x_norm? This is called broadcasting and we’ll talk about it now! 1.5 Broadcasting and the softmax functionA very important concept to understand in numpy is “broadcasting”. It is very useful for performing mathematical operations between arrays of different shapes. For the full details on broadcasting, you can read the official broadcasting documentation. Exercise: Implement a softmax function using numpy. You can think of softmax as a normalizing function used when your algorithm needs to classify two or more classes. You will learn more about softmax in the second course of this specialization. Instructions:$$softmax(x) = softmax\begin{bmatrix} x_{11} &amp; x_{12} &amp; x_{13} &amp; \dots &amp; x_{1n} \\ x_{21} &amp; x_{22} &amp; x_{23} &amp; \dots &amp; x_{2n} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ x_{m1} &amp; x_{m2} &amp; x_{m3} &amp; \dots &amp; x_{mn}\end{bmatrix} = \begin{bmatrix} \frac{e^{x_{11}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{12}}}{\sum_{j}e^{x_{1j}}} &amp; \frac{e^{x_{13}}}{\sum_{j}e^{x_{1j}}} &amp; \dots &amp; \frac{e^{x_{1n}}}{\sum_{j}e^{x_{1j}}} \\ \frac{e^{x_{21}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{22}}}{\sum_{j}e^{x_{2j}}} &amp; \frac{e^{x_{23}}}{\sum_{j}e^{x_{2j}}} &amp; \dots &amp; \frac{e^{x_{2n}}}{\sum_{j}e^{x_{2j}}} \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\\frac{e^{x_{m1}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m2}}}{\sum_{j}e^{x_{mj}}} &amp; \frac{e^{x_{m3}}}{\sum_{j}e^{x_{mj}}} &amp; \dots &amp; \frac{e^{x_{mn}}}{\sum_{j}e^{x_{mj}}}\end{bmatrix} \\= \begin{pmatrix} softmax\text{(first row of x)} \\ softmax\text{(second row of x)} \\ … \\ softmax\text{(last row of x)} \\\end{pmatrix}$$ 12345678910111213141516171819202122232425262728# GRADED FUNCTION: softmaximport numpy as np;def softmax(x): """Calculates the softmax for each row of the input x. Your code should work for a row vector and also for matrices of shape (n, m). Argument: x -- A numpy matrix of shape (n,m) Returns: s -- A numpy matrix equal to the softmax of x, of shape (n,m) """ ### START CODE HERE ### (≈ 3 lines of code) # Apply exp() element-wise to x. Use np.exp(...). x_exp = np.exp(x); # Create a vector x_sum that sums each row of x_exp. Use np.sum(..., axis = 1, keepdims = True). x_norm = np.linalg.norm(x_exp, axis = 1, keepdims = True); # Compute softmax(x) by dividing x_exp by x_sum. It should automatically use numpy broadcasting. s = x_exp / x_norm; ### END CODE HERE ### return s; 1234x = np.array([ [9, 2, 5, 0, 0], [7, 5, 0, 0 ,0]])print(softmax(x)); [[9.99831880e-01 9.11728660e-04 1.83125597e-02 1.23389056e-04 1.23389056e-04] [9.90964875e-01 1.34112512e-01 9.03642998e-04 9.03642998e-04 9.03642998e-04]] Note: If you print the shapes of x_exp, x_sum and s above and rerun the assessment cell, you will see that x_sum is of shape (2,1) while x_exp and s are of shape (2,5). x_exp/x_sum works due to python broadcasting. What you need to remember: np.exp(x) works for any np.array x and applies the exponential function to every coordinate the sigmoid function and its gradient image2vector is commonly used in deep learning np.reshape is widely used. In the future, you’ll see that keeping your matrix/vector dimensions straight will go toward eliminating a lot of bugs. numpy has efficient built-in functions broadcasting is extremely useful 2 VectorizationIn deep learning, you deal with very large datasets. Hence, a non-computationally-optimal function can become a huge bottleneck in your algorithm and can result in a model that takes ages to run. To make sure that your code is computationally efficient, you will use vectorization. For example, try to tell the difference between the following implementations of the dot/outer/elementwise product. 123456789101112131415161718192021222324252627282930313233343536373839import timex1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]### CLASSIC DOT PRODUCT OF VECTORS IMPLEMENTATION ###tic = time.process_time()dot = 0for i in range(len(x1)): dot+= x1[i] * x2[i]toc = time.process_time()print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### CLASSIC OUTER PRODUCT IMPLEMENTATION ###tic = time.process_time()outer = np.zeros((len(x1),len(x2))) # we create a len(x1)*len(x2) matrix with only zerosfor i in range(len(x1)): for j in range(len(x2)): outer[i,j] = x1[i] * x2[j]toc = time.process_time()print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### CLASSIC ELEMENTWISE IMPLEMENTATION ###tic = time.process_time()mul = np.zeros(len(x1))for i in range(len(x1)): mul[i] = x1[i] * x2[i]toc = time.process_time()print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### CLASSIC GENERAL DOT PRODUCT IMPLEMENTATION ###W = np.random.rand(3,len(x1)) # Random 3*len(x1) numpy arraytic = time.process_time()gdot = np.zeros(W.shape[0])for i in range(W.shape[0]): for j in range(len(x1)): gdot[i] += W[i,j] * x1[j]toc = time.process_time()print ("gdot = " + str(gdot) + "\n ----- Computation time = " + str(1000 * (toc - tic)) + "ms") dot = 278 ----- Computation time = 0.0ms outer = [[81. 18. 18. 81. 0. 81. 18. 45. 0. 0. 81. 18. 45. 0. 0.] [18. 4. 4. 18. 0. 18. 4. 10. 0. 0. 18. 4. 10. 0. 0.] [45. 10. 10. 45. 0. 45. 10. 25. 0. 0. 45. 10. 25. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [63. 14. 14. 63. 0. 63. 14. 35. 0. 0. 63. 14. 35. 0. 0.] [45. 10. 10. 45. 0. 45. 10. 25. 0. 0. 45. 10. 25. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [81. 18. 18. 81. 0. 81. 18. 45. 0. 0. 81. 18. 45. 0. 0.] [18. 4. 4. 18. 0. 18. 4. 10. 0. 0. 18. 4. 10. 0. 0.] [45. 10. 10. 45. 0. 45. 10. 25. 0. 0. 45. 10. 25. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.] [ 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]] ----- Computation time = 0.0ms elementwise multiplication = [81. 4. 10. 0. 0. 63. 10. 0. 0. 0. 81. 4. 25. 0. 0.] ----- Computation time = 0.0ms gdot = [19.43421812 18.68022029 16.86207096] ----- Computation time = 0.0ms 1234567891011121314151617181920212223242526x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]### VECTORIZED DOT PRODUCT OF VECTORS ###tic = time.process_time()dot = np.dot(x1,x2)toc = time.process_time()print ("dot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### VECTORIZED OUTER PRODUCT ###tic = time.process_time()outer = np.outer(x1,x2)toc = time.process_time()print ("outer = " + str(outer) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### VECTORIZED ELEMENTWISE MULTIPLICATION ###tic = time.process_time()mul = np.multiply(x1,x2)toc = time.process_time()print ("elementwise multiplication = " + str(mul) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms")### VECTORIZED GENERAL DOT PRODUCT ###tic = time.process_time()dot = np.dot(W,x1)toc = time.process_time()print ("gdot = " + str(dot) + "\n ----- Computation time = " + str(1000*(toc - tic)) + "ms") dot = 278 ----- Computation time = 0.0ms outer = [[81 18 18 81 0 81 18 45 0 0 81 18 45 0 0] [18 4 4 18 0 18 4 10 0 0 18 4 10 0 0] [45 10 10 45 0 45 10 25 0 0 45 10 25 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [63 14 14 63 0 63 14 35 0 0 63 14 35 0 0] [45 10 10 45 0 45 10 25 0 0 45 10 25 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [81 18 18 81 0 81 18 45 0 0 81 18 45 0 0] [18 4 4 18 0 18 4 10 0 0 18 4 10 0 0] [45 10 10 45 0 45 10 25 0 0 45 10 25 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]] ----- Computation time = 0.0ms elementwise multiplication = [81 4 10 0 0 63 10 0 0 0 81 4 25 0 0] ----- Computation time = 0.0ms gdot = [19.43421812 18.68022029 16.86207096] ----- Computation time = 0.0ms As you may have noticed, the vectorized implementation is much cleaner and more efficient. For bigger vectors/matrices, the differences in running time become even bigger. Note that np.dot() performs a matrix-matrix or matrix-vector multiplication. This is different from np.multiply() and the * operator (which is equivalent to .* in Matlab/Octave), which performs an element-wise multiplication. 2.1 Implement the L1 and L2 loss functionsExercise: Implement the numpy vectorized version of the L1 loss. You may find the function abs(x) (absolute value of x) useful. Reminder: The loss is used to evaluate the performance of your model. The bigger your loss is, the more different your predictions $(\hat{y})$ are from the true values $(y)$. In deep learning, you use optimization algorithms like Gradient Descent to train your model and to minimize the cost. L1 loss is defined as:$$\begin{align} &amp; L_1(\hat{y}, y) = \sum_{i=0}^m|y^{(i)} - \hat{y}^{(i)}| \end{align}\tag{5}$$ 1234567891011121314151617# GRADED FUNCTION: L1import numpy as np;def L1(yhat, y): """ Arguments: yhat -- vector of size m (predicted labels) y -- vector of size m (true labels) Returns: loss -- the value of the L1 loss function defined above """ ### START CODE HERE ### (≈ 1 line of code) loss = np.sum(np.abs(y - yhat)); ### END CODE HERE ### return loss; 1234yhat = np.random.randn(1,5);print(yhat);y = np.array([1, 0, 0, 1, 1]);print("L1 = " + str(L1(yhat,y))); [[ 0.17368857 -1.46853016 0.27681907 -0.05448256 0.9010455 ]] L1 = 3.7250977210513185 Exercise: Implement the numpy vectorized version of the L2 loss. There are several way of implementing the L2 loss but you may find the function np.dot() useful. As a reminder, if $x = [x_1, x_2, …, x_n]$ , then $np.dot(x,x) = \sum_{j=0}^n x_j^{2}$ . L2 loss is defined as$$\begin{align} &amp; L_2(\hat{y},y) = \sum_{i=0}^m(y^{(i)} - \hat{y}^{(i)})^2 \end{align}\tag{6}$$ 1234567891011121314151617# GRADED FUNCTION: L2import numpy as np;def L2(yhat, y): """ Arguments: yhat -- vector of size m (predicted labels) y -- vector of size m (true labels) Returns: loss -- the value of the L2 loss function defined above """ ### START CODE HERE ### (≈ 1 line of code) loss =np.dot(y - yhat, y - yhat); ### END CODE HERE ### return loss; 123yhat = np.array([.9, 0.2, 0.1, .4, .9])y = np.array([1, 0, 0, 1, 1])print(L2(yhat,y)); 0.43 What to remember: Vectorization is very important in deep learning. It provides computational efficiency and clarity. You have reviewed the L1 and L2 loss. You are familiar with many numpy functions such as np.sum, np.dot, np.multiply, np.maximum, etc… Part 2： Logistic Regression with a Neural Network mindsetYou will learn to: Build the general architecture of a learning algorithm, including: Initializing parameters Calculating the cost function and its gradient Using an optimization algorithm (gradient descent) Gather all three functions above into a main model function, in the right order. 1. PackagesFirst, let’s run the cell below to import all the packages that you will need during this assignment. numpy is the fundamental package for scientific computing with Python. h5py is a common package to interact with a dataset that is stored on an H5 file. matplotlib is a famous library to plot graphs in Python. PIL and scipy are used here to test your model with your own picture at the end. 123456789import numpy as npimport matplotlib.pyplot as pltimport h5pyimport scipyfrom PIL import Imagefrom scipy import ndimagefrom lr_utils import load_dataset% matplotlib inline 2. Overview of the Problem setProblem Statement: You are given a dataset (“data.h5”) containing: a training set of m_train images labeled as cat (y=1) or non-cat (y=0) a test set of m_test images labeled as cat or non-cat each image is of shape (num_px, num_px, 3) where 3 is for the 3 channels (RGB). Thus, each image is square (height = num_px) and (width = num_px). You will build a simple image-recognition algorithm that can correctly classify pictures as cat or non-cat. Let’s get more familiar with the dataset. Load the data by running the following code. 12# Loading the data (cat/non-cat)train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset(); We added “_orig” at the end of image datasets (train and test) because we are going to preprocess them. After preprocessing, we will end up with train_set_x and test_set_x (the labels train_set_y and test_set_y don’t need any preprocessing). Each line of your train_set_x_orig and test_set_x_orig is an array representing an image. You can visualize an example by running the following code. Feel free also to change the index value and re-run to see other images. 1234# Example of a pictureindex = 25plt.imshow(train_set_x_orig[index])print("y = " + str(train_set_y[:, index]) + ", it's a '" + classes[np.squeeze(train_set_y[:, index])].decode("utf-8") + "' picture."); y = [1], it&apos;s a &apos;cat&apos; picture. Many software bugs in deep learning come from having matrix/vector dimensions that don’t fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. Exercise: Find the values for: m_train (number of training examples) m_test (number of test examples) num_px (= height = width of a training image)Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0]. 1234567891011121314### START CODE HERE ### (≈ 3 lines of code)m_train = train_set_x_orig.shape[0];m_test = test_set_x_orig.shape[0];num_px = train_set_x_orig.shape[1];### END CODE HERE ###print ("Number of training examples: m_train = " + str(m_train));print ("Number of testing examples: m_test = " + str(m_test));print ("Height/Width of each image: num_px = " + str(num_px));print ("Each image is of size: (" + str(num_px) + ", " + str(num_px) + ", 3)");print ("train_set_x shape: " + str(train_set_x_orig.shape));print ("train_set_y shape: " + str(train_set_y.shape));print ("test_set_x shape: " + str(test_set_x_orig.shape));print ("test_set_y shape: " + str(test_set_y.shape)); Number of training examples: m_train = 209 Number of testing examples: m_test = 50 Height/Width of each image: num_px = 64 Each image is of size: (64, 64, 3) train_set_x shape: (209, 64, 64, 3) train_set_y shape: (1, 209) test_set_x shape: (50, 64, 64, 3) test_set_y shape: (1, 50) For convenience, you should now reshape images of shape (num_px, num_px, 3) in a numpy-array of shape (num_px ∗ num_px ∗ 3, 1). After this, our training (and test) dataset is a numpy-array where each column represents a flattened image. There should be m_train (respectively m_test) columns. Exercise: Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px ∗ num_px ∗ 3, 1). A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape (b∗c∗d, a) is to use:1X_flatten = X.reshape(X.shape[0], -1).T # X.T is the transpose of X 123456789101112# Reshape the training and test examples### START CODE HERE ### (≈ 2 lines of code)train_set_x_flatten = train_set_x_orig.reshape(m_train, -1).T;test_set_x_flatten = test_set_x_orig.reshape(m_test, -1).T;### END CODE HERE ###print ("train_set_x_flatten shape: " + str(train_set_x_flatten.shape));print ("train_set_y shape: " + str(train_set_y.shape));print ("test_set_x_flatten shape: " + str(test_set_x_flatten.shape));print ("test_set_y shape: " + str(test_set_y.shape));print ("sanity check after reshaping: " + str(train_set_x_flatten[0:5,0])); train_set_x_flatten shape: (12288, 209) train_set_y shape: (1, 209) test_set_x_flatten shape: (12288, 50) test_set_y shape: (1, 50) sanity check after reshaping: [17 31 56 22 33] To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255. One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel). Let’s standardize our dataset. 12train_set_x = train_set_x_flatten / 255;test_set_x = test_set_x_flatten / 255; What you need to remember: Common steps for pre-processing a new dataset are: Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, …) Reshape the datasets such that each example is now a vector of size (num_px num_px 3, 1) “Standardize”the data 3. General Architecture of the learning algorithmIt’s time to design a simple algorithm to distinguish cat images from non-cat images. You will build a Logistic Regression, using a Neural Network mindset. The following Figure explains why Logistic Regression is actually a very simple Neural Network! Mathematical expression of the algorithm: For one example: $x^{(i)}$ $$z^{(i)} = w^T x^{(i)} + b \tag{1}$$$$\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\tag{2}$$$$\mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} \log(a^{(i)}) - (1-y^{(i)} ) \log(1-a^{(i)})\tag{3}$$The cost is then computed by summing over all training examples:$$J = \frac{1}{m} \sum_{i=1}^m \mathcal{L}(a^{(i)}, y^{(i)})\tag{4}$$ Key steps: In this exercise, you will carry out the following steps: Initialize the parameters of the model Learn the parameters for the model by minimizing the cost Use the learned parameters to make predictions (on the test set) Analyse the results and conclude 4. Building the parts of our algorithmThe main steps for building a Neural Network are: Define the model structure (such as number of input features) Initialize the model’s parameters Loop: Calculate current loss (forward propagation) Calculate current gradient (backward propagation) Update parameters (gradient descent) You often build 1-3 separately and integrate them into one function we call model(). 4.1 Helper functionsExercise: Using your code from “Python Basics”, implement sigmoid(). As you’ve seen in the figure above, you need to compute $sigmoid(w^Tx+b)=\frac{1}{1 + e^{−(w^Tx+b)}}$ to make predictions. Use np.exp(). 123456789101112131415161718# GRADED FUNCTION: sigmoiddef sigmoid(z): """ Compute the sigmoid of z Arguments: z -- A scalar or numpy array of any size. Return: s -- sigmoid(z) """ ### START CODE HERE ### (≈ 1 line of code) s = 1 / (1 + np.exp(-z)); ### END CODE HERE ### return s; 1print("sigmoid([0, 2]) = " + str(sigmoid(np.array([0,2])))); sigmoid([0, 2]) = [0.5 0.88079708] 4.2 Initializing parametersExercise: Implement parameter initialization in the cell below. You have to initialize $w$ as a vector of zeros. If you don’t know what numpy function to use, look up np.zeros() in the Numpy library’s documentation. 1234567891011121314151617181920212223# GRADED FUNCTION: initialize_with_zerosdef initialize_with_zeros(dim): """ This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0. Argument: dim -- size of the w vector we want (or number of parameters in this case) Returns: w -- initialized vector of shape (dim, 1) b -- initialized scalar (corresponds to the bias) """ ### START CODE HERE ### (≈ 1 line of code) w = np.zeros((dim, 1)); b = 0; ### END CODE HERE ### assert(w.shape == (dim, 1)); assert(isinstance(b, float) or isinstance(b, int)); return w, b; 1234dim = 2;w, b = initialize_with_zeros(dim);print ("w = " + str(w));print ("b = " + str(b)); w = [[0.] [0.]] b = 0 For image inputs, w will be of shape $(num_px \times num_px \times 3, 1)$. 4.3. Forward and Backward propagationNow that your parameters are initialized, you can do the “forward” and “backward” propagation steps for learning the parameters. Exercise: Implement a function propagate() that computes the cost function and its gradient. Hints: Forward Propagation: You get $X$ You compute $A = \sigma(w^T X + b) = (a^{(0)}, a^{(1)}, …, a^{(m-1)}, a^{(m)})$ You calculate the cost function $J = -\frac{1}{m}\sum_{i=1}^{m}y^{(i)}\log(a^{(i)})+(1-y^{(i)})\log(1-a^{(i)})$ Here are the two formulas you will be using:$$\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T\tag{5}$$$$\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})\tag{6}$$ 1234567891011121314151617181920212223242526272829303132333435363738394041424344# GRADED FUNCTION: propagatedef propagate(w, b, X, Y): """ Implement the cost function and its gradient for the propagation explained above Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) =&gt; w = n * 1 b -- bias, a scalar =&gt; b = 1 * 1 X -- data of size (num_px * num_px * 3, number of examples) =&gt; X = n * m Y -- true "label" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples) =&gt; Y = 1 * m Return: cost -- negative log-likelihood cost for logistic regression dw -- gradient of the loss with respect to w, thus same shape as w db -- gradient of the loss with respect to b, thus same shape as b Tips: - Write your code step by step for the propagation. np.log(), np.dot() """ m = X.shape[1] # FORWARD PROPAGATION (FROM X TO COST) ### START CODE HERE ### (≈ 2 lines of code) a = sigmoid(np.dot(w.T, X) + b); # compute activation cost = - 1 / m * (np.dot(Y, np.log(a).T) + np.dot(1 - Y, np.log(1 - a).T)); # compute cost ### END CODE HERE ### # BACKWARD PROPAGATION (TO FIND GRAD) ### START CODE HERE ### (≈ 2 lines of code) dw = 1 / m * np.dot(X,(a - Y).T); db = 1 / m * np.sum(a - Y); ### END CODE HERE ### assert(dw.shape == w.shape); assert(db.dtype == float); cost = np.squeeze(cost); assert(cost.shape == ()); grads = &#123;"dw": dw, "db": db&#125;; return grads, cost; 123456import numpy as np;w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]]);grads, cost = propagate(w, b, X, Y);print ("dw = " + str(grads["dw"]));print ("db = " + str(grads["db"]));print ("cost = " + str(cost)); dw = [[0.99845601] [2.39507239]] db = 0.001455578136784208 cost = 5.801545319394553 4.4. Optimization You have initialized your parameters. You are also able to compute a cost function and its gradient. Now, you want to update the parameters using gradient descent. Exercise: Write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\theta$, the update rule is $\theta=\theta−\alpha d\theta$, where $\alpha$ is the learning rate. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061# GRADED FUNCTION: optimizedef optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False): """ This function optimizes w and b by running a gradient descent algorithm Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of shape (num_px * num_px * 3, number of examples) Y -- true "label" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples) num_iterations -- number of iterations of the optimization loop learning_rate -- learning rate of the gradient descent update rule print_cost -- True to print the loss every 100 steps Returns: params -- dictionary containing the weights w and bias b grads -- dictionary containing the gradients of the weights and bias with respect to the cost function costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve. Tips: You basically need to write down two steps and iterate through them: 1) Calculate the cost and the gradient for the current parameters. Use propagate(). 2) Update the parameters using gradient descent rule for w and b. """ costs = [] for i in range(num_iterations): # Cost and gradient calculation (≈ 1-4 lines of code) ### START CODE HERE ### grads, cost = propagate(w, b, X, Y); ### END CODE HERE ### # Retrieve derivatives from grads dw = grads["dw"]; db = grads["db"]; # update rule (≈ 2 lines of code) ### START CODE HERE ### w -= learning_rate * dw; b -= learning_rate * db; ### END CODE HERE ### # Record the costs if i % 100 == 0: costs.append(cost) # Print the cost every 100 training examples if print_cost and i % 100 == 0: print ("Cost after iteration %i: %f" %(i, cost)) params = &#123;"w": w, "b": b&#125; grads = &#123;"dw": dw, "db": db&#125; return params, grads, costs 123456params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = False);print ("w = " + str(params["w"]))print ("b = " + str(params["b"]))print ("dw = " + str(grads["dw"]))print ("db = " + str(grads["db"])) w = [[-0.08608643] [ 0.10971233]] b = -0.1442742664803268 dw = [[0.12311093] [0.13629247]] db = -0.14923915884638042 Exercise: The previous function will output the learned $w$ and $b$. We are able to use w and b to predict the labels for a dataset $X$. Implement the predict() function. There is two steps to computing predictions: Calculate $\hat{Y}=A=σ(w^TX+b)$ Convert the entries of a into 0 (if activation &lt;= 0.5) or 1 (if activation &gt; 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this). 12345678910111213141516171819202122232425262728293031323334353637# GRADED FUNCTION: predictdef predict(w, b, X): ''' Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b) Arguments: w -- weights, a numpy array of size (num_px * num_px * 3, 1) b -- bias, a scalar X -- data of size (num_px * num_px * 3, number of examples) Returns: Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X ''' m = X.shape[1] Y_prediction = np.zeros((1,m)) w = w.reshape(X.shape[0], 1) # Compute vector "A" predicting the probabilities of a cat being present in the picture ### START CODE HERE ### (≈ 1 line of code) A = sigmoid(np.dot(w.T, X) + b); ### END CODE HERE ### for i in range(A.shape[1]): # Convert probabilities A[0,i] to actual predictions p[0,i] ### START CODE HERE ### (≈ 4 lines of code) if A[0,i] &gt; 0.5: Y_prediction[0,i] = 1; else: Y_prediction[0,i] = 0; ### END CODE HERE ### assert(Y_prediction.shape == (1, m)); return Y_prediction; 1234w = np.array([[0.1124579],[0.23106775]]);b = -0.3;X = np.array([[1.,-1.1,-3.2],[1.2,2.,0.1]]);print ("predictions = " + str(predict(w, b, X))); predictions = [[1. 1. 0.]] What to remember: You’ve implemented several functions that: Initialize (w,b) Optimize the loss iteratively to learn parameters (w,b): computing the cost and its gradient updating the parameters using gradient descent Use the learned (w,b) to predict the labels for a given set of examples 5. Merge all functions into a modelYou will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order. Exercise: Implement the model function. Use the following notation: Y_prediction for your predictions on the test set Y_prediction_train for your predictions on the train set w, costs, grads for the outputs of optimize() 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051# GRADED FUNCTION: modeldef model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False): """ Builds the logistic regression model by calling the function you've implemented previously Arguments: X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train) Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train) X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test) Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test) num_iterations -- hyperparameter representing the number of iterations to optimize the parameters learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize() print_cost -- Set to true to print the cost every 100 iterations Returns: d -- dictionary containing information about the model. """ ### START CODE HERE ### # initialize parameters with zeros (≈ 1 line of code) w, b = initialize_with_zeros(X_train.shape[0]); # Gradient descent (≈ 1 line of code) params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost); # Retrieve parameters w and b from dictionary "parameters" w = params["w"]; b = params["b"]; # Predict test/train set examples (≈ 2 lines of code) Y_prediction_train = predict(w, b, X_train); Y_prediction_test = predict(w, b, X_test); ### END CODE HERE ### # Print train/test Errors print("train accuracy: &#123;&#125; %".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100)) print("test accuracy: &#123;&#125; %".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100)) d = &#123;"costs": costs, "Y_prediction_test": Y_prediction_test, "Y_prediction_train" : Y_prediction_train, "w" : w, "b" : b, "learning_rate" : learning_rate, "num_iterations": num_iterations&#125; return d; 1d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 2000, learning_rate = 0.005, print_cost = True); Cost after iteration 0: 0.693147 Cost after iteration 100: 0.584508 Cost after iteration 200: 0.466949 Cost after iteration 300: 0.376007 Cost after iteration 400: 0.331463 Cost after iteration 500: 0.303273 Cost after iteration 600: 0.279880 Cost after iteration 700: 0.260042 Cost after iteration 800: 0.242941 Cost after iteration 900: 0.228004 Cost after iteration 1000: 0.214820 Cost after iteration 1100: 0.203078 Cost after iteration 1200: 0.192544 Cost after iteration 1300: 0.183033 Cost after iteration 1400: 0.174399 Cost after iteration 1500: 0.166521 Cost after iteration 1600: 0.159305 Cost after iteration 1700: 0.152667 Cost after iteration 1800: 0.146542 Cost after iteration 1900: 0.140872 train accuracy: 99.04306220095694 % test accuracy: 70.0 % Comment: Training accuracy is close to 100%. This is a good sanity check: your model is working and has high enough capacity to fit the training data. Test error is 68%. It is actually not bad for this simple model, given the small dataset we used and that logistic regression is a linear classifier. But no worries, you’ll build an even better classifier next week! Also, you see that the model is clearly overfitting the training data. Later in this specialization you will learn how to reduce overfitting, for example by using regularization. Using the code below (and changing the index variable) you can look at predictions on pictures of the test set. 12345# Example of a picture that was wrongly classified.index = 15;plt.imshow(test_set_x[:,index].reshape((num_px, num_px, 3)));print ("y = " + str(test_set_y[0, index]) + ", you predicted that it is a \"" + classes[int(d["Y_prediction_test"][0,index])].decode("utf-8") + "\" picture.");#print("y = " + str(train_set_y[:, index]) + ", it's a '" + classes[np.squeeze(train_set_y[:, index])].decode("utf-8") + "' picture."); y = 1, you predicted that it is a &quot;cat&quot; picture. Let’s also plot the cost function and the gradients. 1234567# Plot learning curve (with costs)costs = np.squeeze(d['costs']);plt.plot(costs);plt.ylabel('cost');plt.xlabel('iterations (per hundreds)');plt.title("Learning rate =" + str(d["learning_rate"]));plt.show(); Interpretation:You can see the cost decreasing. It shows that the parameters are being learned. However, you see that you could train the model even more on the training set. Try to increase the number of iterations in the cell above and rerun the cells. You might see that the training set accuracy goes up, but the test set accuracy goes down. This is called overfitting. 6. Further analysis (optional/ungraded exercise)Congratulations on building your first image classification model. Let’s analyze it further, and examine possible choices for the learning rate $α$. Choice of learning rate Reminder:In order for Gradient Descent to work you must choose the learning rate wisely. The learning rate $α$ determines how rapidly we update the parameters. If the learning rate is too large we may “overshoot” the optimal value. Similarly, if it is too small we will need too many iterations to converge to the best values. That’s why it is crucial to use a well-tuned learning rate. Let’s compare the learning curve of our model with several choices of learning rates. Run the cell below. This should take about 1 minute. Feel free also to try different values than the three we have initialized the learning_rates variable to contain, and see what happens. 1234567891011121314151617learning_rates = [0.01, 0.001, 0.0001];models = &#123;&#125;;for i in learning_rates: print ("learning rate is: " + str(i)); models[str(i)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations = 1500, learning_rate = i, print_cost = False); print ('\n' + "-------------------------------------------------------" + '\n');for i in learning_rates: plt.plot(np.squeeze(models[str(i)]["costs"]), label= str(models[str(i)]["learning_rate"]));plt.ylabel('cost');plt.xlabel('iterations');legend = plt.legend(loc='upper center', shadow=True);frame = legend.get_frame();frame.set_facecolor('0.50');plt.show(); learning rate is: 0.01 train accuracy: 99.52153110047847 % test accuracy: 68.0 % ------------------------------------------------------- learning rate is: 0.001 train accuracy: 88.99521531100478 % test accuracy: 64.0 % ------------------------------------------------------- learning rate is: 0.0001 train accuracy: 68.42105263157895 % test accuracy: 36.0 % ------------------------------------------------------- Interpretation: Different learning rates give different costs and thus different predictions results. If the learning rate is too large (0.01), the cost may oscillate up and down. It may even diverge (though in this example, using 0.01 still eventually ends up at a good value for the cost). A lower cost doesn’t mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy. In deep learning, we usually recommend that you: Choose the learning rate that better minimizes the cost function. If your model overfits, use other techniques to reduce overfitting. (We’ll talk about this in later videos.) 7. Test with your own image (optional/ungraded exercise)Congratulations on finishing this assignment. You can use your own image and see the output of your model. To do that: Click on “File” in the upper bar of this notebook, then click “Open” to go on your Coursera Hub. Add your image to this Jupyter Notebook’s directory, in the “images” folder Change your image’s name in the following code Run the code and check if the algorithm is right (1 = cat, 0 = non-cat)! 12345678910111213## START CODE HERE ## (PUT YOUR IMAGE NAME) my_image = "isacatornot.jpg"; # change this to the name of your image file ## END CODE HERE ### We preprocess the image to fit your algorithm.fname = "images/" + my_image;image = np.array(ndimage.imread(fname, flatten=False));my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((1, num_px*num_px*3)).T;my_predicted_image = predict(d["w"], d["b"], my_image);plt.imshow(image);print("y = " + str(np.squeeze(my_predicted_image)) + ", your algorithm predicts a \"" + classes[int(np.squeeze(my_predicted_image)),].decode("utf-8") + "\" picture."); y = 1.0, your algorithm predicts a &quot;cat&quot; picture. What to remember from this assignment: Preprocessing the dataset is important. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model(). Tuning the learning rate (which is an example of a “hyperparameter”) can make a big difference to the algorithm. You will see more examples of this later in this course!]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>neural-networks-deep-learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04_deep-neural-network]]></title>
    <url>%2F2018%2F02%2F04%2F04_deep-neural-networks%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal note at the 4th week after studying the course neural-networks-deep-learning and the copyright belongs to deeplearning.ai. 01_deep-neural-networkWelcome to the fourth week of this course. By now, you’ve seen four promulgation and back promulgation in the context of a neural network, with a single hidden layer, as well as logistic regression, and you’ve learned about vectorization, and when it’s important to initialize the ways randomly. If you’ve done the past couple weeks homework, you’ve also implemented and seen some of these ideas work for yourself. So by now, you’ve actually seen most of the ideas you need to implement a deep neural network. What we’re going to do this week, is take those ideas and put them together so that you’ll be able to implement your own deep neural network. Because this week’s problem exercise is longer, it just has been more work, I’m going to keep the videos for this week shorter as you can get through the videos a little bit more quickly, and then have more time to do a significant problem exercise at then end, which I hope will leave you having thoughts deep in neural network, that if you feel proud of. but over the last several years the AI, on the machine learning community, has realized that there are functions that very deep neural networks can learn that shallower models are often unable to. Although for any given problem, it might be hard to predict in advance exactly how deep in your network you would want. So it would be reasonable to try logistic regression, try one and then two hidden layers, and view the number of hidden layers as another hyper parameter that you could try a variety of values of, and evaluate on all that across validation data, or on your development set. See more about that later as well. 02_forward-propagation-in-a-deep-networkIn the last video we distract what is the deep neural network and also talked about the notation we use to describe such networks in this video you see how you can perform for propagation in a deep network. One of the ways to increase your odds of having bug-free implementation is to think very systematic and carefully about the matrix dimensions you’re working with so when I’m trying to develop my own code I often pull a piece of paper and just think carefully through so the dimensions of the matrix I’m working with let’s see how you could do that in the next video. 03_getting-your-matrix-dimensions-rightWhen implementing a deep neural network, one of the debugging tools I often use to check the correctness of my code is to pull a piece of paper, and just work through the dimensions and matrix I’m working with. So let me show you how to do that, since I hope this will make it easier for you to implement your deep nets as well. one training example $\because \text{the dimensions of x}(a^{[0]}) \text{: } (n^{[0]}, 1)$ $\therefore $$W^{[l]}: (n^{[l]}, n^{[l-1]})$ $b^{[l]}: (n^{[l]}, 1)$ $dW^{[l]}: (n^{[l]}, n^{[l-1]})$ $db^{[l]}: (n^{[l]}, 1)$ $dz^{[l]}: (n^{[l]}, 1)$ $da^{[l]}: (n^{[l]}, 1)$ is the same shape of $z^{[l]}$. m training examples $\because \text{the dimensions of X}(A^{[0]}) \text{: } (n^{[0]}, m)$ $\therefore$ $W^{[l]} : (n^{[l]}, n^{[l-1]})$ $B^{[l]} : (n^{[l]}, m) $ $dW^{[l]} : (n^{[l]}, n^{[l-1]})$ $dB^{[l]} : (n^{[l]}, m)$ $dZ^{[l]} : (n^{[l]}, m)$ $dA^{[l]} : (n^{[l]}, m) \text{ is the same shape of } Z^{[l]}$ So I hope the little exercise we went through helps clarify the dimensions that the various matrices you’d be working with. When you implement backpropagation for a deep neural network, so long as you work through your code and make sure that all the matrices’ dimensions are consistent. That will usually help, it’ll go some ways toward eliminating some cause of possible bugs. So I hope that exercise for figuring out the dimensions of various matrices you’ll been working with is helpful. When you implement a deep neural network, if you keep straight the dimensions of these various matrices and vectors you’re working with. Hopefully they’ll help you eliminate some cause of possible bugs, it certainly helps me get my code right. So next, we’ve now seen some of the mechanics of how to do forward propagation in a neural network. But why are deep neural networks so effective, and why do they do better than shallow representations? Let’s spend a few minutes in the next video to discuss that. 04_why-deep-representationsWe’ve all been hearing that deep neural networks work really well for a lot of problems, and it’s not just that they need to be big neural networks, is that specifically, they need to be deep or to have a lot of hidden layers. So why is that? Let’s go through a couple examples and try to gain some intuition for why deep networks might work well. To compute $y=x_{1}\oplus x_{2}\oplus x_{3}\oplus \cdots\oplus x_{n}$, the depth of deep neural network is $O(log_{2}^{n})$, the number of activiation units or nodes is $2^{\log_{2}(n)-1} + \cdots + 2 + 1 = 1\cdot \dfrac{1-2^{\log_{2}(n)}}{1-2}=2^{\log_{2}(n)}-1=n-1$. But in one-hidden-layer neural network, the number of activiation units or nodes is $2^{n-1}$. Now, in addition to this reasons for preferring deep neural networks to be roughly on, is I think the other reasons the term deep learning has taken off is just branding. This things just we call neural networks belong to hidden layers, but the phrase deep learning is just a great brand, it’s just so deep. So I think that once that term caught on that really new networks rebranded or new networks with many hidden layers rebranded, help to capture the popular imagination as well. They regard as the PR(public relations) branding deep networks do work well. Sometimes people go overboard and insist on using tons of hidden layers. But when I’m starting out a new problem, I’ll often really start out with even logistic regression then try something with one or two hidden layers and use that as a hyper parameter. Use that as a parameter or hyper parameter that you tune in order to try to find the right depth for your neural network. But over the last several years there has been a trend toward people finding that for some applications, very, very deep neural networks here with maybe many dozens of layers sometimes, can sometimes be the best model for a problem. So that’s it for the intuitions for why deep learning seems to work well. Let’s now take a look at the mechanics of how to implement not just front propagation, but also back propagation. 05_building-blocks-of-deep-neural-networksIn the earlier videos from this week as well as from the videos from the past several weeks you’ve already seen the basic building blocks of board propagation and back propagation the key components you need to implement a deep neural network let’s see how you can put these components together to build a deep net use the network with a few layers. 06_forward-and-backward-propagationIn a previous video you saw the basic blocks of implementing a deep neural network for propagation step for each layer and a corresponding backward propagation step let’s see how you can actually implement these steps. although I have to say you know even today when I implement a learning algorithm sometimes even I’m surprised when my learning algorithm implementation works and it’s because longer complexity of machine learning comes from the data rather than from the lines of code so sometimes you feel like you implement a few lines of code not question what it did but there’s almost magically work and it’s because of all the magic is actually not in the piece of code you write which is often you know not too long it’s not it’s not exactly simple but there’s not you know 10,000 100,000 lines of code but you feed it so much data that sometimes even though I work the machine only for a long time sometimes it’s so you know surprises me a bit when my learning algorithm works because lots of complexity of your learning algorithm comes from the data rather than necessarily from your writing you know thousands and thousands of lines of code all right so that’s um how do you implement deep neural networks and again this will become more concrete when you’ve done the programming exercise before moving on I want to discuss in the next video want to discuss hyper parameters and parameters it turns out that when you’re training deep nets being able to organize your hyper params as well will help you be more efficient in developing your networks in the next video let’s talk about exactly what that means. 07_parameters-vs-hyperparametersBeing effective in developing your deep neural Nets requires that you not only organize your parameters well but also your hyper parameters, so what are hyper parameters. so when you’re training a deep net for your own application you find that there may be a lot of possible settings for the hyper parameters that you need to just try out so applied deep learning today is a very empirical process where often you might have an idea.For example you might have an idea for the best value for the learning rate you might say well maybe alpha equals 0.01 I want to try that then you implemented try it out and then see how that works and then based on that outcome you might say you know what I’ve changed online I want to increase the learning rate to 0.05 and so if you’re not sure what’s the best value for the learning ready-to-use you might try one value of the learning rate alpha and see their cost function j go down like this then you might try a larger value for the learning rate alpha and see the cost function blow up and diverge then you might try another version and see it go down really fast it’s inverse to higher value you might try another version and see it you know see the cost function J do that then I’ll be China so the values you might say okay looks like this the value of alpha gives me a pretty fast learning and allows me to converge to a lower cost function jennice I’m going to use this value of alpha. you saw in a previous slide that there are a lot of different hybrid parameters and it turns out that when you’re starting on the new application I should find it very difficult to know in advance exactly what’s the best value of the hyper parameters so what often happen is you just have to try out many different values and go around this cycle your trial some value really try five hidden layers with this many number of hidden units implement that see if it works and then iterate so the title of this slide is that apply deep learning is very empirical process and empirical process is maybe a fancy way of saying you just have to try a lot of things and see what works. another effect I’ve seen is that deep learning today is applied to so many problems ranging from computer vision to speech recognition to natural language processing to a lot of structured data applications such as maybe a online advertising or web search or product recommendations and so on and what I’ve seen is that first I’ve seen researchers from one discipline any one of these try to go to a different one and sometimes the intuitions about hyper parameters carries over and sometimes it doesn’t so I often advise people especially when starting on a new problem to just try out a range of values and see what works and then the next course we’ll see a systematic way for trying out a range of values all right and second even if you’re working on one application for a long time, you know, maybe you’re working on online advertising. As you make progress on the problem, it is quite possible there the best value for the learning rate, a number of hidden units and so on might change, so even if you tune your system to the best value of hyper parameters to daily as possible you find that the best value might change a year from now. Maybe because the computer infrastructure I’d be you know CPUs or the type of GPU running on or something has changed, but so maybe one rule of thumb is you know every now and then maybe every few months if you’re working on a problem for an extended period of time for many years just try a few values for the hyper parameters and double check if there’s a better value for the hyper parameters and as you do. So you slowly gain intuition as well about the hyper parameters that work best for your problems and I know that this might seem like an unsatisfying part of deep learning that you just have to try on all the values for these hyper parameters but maybe this is one area where deep learning research is still advancing and maybe over time we’ll be able to give better guidance for the best hyper parameters to use, but it’s also possible that because CPUs and GPUs and networks and data says are all changing and it is possible that the guidance won’t to converge for some time and you just need to keep trying out different values and evaluate them on a hold on cross-validation set or something and pick the value that works for your problems. So that was a brief discussion of hyper parameters in the second course we’ll also give some suggestions for how to systematically explore the space of hyper parameters but by now you actually have pretty much all the tools you need to do their programming exercise before you do that adjust or share view one more set of ideas which is I often ask what does deep learning have to do the human brain. 08_what-does-this-have-to-do-with-the-brainSo what a deep learning have to do the punchline I would say not a whole lot but let’s take a quick look at why people keep making the analogy between deep learning and the human brain.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>neural-networks-deep-learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03_shallow-neural-networks]]></title>
    <url>%2F2018%2F02%2F03%2F03_shallow-neural-networks%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal note at the third week after studying the course neural-networks-deep-learning and the copyright belongs to deeplearning.ai. 01_shallow-neural-network01_neural-networks-overviewwelcome back in this week’s you learn to implement a neural network before diving into the technical details I wanted in this video to give you a quick overview of what you’ll be seeing in this week’s videos so if you don’t follow all the details in this video don’t worry about it we’ll delve in the technical details in the next few videos but for now let’s give a quick overview of how you implement in your network. 02_neural-network-representationYou see me draw a few pictures of neural networks. In this video, we’ll talk about exactly what those pictures means. In other words, exactly what those neural networks that we’ve been drawing represent. And we’ll start with focusing on the case of neural networks with what was called a single hidden layer. Here’s a picture of a neural network. Let’s give different parts ofthese pictures some names. note: The term hidden layer refers to the fact that in the training set, the true values for these nodes in the middle are not observed. That is you don’t see what they should be in the training set. You see what the inputs are. You see what the output should be. But the things in the hidden layer are not seen in the training set. So that kind of explains the name hidden there just because you don’t see it in the training set. One funny thing about notational conventions in neural networks is that this network that you’ve seen here is called a two layer neural network. And the reason is that when we count layers in neural networks, we don’t count the input layer. So the hidden layer is layer one and the output layer is layer two. In our notational convention, we’re calling the input layer layer zero, so technically maybe there are three layers in this neural network, because there’s the input layer, the hidden layer, and the output layer. But in conventional usage, if you read research papers and elsewhere in the course, you see people refer to this particular neural network as a two layer neural network, because we don’t count the input layer as an official layer. the shape of the parameter $w$ between 2 layers is (the_number_of_nodes_in_output_layer, the_number_of_nodes_in_intput_layer), that’s merely a conventional way. And the parameter $b$ is only a constant, that’s a (1, 1) matrix. 03_computing-a-neural-networks-outputIn the last video you saw what a single hidden layer neural network looks like in this video let’s go through the details of exactly how this neural network computers outputs what you see is that is like logistic regression the repeat of all the times. 04_vectorizing-across-multiple-examplesIn the last video, you saw how to compute the prediction on a neural network, given a single training example. In this video, you see how to vectorize across multiple training examples. And the outcome will be quite similar to what you saw for logistic regression. Whereby stacking up different training examples in different columns of the matrix, you’d be able to take the equations you had from the previous video. And with very little modification, change them to make the neural network compute the outputs on all the examples on pretty much all at the same time. So let’s see the details on how to do that. note : the row and column indices of the matrix $A, Z$ respectively correspond to the sequence numbers of the train examples and the nodes(units) in layers. And, the row and column indices of the matrix $X$ separately correspond to the sequence numbers of the train examples and the features of a example. Finally, the row and column indices of the matrix $W$ separately correspond to the sequence numbers of the output units and the input units in the layer. 05_explanation-for-vectorized-implementationIn the previous video, we saw how with your training examples stacked up horizontally in the matrix $X$, you can derive a vectorized implementation for propagation through your neural network. Let’s give a bit more justification for why the equations we wrote down is a correct implementation of vectorizing across multiple examples. So let’s go through part of the propagation calculation for the few examples. 06_activation-functionsWhen you boost a neural network, one of the choices you get to make is what activation functions use independent layers as well as at the output unit of your neural network so far we’ve just been using the sigmoid activation function but sometimes other choices can work much better let’s take a look at some of the options. In the forward propagation steps for the neural network we have these two steps where we use the sigmoid function here so that sigmoid is called an activation function, so in the more general case we can have a different function G of z, $g(z)$, where G could be a nonlinear function that may not be the sigmoid function. So for example the sigmoid function goes between 0 &amp; 1, an activation function that almost always works better than the sigmoid function is the tanh function or the hyperbolic tangent function. The tanh function or the hyperbolic tangent function is actually mathematically a shifted version of the sigmoid function so as a you know sigmoid function just like that but shift it so that it now crosses a zero zero point and rescale. So it goes to minus one and plus one and it turns out that for hidden units if you let the function G of Z, $g(z)$, be equal to $tanh(z)$. This almost always works better than the sigmoid function because with values between plus one and minus one. The mean of the activations that come out of your head in there are closer to having a zero mean and so just as sometimes when you train a learning algorithm you might Center the data and have your data have zero mean. Using a $tanh$ function instead of a $sigmoid$ function kind of has the effect of centering your data, so that the mean of the data is close to the zero rather than maybe a 0.5 and this actually makes learning for the next layer a little bit easier we’ll say more about this in the second course when we talk about optimization algorithms as well but one takeaway is that I pretty much never use the sigmoid activation function anymore, the tanh function is almost always strictly superior. The only one exception is for the output layer because if Y is either 0 or 1 then it makes sense for $\hat{y}$ to be a number that you want to output. There’s between 0 and 1 rather than between minus 1 and 1 so the one exception where I would use the sigmoid activation function is when you’re using binary classification in which case you might use the sigmoid activation function for the output layer. Now one of the downsides of both the sigmoid function and the tanh function is that if Z is either very large or very small then the gradient of the derivative or the slope of this function becomes very small so Z is very large or Z is very small the slope of the function you know ends up being close to zero and so this can slow down gradient descent, so one of the toys that is very popular in machine learning is what’s called the rectified linear unit, ReLU. so the value function looks like the down-left function of the above slide and the formula is a equals max of 0 comma Z, $max=\{0, z\}$ , So the derivative is 1 so long as Z is positive and derivative or the slope is 0 when Z is negative. If you’re implementing this technically the derivative when Z is exactly 0 is not well-defined but when you implement is in the computer, the odds that you get exactly the equals 0 0 0 0 0 0 0 0 0 0 it is very small, so you don’t need to worry about it. In practice you could pretend a derivative when Z is equal to 0 you can pretend is either 1 or 0 and you can work just fine athlough the fact that is not differentiable. So here are some rules of thumb for choosing activation functions: If your output is 0 1 value if you’re I’m using binary classification then the sigmoid activation function is very natural for the output layer and then for all other units on ReLU or the rectified linear unit is increasingly the default choice of activation function. so if you’re not sure what to use for your head in there I would just use the relu activation function that’s what you see most people using these days although sometimes people also use the tannish activation function. One advantage of the ReLU is that the derivative is equal to zero when z is negative in practice this works just fine, but there is another version of the ReLU called the leaky ReLU will give you the formula on the next slide, but instead of it being zero when z is negative it just takes a slight slope like the down-right of the above slide. So this is called the leaky ReLU. This usually works better than the RELU activation function although it’s just not used as much in practice. Either one should be fine although if you had to pick one I usually just use the revenue and the advantage of both the ReLU and the leaky ReLU is that for a lot of the space of Z the derivative of the activation function the slope of the activation function is very different from zero and so in practice using the ReLU activation function your new network will often learn much faster than using the tanh or the sigmoid activation function and the main reason is that on this less of this effect of the slope of the function going to zero which slows down learning and I know that for half of the range of Z the slope of relu is zero but in practice enough of your hidden units will have Z greater than zero so learning can still be quite mask for most training examples. On the above slide, the leaky ReLU is $max=\{0.01z, z\}$. You might say you know why is that constant 0.01 well you can also make that another parameter of the learning algorithm and some people say that works even better but I hardly see people do that so but if you feel like trying it in your application you know please feel free to do so and and you can just see how it works and how long works and stick with it if it gives you good result. One of the themes we’ll see in deep learning is that you often have a lot of different choices in how you code your neural network ranging from number of credit units to the chosen activation function to how you initialize the parameters which we’ll see later a lot of choices like that and it turns out that is sometimes difficult to get good guidelines for exactly what will work best for your problem so so these three courses. I’ll keep on giving you a sense of what I see in the industry in terms of what’s more or less popular but for your application with your applications idiosyncrasy. It’s actually very difficult to know in advance exactly what will work best so a piece of advice would be if you’re not sure which one of these activation functions work best you know try them all and then evaluate on like a holdout validation set or like a development set which we’ll talk about later and see which one works better and then go of that and I think that by testing these different choices for your application you’d be better at future proofing your neural network architecture against the idiosyncrasy in our problem as well evolutions of the algorithms rather than you know if I were to tell you always use a ReLU activation and don’t use anything else that just may or may not apply for whatever problem you end up working on you know either either in the near future. 07_why-do-you-need-non-linear-activation-functionsWhy does your nerual network need a nonlinear activation function turns out that for your new network to compute interesting functions you do need to take a nonlinear activation function unless you want. No matter how many layers your neural network has always doing is just computing a linear activation function so you might as well not have any hidden layers some of the cases that briefly mentioned it turns out that if you have a linear activation function here and a sigmoid function here(on output layer) then this model is no more expressive than standard logistic regression without any hidden layer so I won’t bother to prove that but you could try to do so if you want but the take-home is that a linear hidden layer is more or less useless because the composition of two linear functions is itself a linear function. there is just one place where you might use a linear activation function G of Z equals Z and that’s if you are doing machine learning on a regression problem so if Y is a real number so for example if you’re trying to predict housing prices so Y is a it’s not 0 1 but it’s a real number you know anywhere from zero dollars is a price of holes up to however expensive right house of kin I guess maybe however can be you know potentially millions of dollars so however however much houses cost in your data set but if Y takes on these real values then it might be OK to have a linear activation function here so that your output Y hat is also a real number going anywhere from minus infinity to plus infinity but then the hidden units should not use the new activation functions they could use relu or 10h or these you relu or maybe something else so the one place you might use a linear activation function others usually in the output layer but other than that using a linear activation function in a fitting layer except for some very special circumstances relating to compression that won’t want to talk about using a linear activation function is extremely rare oh and of course today actually predicting housing prices as you saw on the week 1 video because housing prices are all non-negative perhaps even then you can use a value activation function so that your outputs Y hat are all greater than or equal to 0. so I hope that gives you a sense of why having a nonlinear activation function is a critical part of neural networks. 08_derivatives-of-activation-functionsWhen you implement back-propagation for your neural network you need to really compute the slope or the derivative of the activation functions so let’s take a look at our choices of activation functions and how you can compute the slope of these functions. Sometimes instead of writing this thing $\frac{dg(z)}{dz}$, the shorthand for the derivative is G prime of Z, $g’(z)$ . so G prime of Z in calculus the the little dash on top is called prime but so G prime of Z is a shorthand for the in calculus for the derivative of the function of G with respect to the input variable Z and then in a new network we have $a = g(z)$, right equals this then this formula also simplifies to $a(1-a)$. 09_gradient-descent-for-neural-networksAll right I think that’s be an exciting video in this video you see how to implement gradient descent for your neural network with one hidden layer in this video I’m going to just give you the equations you need to implement in order to get back propagation of the gradient descent working and then in the video after this one I’ll give some more intuition about why these particular equations are the accurate equations or the correct equations for computing the gradients you need for your neural network. In logistic regression, what we want to do is to modify the parameters, W and B, in order to reduce this loss. $$da = \frac{\partial{L}}{\partial{a}} =\frac{\partial \left\{ {-(ylog(a)+(1-y)log(1-a))} \right\} }{\partial{a}} = -\frac{y}{a} + \frac{1-y}{1-a} $$ $$dz=\frac{\partial{L}}{\partial{z}}=\frac{\partial{L}}{\partial{a}}\cdot \frac{\partial{a}}{\partial{z}} = \left(-\frac{y}{a} + \frac{1-y}{1-a}\right) \cdot a(1-a) = a - y$$ $$dw_1=\frac{\partial{L}}{\partial{w_1}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_1}} = x_1\cdot dz = x_1(a-y)$$ $$dw_2=\frac{\partial{L}}{\partial{w_2}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_2}} = x_2\cdot dz = x_2(a-y)$$ $$db=\frac{\partial{L}}{\partial{b}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{b}} = 1 \cdot dz = a - y$$ $$w_1 := w_1 - \alpha dw_1$$ $$w_2 := w_2 - \alpha dw_2$$ $$b := b - \alpha db$$ 11_random-initializationWhen you change your neural network, it’s important to initialize the weights randomly. For logistic regression, it was okay to initialize the weights to zero. But for a neural network of initialize the weights to parameters to all zero and then applied gradient descent, it won’t work. Let’s see why. So you have here two input features, so n0=2, and two hidden units, so n1=2. And so the matrix associated with the hidden layer, w 1, is going to be two-by-two. Let’s say that you initialize it to all 0s, so 0 0 0 0, two-by-two matrix. And let’s say B1 is also equal to 0 0. It turns out initializing the bias terms b to 0 is actually okay, but initializing w to all 0s is a problem. So the problem with this formalization is that for any example you give it, you’ll have that a1,1 and a1,2, will be equal, right? So this activation and this activation will be the same, because both of these hidden units are computing exactly the same function. And then, when you compute backpropagation, it turns out that dz11 and dz12 will also be the same colored by symmetry, right? Both of these hidden unit will initialize the same way. Technically, for what I’m saying, I’m assuming that the outgoing weights or also identical. So that’s w2 is equal to 0 0. But if you initialize the neural network this way, then this hidden unit and this hidden unit are completely identical. Sometimes you say they’re completely symmetric, which just means that they’re completing exactly the same function. And by kind of a proof by induction, it turns out that after every single iteration of training your two hidden units are still computing exactly the same function. Since [INAUDIBLE] show that dw will be a matrix that looks like this. Where every row takes on the same value. So we perform a weight update. So when you perform a weight update, w1 gets updated as w1- alpha times dw. You find that w1, after every iteration, will have the first row equal to the second row. So it’s possible to construct a proof by induction that if you initialize all the ways, all the values of w to 0, then because both hidden units start off computing the same function. And both hidden the units have the same influence on the output unit, then after one iteration, that same statement is still true, the two hidden units are still symmetric. And therefore, by induction, after two iterations, three iterations and so on, no matter how long you train your neural network, both hidden units are still computing exactly the same function. And so in this case, there’s really no point to having more than one hidden unit. Because they are all computing the same thing. And of course, for larger neural networks, let’s say of three features and maybe a very large number of hidden units, a similar argument works to show that with a neural network like this. [INAUDIBLE] drawing all the edges, if you initialize the weights to zero, then all of your hidden units are symmetric. And no matter how long you’re upgrading the center, all continue to compute exactly the same function. So that’s not helpful, because you want the different hidden units to compute different functions. The solution to this is to initialize your parameters randomly. So here’s what you do. You can set w1 = np.random.randn. This generates a gaussian random variable (2,2). And then usually, you multiply this by very small number, such as 0.01. So you initialize it to very small random values. And then b, it turns out that b does not have the symmetry problem, what’s called the symmetry breaking problem. So it’s okay to initialize b to just zeros. Because so long as w is initialized randomly, you start off with the different hidden units computing different things. And so you no longer have this symmetry breaking problem. And then similarly, for w2, you’re going to initialize that randomly. And b2, you can initialize that to 0. So you might be wondering, where did the constant come from and why is it 0.01? Why not put the number 100 or 1000? Turns out that we usually prefer to initialize the weights to very small random values. Because if you are using a tanh or sigmoid activation function, or the other sigmoid, even just at the output layer. If the weights are too large, then when you compute the activation values, remember that z[1]=w1 x + b. And then a1 is the activation function applied to z1. So if w is very big, z will be very, or at least some values of z will be either very large or very small. And so in that case, you’re more likely to end up at these fat parts of the tanh function or the sigmoid function, where the slope or the gradient is very small. Meaning that gradient descent will be very slow. So learning was very slow. So just a recap, if w is too large, you’re more likely to end up even at the very start of training, with very large values of z. Which causes your tanh or your sigmoid activation function to be saturated, thus slowing down learning. If you don’t have any sigmoid or tanh activation functions throughout your neural network, this is less of an issue. But if you’re doing binary classification, and your output unit is a sigmoid function, then you just don’t want the initial parameters to be too large. So that’s why multiplying by 0.01 would be something reasonable to try, or any other small number. And same for w2, right? This can be random.random. I guess this would be 1 by 2 in this example, times 0.01. Missing an s there. So finally, it turns out that sometimes they can be better constants than 0.01. When you’re training a neural network with just one hidden layer, it is a relatively shallow neural network, without too many hidden layers. Set it to 0.01 will probably work okay. But when you’re training a very very deep neural network, then you might want to pick a different constant than 0.01. And in next week’s material, we’ll talk a little bit about how and when you might want to choose a different constant than 0.01. But either way, it will usually end up being a relatively small number. So that’s it for this week’s videos. You now know how to set up a neural network of a hidden layer, initialize the parameters, make predictions using. As well as compute derivatives andimplement gradient descent, using backprop. So that,you should be able to do the quizzes, as well as this week’s programming exercises. Best of luck with that. I hope you have fun with the problem exercise, and look forward to seeing you in the week four materials.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>neural-networks-deep-learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[转载-希腊字母常用指代意义及中文读音]]></title>
    <url>%2F2018%2F02%2F02%2Fgreek-letters%2F</url>
    <content type="text"><![CDATA[更多细节请参考 百度百科-希腊字母]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>english</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02_logistic-regression-as-a-neural-network]]></title>
    <url>%2F2018%2F02%2F02%2F02_neural-networks-basics%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal note at the 2nd week after studying the course neural-networks-deep-learning and the copyright belongs to deeplearning.ai. 01_logistic-regression-as-a-neural-network01_binary-classificationBinary ClassificationIn a binary classification problem, the result is a discrete value output. For example account hacked (1) or compromised (0) a tumor malign (1) or benign (0) Example: Cat vs Non-CatThe goal is to train a classifier that the input is an image represented by a feature vector, $x$, and predicts whether the corresponding label $y$ is 1 or 0. In this case, whether this is a cat image (1) or a non-cat image (0). An image is store in the computer in three separate matrices corresponding to the Red, Green, and Blue color channels of the image. The three matrices have the same size as the image, for example, the resolution of the cat image is 64 pixels X 64 pixels, the three matrices (RGB) are 64 X 64 each.The value in a cell represents the pixel intensity which will be used to create a feature vector of ndimension. In pattern recognition and machine learning, a feature vector represents an object, in this case, a cat or no cat.To create a feature vector, $x$, the pixel intensity values will be “unroll” or “reshape” for each color. The dimension of the input feature vector $x$ is $ n_x = 64 \times 64 \times 3 = 12 288.$ notation 02_Logistic RegressionLogistic regression is a learning algorithm used in a supervised learning problem when the output $y$ are all either zero or one. The goal of logistic regression is to minimize the error between its predictions and training data. Example: Cat vs No - catGiven an image represented by a feature vector $x$, the algorithm will evaluate the probability of a cat being in that image. $$\text{Civen }x, \hat{y}=P(y=1|x), \text{where } 0 \le \hat{y} \le 1$$ The parameters used in Logistic regression are:• The input features vector: $x ∈ ℝ^{n_x}$, where $n_x$ is the number of features• The training label: $y ∈ 0,1$• The weights: $w ∈ ℝ^{n_x}$ , where $n_x$ is the number of features• The threshold: $b ∈ ℝ$• The output: $\hat{y} = \sigma(w^Tx+b)$• Sigmoid function: $s = \sigma(w^Tx+b) = \sigma(z)= \frac{1}{1+e^{-z}}$ $(w^Tx +b )$ is a linear function $(ax + b)$, but since we are looking for a probability constraint between [0,1], the sigmoid function is used. The function is bounded between [0,1] as shown in the graph above.Some observations from the graph:• If $z$ is a large positive number, then $\sigma(z) = 1$• If $z$ is small or large negative number, then $\sigma(z) = 0$• If $z$ = 0, then $\sigma(z) = 0.5$ notation 03_logistic-regression-cost-function 04_gradient-descent 05_06_derivatives 07_computation-graphYou’ve heard me say that the computations of a neural network are organized in terms of a forward pass or a forward propagation step, in which we compute the output of the neural network, followed by a backward pass or back propagation step, which we use to compute gradients or compute derivatives. The computation graph explains why it is organized this way. 09_logistic-regression-gradient-descentWelcome back. In this video, we’ll talk about how to compute derivatives for you to implement gradient descent for logistic regression. The key takeaways will be what you need to implement. That is, the key equations you need in order to implement gradient descent for logistic regression. In this video, I want to do this computation using the computation graph. I have to admit, using the computation graph is a little bit of an overkill for deriving gradient descent for logistic regression, but I want to start explaining things this way to get you familiar with these ideas so that, hopefully, it will make a bit more sense when we talk about fully-fledged neural networks. To that, let’s dive into gradient descent for logistic regression. In logistic regression, what we want to do is to modify the parameters, W and B, in order to reduce this loss. $$da = \frac{\partial{L}}{\partial{a}} =\frac{\partial \left\{ {-(ylog(a)+(1-y)log(1-a))} \right\} }{\partial{a}} = -\frac{y}{a} + \frac{1-y}{1-a} $$ $$dz=\frac{\partial{L}}{\partial{z}}=\frac{\partial{L}}{\partial{a}}\cdot \frac{\partial{a}}{\partial{z}} = \left(-\frac{y}{a} + \frac{1-y}{1-a}\right) \cdot a(1-a) = a - y$$ $$dw_1=\frac{\partial{L}}{\partial{w_1}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_1}} = x_1\cdot dz = x_1(a-y)$$ $$dw_2=\frac{\partial{L}}{\partial{w_2}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{w_2}} = x_2\cdot dz = x_2(a-y)$$ $$db=\frac{\partial{L}}{\partial{b}}=\frac{\partial{L}}{\partial{z}}\cdot \frac{\partial{z}}{\partial{b}} = 1 \cdot dz = a - y$$ $$w_1 := w_1 - \alpha dw_1$$ $$w_2 := w_2 - \alpha dw_2$$ $$b := b - \alpha db$$ 10_gradient-descent-on-m-examplesin a previous video you saw how to compute derivatives and implement gradient descent with respect to just one training example for religious regression now we want to do it for m training examples. one single step gradient descent 02_python-and-vectorization01_vectorizationWelcome back. Vectorization is basically the art of getting rid of explicit folders in your code. In the deep learning era safety in deep learning in practice, you often find yourself training on relatively large data sets, because that’s when deep learning algorithms tend to shine. And so, it’s important that your code very quickly because otherwise, if it’s running on a big data set, your code might take a long time to run then you just find yourself waiting a very long time to get the result. So in the deep learning era, I think the ability to perform vectorization has become a key skill. Yeah. Vectorize version 1.5 milliseconds seconds and the four loop. So 481 milliseconds, again, about 300 times slower to do the explicit four loop. If the engine x slows down, it’s the difference between your code taking maybe one minute to run versus taking say five hours to run. And when you are implementing deep learning algorithms, you can really get a result back faster. It will be much faster if you vectorize your code. Some of you might have heard that a lot of scaleable deep learning implementations are done on a GPU or a graphics processing unit. But all the demos I did just now in the Jupiter notebook where actually on the CPU. And it turns out that both GPU and CPU have parallelization instructions. They’re sometimes called SIMD instructions. This stands for a single instruction multiple data. But what this basically means is that, if you use built-in functions such as this np.function or other functions that don’t require you explicitly implementing a for loop. It enables Phyton Pi to take much better advantage of parallelism to do your computations much faster. And this is true both computations on CPUs and computations on GPUs. It’s just that GPUs are remarkably good at these SIMD calculations but CPU is actually also not too bad at that. Maybe just not as good as GPUs. You’re seeing how vectorization can significantly speed up your code. The rule of thumb to remember is whenever possible, avoid using explicit four loops. 02_more-vectorization-examples 03_vectorizing-logistic-regressionWe have talked about how vectorization lets you speed up your code significantly. In this video, we’ll talk about how you can vectorize the implementation of logistic regression, so they can process an entire training set, that is implement a single elevation of grading descent with respect to an entire training set without using even a single explicit for loop. I’m super excited about this technique, and when we talk about neural networks later without using even a single explicit for loop. Here are details of python broadcasting 04_vectorizing-logistic-regressions-gradient-outputIn the previous video, you saw how you can use vectorization to compute their predictions. The lowercase a’s for an entire training set O at the same time. In this video, you see how you can use vectorization to also perform the gradient computations for all M training samples. Again, all sort of at the same time. And then at the end of this video, we’ll put it all together and show how you can derive a very efficient implementation of logistic regression. 05_broadcasting-in-python Summary: Python or Numpy automatically expands two arrays or numbers to the same dimensions and operate element-wise. 06_a-note-on-python-numpy-vectorsThe ability of python to allow you to use broadcasting operations and more generally, the great flexibility of the python numpy program language is, I think, both a strength as well as a weakness of the programming language. I think it’s a strength because they create expressivity of the language. A great flexibility of the language lets you get a lot done even with just a single line of code. But there’s also weakness because with broadcasting and this great amount of flexibility, sometimes it’s possible you can introduce very subtle bugs or very strange looking bugs, if you’re not familiar with all of the intricacies of how broadcasting and how features like broadcasting work. For example, if you take a column vector and add it to a row vector, you would expect it to throw up a dimension mismatch or type error or something. But you might actually get back a matrix as a sum of a row vector and a column vector. So there is an internal logic to these strange effects of Python. But if you’re not familiar with Python, I’ve seen some students have very strange, very hard to find bugs. So what I want to do in this video is share with you some couple tips and tricks that have been very useful for me to eliminate or simplify and eliminate all the strange looking bugs in my own code. And I hope that with these tips and tricks, you’ll also be able to much more easily write bug-free, python and numpy code. To illustrate one of the less intuitive effects of Python-Numpy, especially how you construct vectors in Python-Numpy, let me do a quick demo. one rank array practical tips 07_quick-tour-of-jupyter-ipython-notebooksWith everything you’ve learned, you’re just about ready to tackle your first programming assignment. Before you do that, let me just give you a quick tour of iPython notebooks in Coursera. Please see the video to get details. 08_explanation-of-logistic-regression-cost-function-optional]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>neural-networks-deep-learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01_introduction-to-deep-learning]]></title>
    <url>%2F2018%2F02%2F01%2F01_introduction-to-deep-learning%2F</url>
    <content type="text"><![CDATA[NoteThis is my personal note at the first week after studying the course neural-networks-deep-learning and the copyright belongs to deeplearning.ai. 01_introduction-to-deep-learning01_What is neural network?It is a powerful learning algorithm inspired by how the brain works. Example 1 – single neural networkGiven data about the size of houses on the real estate market and you want to fit a function that will predict their price. It is a linear regression problem because the price as a function of size is a continuous output.We know the prices can never be negative so we are creating a function called Rectified Linear Unit (ReLU) which starts at zero.The input is the size of the house (x)The output is the price (y)The “neuron” implements the function ReLU (blue line) Example 2 – Multiple neural networkThe price of a house can be affected by other features such as size, number of bedrooms, zip code andwealth. The role of the neural network is to predicted the price and it will automatically generate the hidden units. We only need to give the inputs x and the output y. 02_supervised-learning-with-neural-networksSupervised learning for Neural NetworkIn supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.Supervised learning problems are categorized into “regression“ and “classification“ problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.Here are some examples of supervised learning. There are different types of neural network, for example Convolution Neural Network (CNN) used often for image application and Recurrent Neural Network (RNN) used for one-dimensional sequence data such as translating English to Chinses or a temporal component such as text transcript. As for the autonomous driving, it is a hybrid neural network architecture. Structured vs unstructured dataStructured data refers to things that has a defined meaning such as price, age whereas unstructured data refers to thing like pixel, raw audio, text. 03_why-is-deep-learning-taking-offWhy is deep learning taking off?Deep learning is taking off due to a large amount of data available through the digitization of the society, faster computation and innovation in the development of neural network algorithm. Two things have to be considered to get to the high level of performance: Being able to train a big enough neural network Huge amount of labeled data The process of training a neural network is iterative. It could take a good amount of time to train a neural network, which affects your productivity. Faster computation helps to iterate and improve new algorithm. 04_about-this-courseOutline of this Course Week 1: Introduction Week 2: Basics of Neural Network programming Week 3: One hidden layer Neural Networks Week 4: Deep Neural Networks]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>neural-networks-deep-learning</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[My summary about Machine Learning]]></title>
    <url>%2F2018%2F01%2F20%2Fsummary_of_ml-coursera-andrew-ng%2F</url>
    <content type="text"><![CDATA[The content table of Machine LearningThis course is a coursera version teached by Andrew NG, AP of Stanford University, which corresponds to the full-time campus version CS229 at Stanford university, that is increasing difficult version. 01_introduction02_linear-regression-with-one-variable03_linear-algebra-review04_linear-regression-with-multiple-variables05_octave-matlab-tutorial06_logistic-regression07_regularization08_neural-networks-representation09_neural-networks-learning10_advice-for-applying-machine-learning11_machine-learning-system-design12_support-vector-machines13_unsupervised-learning14_dimensionality-reduction15_anomaly-detection16_recommender-systems17_large-scale-machine-learning18_application-example-photo-ocr Conclusion of Andrew NGWelcome to the final video of this Machine Learning class. We’ve been through a lot of different videos together. In this video I would like to just quickly summarize the main topics of this course and then say a few words at the end and that will wrap up the class. So what have we done? summaryIn this class we spent a lot of time talking about supervised learning algorithms like linear regression, logistic regression, neural networks, SVMs. for problems where you have labelled data and labelled examples like x(i), y(i) And we also spent quite a lot of time talking about unsupervised learning like K-means clustering, Principal Components Analysis for dimensionality reduction and Anomaly Detection algorithms for when you have only unlabelled data x(i) Although Anomaly Detection can also use some labelled data to evaluate the algorithm. We also spent some time talking about special applications or special topics like Recommender Systems and large scale machine learning systems including parallelized and rapid-use systems as well as some special applications like sliding windows object classification for computer vision. And finally we also spent a lot of time talking about different aspects of, sort of, advice on building a machine learning system. And this involved both trying to understand what is it that makes a machine learning algorithm work or not work. So we talked about things like bias and variance, and how regularization can help with some variance problems. And we also spent a little bit of time talking about this question of how to decide what to work on next. So, how to prioritize how you spend your time when you’re developing a machine learning system. So we talked about evaluation of learning algorithms, evaluation metrics like precision recall, F1 score as well as practical aspects of evaluation like the training, cross-validation and test sets. And we also spent a lot of time talking about debugging learning algorithms and making sure the learning algorithm is working. So we talked about diagnostics like learning curves and also talked about things like error analysis and ceiling analysis. And so all of these were different tools for helping you to decide what to do next and how to spend your valuable time when you’re developing a machine learning system. And in addition to having the tools of machine learning at your disposal so knowing the tools of machine learning like supervised learning and unsupervised learning and so on, I hope that you now not only have the tools, but that you know how to apply these tools really well to build powerful machine learning systems. So, that’s it. Those were the topics of this class and if you worked all the way through this course you should now consider yourself an expert in machine learning. As you know, machine learning is a technology that’s having huge impact on science, technology and industry. And you’re now well qualified to use these tools of machine learning to great effect. THXI hope that many of you in this class will find ways to use machine learning to build cool systems and cool applications and cool products. And I hope that you find ways to use machine learning not only to make your life better but maybe someday to use it to make many other people’s life better as well. I also wanted to let you know that this class has been great fun for me to teach. So, thank you for that. And before wrapping up, there’s just one last thing I wanted to say. Which is that: It was maybe not so long ago, that I was a student myself. And even today, you know, I still try to take different courses when I have time to try to learn new things. And so I know how time-consuming it is to learn this stuff. I know that you’re probably a busy person with many, many other things going on in your life. And so the fact that you still found the time or took the time to watch these videos and, you know, many of these videos just went on for hours, right? And the fact many of you took the time to go through the review questions and that many of you took the time to work through the programming exercises. And these were long and complicate programming exercises. I wanted to say thank you for that. And I know that many of you have worked hard on this class and that many of you have put a lot of time into this class, that many of you have put a lot of yourselves into this class. So I hope that you also got a lot of out this class. And I wanted to say: Thank you very much for having been a student in this class. My GradesHere are the details of my grades]]></content>
      <categories>
        <category>英文</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[18_application-example-photo-ocr note18]]></title>
    <url>%2F2018%2F01%2F18%2F18_application-example-photo-ocr%2F</url>
    <content type="text"><![CDATA[01_photo-ocr01_problem-description-and-pipelineIn this and the next few videos, I want to tell you about a machine learning application example, or a machine learning application history centered around an application called Photo OCR . There are three reasons why I want to do this, first I wanted to show you an example of how a complex machine learning system can be put together. Second, once told the concepts of a machine learning a type line and how to allocate resources when you’re trying to decide what to do next. And this can either be in the context of you working by yourself on the big application Or it can be the context of a team of developers trying to build a complex application together. And then finally, the Photo OCR problem also gives me an excuse to tell you about just a couple more interesting ideas for machine learning. One is some ideas of how to apply machine learning to computer vision problems, and second is the idea of artificial data synthesis, which we’ll see in a couple of videos. So, let’s start by talking about what is the Photo OCR problem. Photo OCR stands for Photo Optical Character Recognition. With the growth of digital photography and more recently the growth of camera in our cell phones we now have tons of visual pictures that we take all over the place. And one of the things that has interested many developers is how to get our computers to understand the content of these pictures a little bit better. The photo OCR problem focuses on how to get computers to read the text to the purest in images that we take. Given an image like this it might be nice if a computer can read the text in this image so that if you’re trying to look for this picture again you type in the words, lulu bees and and have it automatically pull up this picture, so that you’re not spending lots of time digging through your photo collection Maybe hundreds of thousands of pictures in. The Photo OCR problem does exactly this, and it does so in several steps. First, given the picture it has to look through the image and detect where there is text in the picture. And after it has done that or if it successfully does that it then has to look at these text regions and actually read the text in those regions, and hopefully if it reads it correctly, it’ll come up with these transcriptions of what is the text that appears in the image. Whereas OCR, or optical character recognition of scanned documents is relatively easier problem, doing OCR from photographs today is still a very difficult machine learning problem, and you can do this. Not only can this help our computers to understand the content of our though images better, there are also applications like helping blind people, for example, if you could provide to a blind person a camera that can look at what’s in front of them, and just tell them the words that my be on the street sign in front of them. With car navigation systems. For example, imagine if your car could read the street signs and help you navigate to your destination. In order to perform photo OCR, here’s what we can do. First we can go through the image and find the regions where there’s text and image. So, shown here is one example of text and image that the photo OCR system may find. Second, given the rectangle around that text region, we can then do character segmentation, where we might take this text box that says “Antique Mall” and try to segment it out into the locations of the individual characters. And finally, having segmented out into individual characters, we can then run a crossfire, which looks at the images of the visual characters, and tries to figure out the first character’s an A, the second character’s an N, the third character is a T, and so on, so that up by doing all this how that hopefully you can then figure out that this phrase is Rulegee’s antique mall and similarly for some of the other words that appear in that image. I should say that there are some photo OCR systems that do even more complex things, like a bit of spelling correction at the end. So if, for example, your character segmentation and character classification system tells you that it sees the word c 1 e a n i n g. Then, you know, a sort of spelling correction system might tell you that this is probably the word ‘cleaning’, and your character classification algorithm had just mistaken the l for a 1. But for the purpose of what we want to do in this video, let’s ignore this last step and just focus on the system that does these three steps of text detection, character segmentation, and character classification. A system like this is what we call a machine learning pipeline. In particular, here’s a picture showing the photo OCR pipeline. We have an image, which then fed to the text detection system text regions, we then segment out the characters–the individual characters in the text–and then finally we recognize the individual characters. In many complex machine learning systems, these sorts of pipelines are common, where you can have multiple modules–in this example, the text detection, character segmentation, character recognition modules–each of which may be machine learning component, or sometimes it may not be a machine learning component but to have a set of modules that act one after another on some piece of data in order to produce the output you want, which in the photo OCR example is to find the transcription of the text that appeared in the image. If you’re designing a machine learning system one of the most important decisions will often be what exactly is the pipeline that you want to put together. In other words, given the photo OCR problem, how do you break this problem down into a sequence of different modules. And you design the pipeline and each the performance of each of the modules in your pipeline. will often have a big impact on the final performance of your algorithm. If you have a team of engineers working on a problem like this is also very common to have different individuals work on different modules. So I could easily imagine tech easily being the of anywhere from 1 to 5 engineers, character segmentation maybe another 1-5 engineers, and character recognition being another 1-5 engineers, and so having a pipeline like often offers a natural way to divide up the workload amongst different members of an engineering team, as well. Although, or course, all of this work could also be done by just one person if that’s how you want to do it. In complex machine learning systems the idea of a pipeline, of a machine of a pipeline, is pretty pervasive. And what you just saw is a specific example of how a Photo OCR pipeline might work. In the next few videos I’ll tell you a little bit more about this pipeline, and we’ll continue to use this as an example to illustrate–I think–a few more key concepts of machine learning. 02_sliding-windowsIn the previous video, we talked about the photo OCR pipeline and how that worked. In which we would take an image and pass the Through a sequence of machine learning components in order to try to read the text that appears in an image. In this video I like to. A little bit more about how the individual components of the pipeline works. In particular most of this video will center around the discussion. of whats called a sliding windows. The first stage of the filter was the Text detection where we look at an image like this and try to find the regions of text that appear in this image. Text detection is an unusual problem in computer vision. Because depending on the length of the text you’re trying to find, these rectangles that you’re trying to find can have different aspect. So in order to talk about detecting things in images let’s start with a simpler example of pedestrian detection and we’ll then later go back to. Ideas that were developed in pedestrian detection and apply them to text detection. So in pedestrian detection you want to take an image that looks like this and the whole idea is the individual pedestrians that appear in the image. So there’s one pedestrian that we found, there’s a second one, a third one a fourth one, a fifth one. And a one. This problem is maybe slightly simpler than text detection just for the reason that the aspect ratio of most pedestrians are pretty similar. Just using a fixed aspect ratio for these rectangles that we’re trying to find. So by aspect ratio I mean the ratio between the height and the width of these rectangles. They’re all the same. for different pedestrians but for text detection the height and width ratio is different for different lines of text Although for pedestrian detection, the pedestrians can be different distances away from the camera and so the height of these rectangles can be different depending on how far away they are. but the aspect ratio is the same. In order to build a pedestrian detection system here’s how you can go about it. Let’s say that we decide to standardize on this aspect ratio of 82 by 36 and we could have chosen some rounded number like 80 by 40 or something, but 82 by 36 seems alright. What we would do is then go out and collect large training sets of positive and negative examples. Here are examples of 82 X 36 image patches that do contain pedestrians and here are examples of images that do not. On this slide I show 12 positive examples of y1 and 12 examples of y0. In a more typical pedestrian detection application, we may have anywhere from a 1,000 training examples up to maybe 10,000 training examples, or even more if you can get even larger training sets. And what you can do, is then train in your network or some other learning algorithm to take this input, an MS patch of dimension 82 by 36, and to classify ‘y’ and to classify that image patch as either containing a pedestrian or not. So this gives you a way of applying supervised learning in order to take an image patch can determine whether or not a pedestrian appears in that image capture. Now, lets say we get a new image, a test set image like this and we want to try to find a pedestrian’s picture image. What we would do is start by taking a rectangular patch of this image. Like that shown up here, so that’s maybe a 82 X 36 patch of this image, and run that image patch through our classifier to determine whether or not there is a pedestrian in that image patch, and hopefully our classifier will return y equals 0 for that patch, since there is no pedestrian.Next, we then take that green rectangle and we slide it over a bit and then run that new image patch through our classifier to decide if there’s a pedestrian there.And having done that, we then slide the window further to the right and run that patch through the classifier again. The amount by which you shift the rectangle over each time is a parameter, that’s sometimes called the step size of the parameter, sometimes also called the slide parameter, and if you step this one pixel at a time. So you can use the step size or stride of 1, that usually performs best, that is more cost effective, and so using a step size of maybe 4 pixels at a time, or eight pixels at a time or some large number of pixels might be more common, since you’re then moving the rectangle a little bit more each time.So, using this process, you continue stepping the rectangle over to the right a bit at a time and running each of these patches through a classifier, until eventually, as you slide this window over the different locations in the image, first starting with the first row and then we go further rows in the image, you would then run all of these different image patches at some step size or some stride through your classifier. Now, that was a pretty small rectangle, that would only detect pedestrians of one specific size.What we do next is start to look at larger image patches. So now let’s take larger images patches, like those shown here and run those through the classifier as well.And by the way when I say take a larger image patch, what I really mean is when you take an image patch like this, what you’re really doing is taking that image patch, and resizing it down to 82 X 36, say. So you take this larger patch and re-size it to be smaller image and then it would be the smaller size image that is what you would pass through your classifier to try and decide if there is a pedestrian in that patch.And finally you can do this at an even larger scales and run that side of Windows to the end.And after this whole process hopefully your algorithm will detect whether theres pedestrian appears in the image, so thats how you train a the classifier, and then use a sliding windows classifier, or use a sliding windows detector in order to find pedestrians in the image. Let’s now return to the text detection example and talk about that stage in our photo OCR pipeline, where our goal is to find the text regions in unit. similar to pedestrian detection you can come up with a label training set with positive examples and negative examples with examples corresponding to regions where text appears. So instead of trying to detect pedestrians, we’re now trying to detect texts. And so positive examples are going to be patches of images where there is text. And negative examples is going to be patches of images where there isn’t text. Having trained this we can now apply it to a new image, into a test set image.So here’s the image that we’ve been using as example. Now, last time we run, for this example we are going to run a sliding windows at just one fixed scale just for purpose of illustration, meaning that I’m going to use just one rectangle size. But lets say I run my little sliding windows classifier on lots of little image patches like this if I do that, what Ill end up with is a result like this where the white region show where my text detection system has found text and so the axis’ of these two figures are the same. So there is a region up here, of course also a region up here, so the fact that this black up here represents that the classifier does not think it’s found any texts up there, whereas the fact that there’s a lot of white stuff here, that reflects that classifier thinks that it’s found a bunch of texts. over there on the image. What i have done on this image on the lower left is actually use white to show where the classifier thinks it has found text. And different shades of grey correspond to the probability that was output by the classifier, so like the shades of grey corresponds to where it thinks it might have found text but has lower confidence the bright white response to whether the classifier, up with a very high probability, estimated probability of there being pedestrians in that location.We aren’t quite done yet because what we actually want to do is draw rectangles around all the region where this text in the image, so were going to take one more step which is we take the output of the classifier and apply to it what is called an expansion operator. So what that does is, it take the image here, and it takes each of the white blobs, it takes each of the white regions and it expands that white region.Mathematically, the way you implement that is, if you look at the image on the right, what we’re doing to create the image on the right is, for every pixel we are going to ask, is it withing some distance of a white pixel in the left image. And so, if a specific pixel is within, say, five pixels or ten pixels of a white pixel in the leftmost image, then we’ll also color that pixel white in the rightmost image. And so, the effect of this is, we’ll take each of the white blobs in the leftmost image and expand them a bit, grow them a little bit, by seeing whether the nearby pixels, the white pixels, and then coloring those nearby pixels in white as well. Finally, we are just about done. We can now look at this rightmost image and just look at the connecting components and look at the contiguous white regions and draw bounding boxes around them. And in particular, if we look at all the white regions, like this one, this one, this one, and so on, and if we use a simple heuristic to rule out rectangles whose aspect ratios look funny because we know that boxes around text should be much wider than they are tall. And so if we ignore the thin, tall blocks like this one and this one, and we discard these ones because they are too tall and thin, and we then draw a the rectangles around the ones whose aspect ratio thats a height to what ratio looks like for text regions, then we can draw rectangles, the bounding boxes around this text region, this text region, and that text region, corresponding to the Lula B’s antique mall logo, the LULA B’s, and this little open sign. Of over there.This example by the actually misses one piece of text. This is very hard to read, but there is actually one piece of text there. That says [xx] are corresponding to this but the aspect ratio looks wrong so we discarded that one. So you know it’s ok on this image, but in this particular example the classifier actually missed one piece of text. It’s very hard to read because there’s a piece of text written against a transparent window. So that’s text detection using sliding windows. And having found these rectangles with the text in it, we can now just cut out these image regions and then use later stages of pipeline to try to meet the texts. Now, you recall that the second stage of pipeline was character segmentation, so given an image like that shown on top, how do we segment out the individual characters in this image? So what we can do is again use a supervised learning algorithm with some set of positive and some set of negative examples, what were going to do is look in the image patch and try to decide if there is split between two characters right in the middle of that image match. So for initial positive examples. This first cross example, this image patch looks like the middle of it is indeed the middle has splits between two characters and the second example again this looks like a positive example, because if I split two characters by putting a line right down the middle, that’s the right thing to do. So, these are positive examples, where the middle of the image represents a gap or a split between two distinct characters, whereas the negative examples, well, you know, you don’t want to split two characters right in the middle, and so these are negative examples because they don’t represent the midpoint between two characters. So what we will do is, we will train a classifier, maybe using new network, maybe using a different learning algorithm, to try to classify between the positive and negative examples. Having trained such a classifier, we can then run this on this sort of text that our text detection system has pulled out. As we start by looking at that rectangle, and we ask, “Gee, does it look like the middle of that green rectangle, does it look like the midpoint between two characters?”. And hopefully, the classifier will say no, then we slide the window overand this is a one dimensional sliding window classifier, because were going to slide the window only in one straight line from left to right, theres no different rows here. There’s only one row here. But now, with the classifier in this position, we ask, well, should we split those two characters or should we put a split right down the middle of this rectangle. And hopefully, the classifier will output y equals one, in which case we will decide to draw a line down there, to try to split two characters.Then we slide the window over again, optic process, don’t close the gap, slide over again, optic says yes, do split there and so on, and we slowly slide the classifier over to the right and hopefully it will classify this as another positive example and so on.And we will slide this window over to the right, running the classifier at every step, and hopefully it will tell us, you know, what are the right locations to split these characters up into, just split this image up into individual characters.And so thats 1D sliding windows for character segmentation. So, here’s the overall photo OCR pipe line again. In this video we’ve talked about the text detection step, where we use sliding windows to detect text. And we also use a one-dimensional sliding windows to do character segmentation to segment out, you know, this text image in division of characters. The final step through the pipeline is the character qualification step and that step you might already be much more familiar with the early videos on supervised learning where you can apply a standard supervised learning within maybe on your network or maybe something else in order to take it’s input, an image like that and classify which alphabet or which 26 characters A to Z, or maybe we should have 36 characters if you have the numerical digits as well, the multi class classification problem where you take it’s input and image contained a character and decide what is the character that appears in that image?So that was the photo OCR pipeline and how you can use ideas like sliding windows classifiers in order to put these different components to develop a photo OCR system. In the next few videos we keep on using the problem of photo OCR to explore somewhat interesting issues surrounding building an application like this. 03_getting-lots-of-data-and-artificial-dataI’ve seen over and over that one of the most reliable ways to get a high performance machine learning system is to take a low bias learning algorithm and to train it on a massive training set. But where did you get so much training data from? Turns out that the machine learnings there’s a fascinating idea called artificial data synthesis, this doesn’t apply to every single problem, and to apply to a specific problem, often takes some thought and innovation and insight. But if this idea applies to your machine, only problem, it can sometimes be a an easy way to get a huge training set to give to your learning algorithm. The idea of artificial data synthesis comprises of two variations, main the first is if we are essentially creating data from [xx], creating new data from scratch. And the second is if we already have it’s small label training set and we somehow have amplify that training set or use a small training set to turn that into a larger training set and in this video we’ll go over both those ideas. artificial data synthesis for Photo OCRTo talk about the artificial data synthesis idea, let’s use the character portion of the photo OCR pipeline, we want to take it’s input image and recognize what character it is.If we go out and collect a large label data set, here’s what it is and what it look like. For this particular example, I’ve chosen a square aspect ratio. So we’re taking square image patches. And the goal is to take an image patch and recognize the character in the middle of that image patch. And for the sake of simplicity, I’m going to treat these images as grey scale images, rather than color images. It turns out that using color doesn’t seem to help that much for this particular problem. So given this image patch, we’d like to recognize that that’s a T. Given this image patch, we’d like to recognize that it’s an ‘S’. Given that image patch we would like to recognize that as an ‘I’ and so on.So all of these, our examples of row images, how can we come up with a much larger training set? Modern computers often have a huge font library and if you use a word processing software, depending on what word processor you use, you might have all of these fonts and many, many more Already stored inside. And, in fact, if you go different websites, there are, again, huge, free font libraries on the internet we can download many, many different types of fonts, hundreds or perhaps thousands of different fonts.So if you want more training examples, one thing you can do is just take characters from different fonts and paste these characters against different random backgrounds. So you might take this —- and paste that c against a random background. If you do that you now have a training example of an image of the character C. So after some amount of work, you know this, and it is a little bit of work to synthisize realistic looking data. But after some amount of work, you can get a synthetic training set like that.Every image shown on the right was actually a synthesized image. Where you take a font, maybe a random font downloaded off the web and you paste an image of one character or a few characters from that font against this other random background image. And then apply maybe a little blurring operators —–of app finder, distortions that app finder, _meaning just the sharing and scaling and little rotation operations_ and if you do that you get a synthetic training set, on what the one shown here. And this is work, grade, it is, it takes thought at work, in order to make the synthetic data look realistic, and if you do a sloppy job in terms of how you create the synthetic data then it actually won’t work well. But if you look at the synthetic data looks remarkably similar to the real data. And so by using synthetic data you have essentially an unlimited supply of training examples for artificial training synthesis And so, if you use this source synthetic data, you have essentially unlimited supply of label data to create a improvised learning algorithm for the character recognition problem. So this is an example of artificial data synthesis where youre basically creating new data from scratch, you just generating brand new images from scratch. The other main approach to artificial data synthesis is where you take a examples that you currently have, that we take a real example, maybe from real image, and you create additional data, so as to amplify your training set. So here is an image of a compared to a from a real image, not a synthesized image, and I have overlayed this with the grid lines just for the purpose of illustration. Actually have these —-. So what you can do is then take this alphabet here, take this image and introduce artificial warping or artificial distortions into the image so they can take the image a and turn that into 16 new examples. So in this way you can take a small label training set and amplify your training set to suddenly get a lot more examples, all of it. Again, in order to do this for application, it does take thought and it does take insight to figure out what our reasonable sets of distortions, or whether these are ways that amplify and multiply your training set, and for the specific example of character recognition, introducing these warping seems like a natural choice, but for a different learning machine application, there may be different the distortions that might make more sense. Synthesizing data by introducing distortions: Speech recognitionLet me just show one example from the totally different domain of speech recognition. So the speech recognition, let’s say you have audio clips and you want to learn from the audio clip to recognize what were the words spoken in that clip. So let’s see how one labeled training example. So let’s say you have one labeled training example, of someone saying a few specific words. So let’s play that audio clip here.Original Audio 0 -1-2-3-4-5. Alright, so someone counting from 0 to 5, and so you want to try to apply a learning algorithm to try to recognize the words said in that. So, how can we amplify the data set? Well, one thing we do is introduce additional audio distortions into the data set. So here I’m going to add background sounds to simulate a bad cell phone connection. When you hear beeping sounds, that’s actually part of the audio track, that’s nothing wrong with the speakers, I’m going to play this now. 0-1-2-3-4-5. Right, so you can listen to that sort of audio clip and recognize the sounds, that seems like another useful training example to have, here’s another example, noisy background. Zero, one, two, three four five you know of cars driving past, people walking in the background, here’s another one, so taking the original clean audio clip so taking the clean audio of someone saying 0 1 2 3 4 5 we can then automatically synthesize these additional training examples and thus amplify one training example into maybe four different training examples. So let me play this final example, as well. 0-1 3-4-5 So by taking just one labelled example, we have to go through the effort to collect just one labelled example of the 012 to 5, and by synthesizing additional distortions, by introducing different background sounds, we’ve now multiplied this one example into many more examples. Much work by just automatically adding these different background sounds to the clean audio. Just one word of warning about synthesizing data by introducing distortions: if you try to do this yourself, the distortions you introduce should be representative the source of noises, or distortions, that you might see in the test set. So, for the character recognition example, you know, the working things begin introduced are actually kind of reasonable, because an image A that looks like that, that’s, could be an image that we could actually see in a test set. Reflect a fact And, you know, that image on the upper-right, that could be an image that we could imagine seeing. And for audio, well, we do wanna recognize speech, even against a bad self internal connection, against different types of background noise, and so for the audio, we’re again synthesizing examples are actually representative of the sorts of examples that we want to classify, that we want to recognize correctly. In contrast, usually it does not help perhaps you actually a meaning as noise to your data. I’m not sure you can see this, but what we’ve done here is taken the image, and for each pixel, in each of these 4 images, has just added some random Gaussian noise to each pixel. To each pixel, is the pixel brightness, it would just add some, you know, maybe Gaussian random noise to each pixel. So it’s just a totally meaningless noise, right? And so, unless you’re expecting to see these sorts of pixel wise noise in your test set, this sort of purely random meaningless noise is less likely to be useful. But the process of artificial data synthesis it is you know a little bit of an art as well and sometimes you just have to try it and see if it works. But if you’re trying to decide what sorts of distortions to add, you know, do think about what other meaningful distortions you might add that will cause you to generate additional training examples that are at least somewhat representative of the sorts of images you expect to see in your test sets. Discussion on getting more dataFinally, to wrap up this video, I just wanna say a couple of words, more about this idea of getting lot’s of data via artificial data synthesis. As always, before expending a lot of effort, you know, figuring out how to create artificial training examples, it’s often a good practice is to make sure that you really have a low biased classifier and having a lot more training data will be of help. And standard way to do this is to plot the learning curves, and make sure that you only have a low bias as well, high variance classifier. Or if you don’t have a low bias classifier, you know, one other thing that’s worth trying is to keep increasing the number of features that your classifier has, increasing the number of hidden units in your network, saying, until you actually have a low bias falsifier, and only then, should you put the effort into creating a large, artificial training set, so what you really want to avoid is to, you know, spend a whole week or spend a few months figuring out how to get a great artificially synthesized data set. Only to realize afterward, that, you know, your learning algorithm, performance doesn’t improve that much, even when you’re given a huge training set. So that’s about my usual advice about of a testing that you really can make use of a large training set before spending a lot of effort going out to get that large training set. Second is, when i’m working on machine learning problems, one question I often ask the team I’m working with, often ask my students, which is, how much work would it be to get 10 times as much date as we currently had. When I face a new machine learning application very often I will sit down with a team and ask exactly this question, I’ve asked this question over and over and over and I’ve been very surprised how often this answer has been that. You know, it’s really not that hard, maybe a few days of work at most, to get ten times as much data as we currently have for a machine learning application and very often if you can get ten times as much data there will be a way to make your algorithm do much better. So, you know, if you ever join the product team working on some machine learning application product this is a very good questions ask yourself ask the team don’t be too surprised if after a few minutes of brainstorming if your team comes up with a way to get literally ten times this much data, in which case, I think you would be a hero to that team, because with 10 times as much data, I think you’ll really get much better performance, just from learning from so much data. So there are several waysand that comprised both the ideas of generating data from scratch using random fonts and so on. As well as the second idea of taking an existing example and and introducing distortions that amplify to enlarge the training set A couple of other examples of ways to get a lot more data are to collect the data or to label them yourself. So one useful calculation that I often do is, you know, how many minutes, how many hours does it take to get a certain number of examples, so actually sit down and figure out, you know, suppose it takes me ten seconds to label one example then and, suppose that, for our application, currently we have 1000 labeled examples examples so ten times as much of that would be if n were equal to ten thousand. A second way to get a lot of data is to just collect the data and you label it yourself. So what I mean by this is I will often set down and do a calculation to figure out how much time, you know just like how many hours will it take, how many hours or how many days will it take for me or for someone else to just sit down and collect ten times as much data, as we have currently, by collecting the data ourselves and labeling them ourselves. So, for example, that, for our machine learning application, currently we have 1,000 examples, so M 1,000. That what we do is sit down and ask, how long does it take me really to collect and label one example. And sometimes maybe it will take you, you know ten seconds to label one new example, and so if I want 10 X as many examples, I’d do a calculation. If it takes me 10 seconds to get one training example. If I wanted to get 10 times as much data, then I need 10,000 examples. So I do the calculation, how long is it gonna take to label, to manually label 10,000 examples, if it takes me 10 seconds to label 1 example. So when you do this calculation, often I’ve seen many you would be surprised, you know, how little, or sometimes a few days at work, sometimes a small number of days of work, well I’ve seen many teams be very surprised that sometimes how little work it could be, to just get a lot more data, and let that be a way to give your learning app to give you a huge boost in performance, and necessarily, you know, sometimes when you’ve just managed to do this, you will be a hero and whatever product development, whatever team you’re working on, because this can be a great way to get much better performance. Third and finally, one sometimes good way to get a lot of data is to use what’s now called crowd sourcing. So today, there are a few websites or a few services that allow you to hire people on the web to, you know, fairly inexpensively label large training sets for you. So this idea of crowd sourcing, or crowd sourced data labeling, is something that has, is obviously, like an entire academic literature, has some of it’s own complications and so on, pertaining to labeler reliability. Maybe, you know, hundreds of thousands of labelers, around the world, working fairly inexpensively to help label data for you, and that I’ve just had mentioned, there’s this one alternative as well. And probably Amazon Mechanical Turk systems is probably the most popular crowd sourcing option right now. This is often quite a bit of work to get to work, if you want to get very high quality labels, but is sometimes an option worth considering as well. If you want to try to hire many people, fairly inexpensively on the web, our labels launch miles of data for you. So this video, we talked about the idea of artificial data synthesis of either creating new data from scratch, looking, using the ramming funds as an example, or by amplifying an existing training set, by taking existing label examples and introducing distortions to it, to sort of create extra label examples. And finally, one thing that I hope you remember from this video this idea of if you are facing a machine learning problem, it is often worth doing two things. One just a sanity check, with learning curves, that having more data would help. And second, assuming that that’s the case, I will often seat down and ask yourself seriously: what would it take to get ten times as much creative data as you currently have, and not always, but sometimes, you may be surprised by how easy that turns out to be, maybe a few days, a few weeks at work, and that can be a great way to give your learning algorithm a huge boost in performance 04_ceiling-analysis-what-part-of-the-pipeline-to-work-on-nextIn earlier videos, I’ve said over and over that, when you’re developing a machine learning system, one of the most valuable resources is your time as the developer, in terms of picking what to work on next. Or, if you have a team of developers or a team of engineers working together on a machine learning system. Again, one of the most valuable resources is the time of the engineers or the developers working on the system. And what you really want to avoid is that you or your colleagues your friends spend a lot of time working on some component. Only to realize after weeks or months of time spent, that all that worked just doesn’t make a huge difference on the performance of the final system. In this video what I’d like to do is something called ceiling analysis. When you’re the team working on the pipeline machine on your system, this can sometimes give you a very strong signal, a very strong guidance on what parts of the pipeline might be the best use of your time to work on. To talk about ceiling analysis I’m going to keep on using the example of the photo OCR pipeline. And see right here each of these boxes, text detection, character segmentation, character recognition, each of these boxes can have even a small engineering team working on it. Or maybe the entire system is just built by you, either way.But the question is where should you allocate resources? Which of these boxes is most worth your effort of trying to improve the performance of. In order to explain the idea of ceiling analysis, I’m going to keep using the example of our photo OCR pipeline. As I mentioned earlier, each of these boxes here, each of these machines and components could be the work of a small team of engineers, or the whole system could be built by just one person. But the question is, where should you allocate scarce resources? That is, which of these components, which one or two or maybe all three of these components is most worth your time, to try to improve the performance of. So here’s the idea of ceiling analysis. As in the development process for other machine learning systems as well, in order to make decisions on what to do for developing the system is going to be very helpful to have a single rolled number evaluation metric for this learning system. So let’s say we pick character level accuracy. So if you’re given a test set image, what is the fraction of alphabets or characters in a test image that we recognize correctly? Or you can pick some other single road number evaluation that you could, if you want. But let’s say for whatever evaluation measure we pick, we find that the overall system currently has 72% accuracy. So in other words, we have some set of test set images. And from each test set images, we run it through text detection, then character segmentation, then character recognition. And we find that on our test set the overall accuracy of the entire system was 72% on whatever metric you chose. Now here’s the idea behind ceiling analysis, which is that we’re going to go through, let’s say the first module of our machinery pipeline, say text detection. And what we’re going to do, is we’re going to monkey around with the test set. We’re gonna go to the test set. For every test example, which is going to provide it the correct text detection outputs, so in other words, we’re going to go to the test set and just manually tell the algorithm where the text is in each of the test examples. So in other words gonna simulate what happens if you have a text detection system with a hundred percent accuracy, for the purpose of detecting text in an image. And really the way you do that’s pretty simple, right? Instead of letting your learning algorhtim detect the text in the images. You wouldn’t say go to the images and just manually label what is the location of the text in my test set image. And you would then let these correct or let these ground truth labels of where is the text be part of your test set. And just use these ground truth labels as what you feed in to the next stage of the pipeline, so the character segmentation pipeline. Okay? So just to say that again. By putting a checkmark over here, what I mean is I’m going to go to my test set and just give it the correct answers. Give it the correct labels for the text detection part of the pipeline. So that as if I have a perfect test detection system on my test set. What we need to do then is run this data through the rest of the pipeline. Through character segmentation and character recognition. And then use the same evaluation metric as before, to measure what was the overall accuracy of the entire system. And with perfect text detection, hopefully the performance will go up. And in this example, it goes up by by 89%. And then we’re gonna keep going, let’s go to the next stage of the pipeline, so character segmentation. So again, I’m gonna go to my test set, and now I’m going to give it the correct text detection output and give it the correct character segmentation output. So go to the test set and manually label the correct segmentations of the text into individual characters, and see how much that helps. And let’s say it goes up to 90% accuracy for the overall system. Right? So as always the accuracy of the overall system. So is whatever the final output of the character recognition system is. Whatever the final output of the overall pipeline, is going to measure the accuracy of that. And finally I’m going to build a character recognition system and give that correct labels as well, and if I do that too then no surprise I should get 100% accuracy. Now the nice thing about having done this analysis is, we can now understand what is the upside potential of improving each of these components? So we see that if we get perfect text detection, our performance went up from 72 to 89%. So that’s a 17% performance gain. So this means that if we take our current system we spend a lot of time improving text detection, that means that we could potentially improve our system’s performance by 17%. It seems like it’s well worth our while. Whereas in contrast, when going from text detection when we gave it perfect character segmentation, performance went up only by 1%, so that’s a more sobering message. It means that no matter how much time you spend on character segmentation. Maybe the upside potential is going to be pretty small, and maybe you do not want to have a large team of engineers working on character segmentation. This sort of analysis shows that even when you give it the perfect character segmentation, you performance goes up by only one percent. That really estimates what is the ceiling, or what is an upper bound on how much you can improve the performance of your system and working on one of these components. And finally, going from character, when we get better character recognition with the forms went up by ten percent. So again you can decide is ten percent improvement, how much is worth your while? This tells you that maybe with more effort spent on the last stage of the pipeline, you can improve the performance of the systems as well. Another way of thinking about this, is that by going through these sort of analysis you’re trying to think about what is the upside potential of improving each of these components. Or how much could you possibly gain if one of these components became absolutely perfect? And this really places an upper bound on the performance of that system. So the idea of ceiling analysis is pretty important, let me just answer this idea again but with a different example but more complex one. Let’s say that you want to do face recognition from images. You want to look at the picture and recognize whether or not the person in this picture is a particular friend of yours, and try to recognize the person Shown in this image. This is a slightly artificial example, this isn’t actually how face recognition is done in practice. But we’re going to set for an example, what a pipeline might look like to give you another example of how a ceiling analysis process might look. So we have a camera image, and let’s say that we design a pipeline as follows, the first thing you wanna do is pre-processing of the image. So let’s take this image like we have shown on the upper right, and let’s say we want to remove the background. So do pre-processing and the background disappears. Next we want to say detect the face of the person, that’s usually done on the learning So we’ll run a sliding Windows crossfire to draw a box around a person’s face. Having detected the face, it turns out that if you want to recognize people, it turns out that the eyes is a highly useful cue. We actually are, in terms of recognizing your friends the appearance of their eyes is actually one of the most important cues that you use. So lets run another crossfire to detect the eyes of the person. So the segment of the eyes and then since this will give us useful features to recognize the person. And then other parts of the face of physical interest. Maybe segment of the nose, segment of the mouth. And then having found the eyes, the nose, and the mouth, all of these give us useful features to maybe feed into a logistic regression classifier. And there’s a job with a cost priority, they’d give us the overall label, to find the label for who we think is the identity of this person. So this is a kind of complicated pipeline, it’s actually probably more complicated than you should be using if you actually want to recognize people, but there’s an illustrative example that’s useful to think about for ceiling analysis. So how do you go through ceiling analysis for this pipeline. Well se step through these pieces one at a time. Let’s say your overall system has 85% accuracy. The first thing I do is go to my test set and manually give it the full background segmentation. So manually go to the test set. And use Photoshop or something to just tell it where’s the background and just manually remove the graph background, so this is a ground true background, and see how much the accuracy changes. In this example the accuracy goes up by 0.1%. So this is a strong sign that even if you have perfect background segmentation, the form is, even with perfect background removal the performance or your system isn’t going to go up that much. So it’s maybe not worth a huge effort to work on pre-processing on background removal. Then quickly goes to test set give it the correct face detection images then again step though the eyes nose and mouth segmentation in some order just pick one order. Just give the correct location of the eyes. Correct location in noses, correct location in mouth, and then finally if I just give it the correct overall label I can get 100% accuracy. And so as I go through the system and just give more and more components, the correct labels in the test set, the performance of the overall system goes up and you can look at how much the performance went up on different steps. So from giving it the perfect face detection, it looks like the overall performance of the system went up by 5.9%. So that’s a pretty big jump. It means that maybe it’s worth quite a bit effort on better face detection. Went up 4% there, it went up 1% there. 1% there, and 3% there. So it looks like the components that most work are while are, when I gave it perfect face detection system went up by 5.9 performance when given perfect eyes segmentation went to four percent. And then my final which is cost for well there’s another three percent, gap there maybe. And so this tells maybe whether the components are most worthwhile working on. And by the way I want to tell you a true cautionary story. The reason I put this is in this in preprocessing background removal is because I actually know of a true story where there was a research team that actually literally had to people spend about a year and a half, spend 18 months working on better background removal. But actually I’m obscuring the details for obvious reasons, but there was a computer vision application where there’s a team of two engineers that literally spent about a year and a half working on better background removal, actually worked out really complicated algorithms and ended up publishing one research paper. But after all that work they found that it just did not make huge difference to the overall performance of the actual application they were working on and if only someone were to do ceiling analysis before hand maybe they could have realized. And one of them said to me afterward. If only you’ve did this sort of analysis like this maybe they could have realized before their 18 months of work. That they should have spend their effort focusing on some different component then literally spending 18 months working on background removal. So to summarize, pipelines are pretty pervasive in complex machine learning applications. And when you’re working on a big machine learning application, your time as developer is so valuable, so just don’t waste your time working on something that ultimately isn’t going to matter. And in this video we’ll talk about this idea of ceiling analysis, which I’ve often found to be a very good tool for identifying the component of a video as you put focus on that component and make a big difference. Will actually have a huge effect on the overall performance of your final system. So over the years working machine learning, I’ve actually learned to not trust my own gut feeling about what components to work on. So very often, I’ve work on machine learning for a long time, but often I look at a machine learning problem, and I may have some gut feeling about oh, let’s jump on that component and just spend all the time on that. But over the years, I’ve come to even trust my own gut feelings and learn not to trust gut feelings that much. And instead, if you have a sort of machine learning problem where it’s possible to structure things and do a ceiling analysis, often there’s a much better and much more reliable way for deciding where to put a focused effort, to really improve the performance of some component. And be kind of reassured that, when you do that, it won’t actually have a huge effect on the final performance of the overall system.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[17_large-scale-machine-learning note17]]></title>
    <url>%2F2018%2F01%2F17%2F17_large-scale-machine-learning%2F</url>
    <content type="text"><![CDATA[01_gradient-descent-with-large-datasets01_learning-with-large-datasetsIn the next few videos, we’ll talk about large scale machine learning. That is, algorithms but viewing with big data sets. If you look back at a recent 5 or 10-year history of machine learning. One of the reasons that learning algorithms work so much better now than even say, 5-years ago, is just the sheer amount of data that we have now and that we can train our algorithms on. In these next few videos, we’ll talk about algorithms for dealing when we have such massive data sets. So why do we want to use such large data sets? We’ve already seen that one of the best ways to get a high performance machine learning system, is if you take a low-bias learning algorithm, and train that on a lot of data. And so, one early example we have already seen was this example of classifying between confusable words. So, for breakfast, I ate two (TWO) eggs and we saw in this example, these sorts of results, where, you know, so long as you feed the algorithm a lot of data, it seems to do very well. And so it’s results like these that has led to the saying in machine learning that often it’s not who has the best algorithm that wins. It’s who has the most data. So you want to learn from large data sets, at least when we can get such large data sets. But learning with large data sets comes with its own unique problems, specifically, computational problems. Let’s say your training set size is M equals 100,000,000. And this is actually pretty realistic for many modern data sets. If you look at the US Census data set, if there are, you know, 300 million people in the US, you can usually get hundreds of millions of records. If you look at the amount of traffic that popular websites get, you easily get training sets that are much larger than hundreds of millions of examples. And let’s say you want to train a linear regression model, or maybe a logistic regression model, in which case this is the gradient descent rule. And if you look at what you need to do to compute the gradient, which is this term over here, then when M is a hundred million, you need to carry out a summation over a hundred million terms, in order to compute these derivatives terms and to perform a single step of decent. Because of the computational expense of summing over a hundred million entries in order to compute just one step of gradient descent, in the next few videos we’ve talk about techniques for either replacing this with something else or to find more efficient ways to compute this derivative. By the end of this sequence of videos on large scale machine learning, you know how to fit models, linear regression, logistic regression, neural networks and so on even today’s data sets with, say, a hundred million examples. Of course, before we put in the effort into training a model with a hundred million examples, We should also ask ourselves, well, why not use just a thousand examples. Maybe we can randomly pick the subsets of a thousand examples out of a hundred million examples and train our algorithm on just a thousand examples. So before investing the effort into actually developing and the software needed to train these massive models is often a good sanity check, if training on just a thousand examples might do just as well. The way to sanity check of using a much smaller training set might do just as well, that is if using a much smaller n equals 1000 size training set, that might do just as well, it is the usual method of plotting the learning curves, so if you were to plot the learning curves and if your training objective were to look like this, that’s J train theta. And if your cross-validation set objective, Jcv of theta would look like this, then this looks like a high-variance learning algorithm, and we will be more confident that adding extra training examples would improve performance. Whereas in contrast if you were to plot the learning curves, if your training objective were to look like this, and if your cross-validation objective were to look like that, then this looks like the classical high-bias learning algorithm. And in the latter case, you know, if you were to plot this up to, say, m equals 1000 and so that is m equals 500 up to m equals 1000, then it seems unlikely that increasing m to a hundred million will do much better and then you’d be just fine sticking to n equals 1000, rather than investing a lot of effort to figure out how the scale of the algorithm. Of course, if you were in the situation shown by the figure on the right, then one natural thing to do would be to add extra features, or add extra hidden units to your neural network and so on, so that you end up with a situation closer to that on the left, where maybe this is up to n equals 1000, and this then gives you more confidence that trying to add infrastructure to change the algorithm to use much more than a thousand examples that might actually be a good use of your time. So in large-scale machine learning, we like to come up with computationally reasonable ways, or computationally efficient ways, to deal with very big data sets. In the next few videos, we’ll see two main ideas. The first is called stochastic gradient descent and the second is called Map Reduce, for viewing with very big data sets. And after you’ve learned about these methods, hopefully that will allow you to scale up your learning algorithms to big data and allow you to get much better performance on many different applications. 02_stochastic-gradient-descentFor many learning algorithms, among them linear regression, logistic regression and neural networks, the way we derive the algorithm was by coming up with a cost function or coming up with an optimization objective. And then using an algorithm like gradient descent to minimize that cost function. We have a very large training set gradient descent becomes a computationally very expensive procedure. In this video, we’ll talk about a modification to the basic gradient descent algorithm called Stochastic gradient descent, which will allow us to scale these algorithms to much bigger training sets. Suppose you are training a linear regression model using gradient descent. As a quick recap, the hypothesis will look like this, and the cost function will look like this, which is the sum of one half of the average square error of your hypothesis on your m training examples, and the cost function we’ve already seen looks like this sort of bow-shaped function. So, plotted as function of the parameters theta 0 and theta 1, the cost function J is a sort of a bow-shaped function. And gradient descent looks like this, where in the inner loop of gradient descent you repeatedly update the parameters theta using that expression. Now in the rest of this video, I’m going to keep using linear regression as the running example. But the ideas here, the ideas of Stochastic gradient descent is fully general and also applies to other learning algorithms like logistic regression, neural networks and other algorithms that are based on training gradient descent on a specific training set. So here’s a picture of what gradient descent does, if the parameters are initialized to the point there then as you run gradient descent different iterations of gradient descent will take the parameters to the global minimum. So take a trajectory that looks like that and heads pretty directly to the global minimum. Now, the problem with gradient descent is that if m is large. Then computing this derivative term can be very expensive, because the surprise, summing over all m examples. So if m is 300 million, alright. So in the United States, there are about 300 million people. And so the US or United States census data may have on the order of that many records. So you want to fit the linear regression model to that then you need to sum over 300 million records. And that’s very expensive. To give the algorithm a name, this particular version of gradient descent is also called Batch gradient descent. And the term Batch refers to the fact that we’re looking at all of the training examples at a time. We call it sort of a batch of all of the training examples. And it really isn’t the, maybe the best name but this is what machine learning people call this particular version of gradient descent. And if you imagine really that you have 300 million census records stored away on disc. The way this algorithm works is you need to read into your computer memory all 300 million records in order to compute this derivative term. You need to stream all of these records through computer because you can’t store all your records in computer memory. So you need to read through them and slowly, you know, accumulate the sum in order to compute the derivative. And then having done all that work, that allows you to take one step of gradient descent. And now you need to do the whole thing again. You know, scan through all 300 million records, accumulate these sums. And having done all that work, you can take another little step using gradient descent. And then do that again. And then you take yet a third step. And so on. And so it’s gonna take a long time in order to get the algorithm to converge. In contrast to Batch gradient descent, what we are going to do is come up with a different algorithm that doesn’t need to look at all the training examples in every single iteration, but that needs to look at only a single training example in one iteration. Before moving on to the new algorithm, here’s just a Batch gradient descent algorithm written out again with that being the cost function and that being the update and of course this term here, that’s used in the gradient descent rule, that is the partial derivative with respect to the parameters theta J of our optimization objective, J train of theta. Now, let’s look at the more efficient algorithm that scales better to large data sets. In order to work off the algorithms called Stochastic gradient descent, this vectors the cost function in a slightly different way then they define the cost of the parameter theta with respect to a training example x(i), y(i) to be equal to one half times the squared error that my hypothesis incurs on that example, x(i), y(i). So this cost function term really measures how well is my hypothesis doing on a single example x(i), y(i). Now you notice that the overall cost function j train can now be written in this equivalent form. So j train is just the average over my m training examples of the cost of my hypothesis on that example x(i), y(i). Armed with this view of the cost function for linear regression, let me now write out what Stochastic gradient descent does. The first step of Stochastic gradient descent is to randomly shuffle the data set. So by that I just mean randomly shuffle, or randomly reorder your m training examples. It’s sort of a standard pre-processing step, come back to this in a minute. But the main work of Stochastic gradient descent is then done in the following. We’re going to repeat for i equals 1 through m. So we’ll repeatedly scan through my training examples and perform the following update. Gonna update the parameter theta j as theta j minus alpha times h of x(i) minus y(i) times x(i)j. And we’re going to do this update as usual for all values of j. Now, you notice that this term over here is exactly what we had inside the summation for Batch gradient descent. In fact, for those of you that are calculus is possible to show that that term here, that’s this term here, is equal to the partial derivative with respect to my parameter theta j of the cost of the parameters theta on x(i), y(i). Where cost is of course this thing that was defined previously. And just the wrap of the algorithm, let me close my curly braces over there. So what Stochastic gradient descent is doing is it is actually scanning through the training examples. And first it’s gonna look at my first training example x(1), y(1). And then looking at only this first example, it’s gonna take like a basically a little gradient descent step with respect to the cost of just this first training example. So in other words, we’re going to look at the first example and modify the parameters a little bit to fit just the first training example a little bit better. Having done this inside this inner for-loop is then going to go on to the second training example. And what it’s going to do there is take another little step in parameter space, so modify the parameters just a little bit to try to fit just a second training example a little bit better. Having done that, is then going to go onto my third training example. And modify the parameters to try to fit just the third training example a little bit better, and so on until you know, you get through the entire training set. And then this ultra repeat loop may cause it to take multiple passes over the entire training set. This view of Stochastic gradient descent also motivates why we wanted to start by randomly shuffling the data set. This doesn’t show us that when we scan through the training site here, that we end up visiting the training examples in some sort of randomly sorted order. Depending on whether your data already came randomly sorted or whether it came originally sorted in some strange order, in practice this would just speed up the conversions to Stochastic gradient descent just a little bit. So in the interest of safety, it’s usually better to randomly shuffle the data set if you aren’t sure if it came to you in randomly sorted order. But more importantly another view of Stochastic gradient descent is that it’s a lot like descent but rather than wait to sum up these gradient terms over all m training examples, what we’re doing is we’re taking this gradient term using just one single training example and we’re starting to make progress in improving the parameters already. So rather than, you know, waiting ‘till taking a path through all 300,000 United States Census records, say, rather than needing to scan through all of the training examples before we can modify the parameters a little bit and make progress towards a global minimum. For Stochastic gradient descent instead we just need to look at a single training example and we’re already starting to make progress in this case of parameters towards, moving the parameters towards the global minimum. So, here’s the algorithm written out again where _the first step_ is to randomly shuffle the data and _the second step_ is where the real work is done, where that’s the update theta with respect to a single training example x(i), y(i). So, let’s see what this algorithm does to the parameters. Previously, we saw that when we are using Batch gradient descent, that is the algorithm that looks at all the training examples in time, Batch gradient descent will tend to, you know, take a reasonably straight line trajectory to get to the global minimum like that. In contrast with Stochastic gradient descent every iteration is going to be much faster because we don’t need to sum up over all the training examples. But every iteration is just trying to fit single training example better. So, if we were to start stochastic gradient descent, oh, let’s start stochastic gradient descent at a point like that. The first iteration, you know, may take the parameters in that direction and maybe the second iteration looking at just the second example maybe just by chance, we get more unlucky and actually head in a bad direction with the parameters like that. In the third iteration where we tried to modify the parameters to fit just the third training examples better, maybe we’ll end up heading in that direction. And then we’ll look at the fourth training example and we will do that. The fifth example, sixth example, 7th and so on. And as you run Stochastic gradient descent, what you find is that it will generally move the parameters in the direction of the global minimum, but not always. And so take some more random-looking, circuitous path to watch the global minimum. And in fact as you run Stochastic gradient descent it doesn’t actually converge in the same same sense as Batch gradient descent does and what it ends up doing is wandering around continuously in some region that’s in some region close to the global minimum, but it doesn’t just get to the global minimum and stay there. But in practice this isn’t a problem because, you know, so long as the parameters end up in some region there maybe it is pretty close to the global minimum. So, as parameters end up pretty close to the global minimum, that will be a pretty good hypothesis and so usually running Stochastic gradient descent we get a parameter near the global minimum and that’s good enough for, you know, essentially any, most practical purposes. Just one final detail. In Stochastic gradient descent, we had this outer loop repeat which says to do this inner loop multiple times. So, how many times do we repeat this outer loop? Depending on the size of the training set, doing this loop just a single time may be enough. And up to, you know, maybe 10 times may be typical so we may end up repeating this inner loop anywhere from once to ten times. So if we have a you know, truly massive data set like the this US census gave us that example that I’ve been talking about with 300 million examples, it is possible that by the time you’ve taken just a single pass through your training set. So, this is for i equals 1 through 300 million. It’s possible that by the time you’ve taken a single pass through your data set you might already have a perfectly good hypothesis. In which case, you know, this inner loop you might need to do only once if m is very, very large. But in general taking anywhere from 1 through 10 passes through your data set, you know, maybe fairly common. But really it depends on the size of your training set. And if you contrast this to Batch gradient descent. With Batch gradient descent, after taking a pass through your entire training set, you would have taken just one single gradient descent steps. So one of these little baby steps of gradient descent where you just take one small gradient descent step and this is why Stochastic gradient descent can be much faster. So, that was the Stochastic gradient descent algorithm. And if you implement it, hopefully that will allow you to scale up many of your learning algorithms to much bigger data sets and get much more performance that way. 03_mini-batch-gradient-descentIn the previous video, we talked about Stochastic gradient descent, and how that can be much faster than Batch gradient descent. In this video, let’s talk about another variation on these ideas is called Mini-batch gradient descent they can work sometimes even a bit faster than stochastic gradient descent. To summarize the algorithms we talked about so far. In Batch gradient descent we will use all m examples in each generation. Whereas in Stochastic gradient descent we will use a single example in each generation. What Mini-batch gradient descent does is somewhere in between. Specifically, with this algorithm we’re going to use b examples in each iteration where b is a parameter called the “mini batch size” so the idea is that this is somewhat in-between Batch gradient descent and Stochastic gradient descent. This is just like batch gradient descent, except that I’m going to use a much smaller batch size. A typical choice for the value of b might be b equals 10, lets say, and a typical range really might be anywhere from b equals 2 up to b equals 100. So that will be a pretty typical range of values for the Mini-batch size. And the idea is that rather than using one example at a time or m examples at a time we will use b examples at a time. So let me just write this out informally, we’re going to get, let’s say, b. For this example, let’s say b equals 10. So we’re going to get, the next 10 examples from my training set so that may be some set of examples xi, yi. If it’s 10 examples then the indexing will be up to x (i+9), y (i+9) so that’s 10 examples altogether and then we’ll perform essentially a gradient descent update using these 10 examples. So, that’s any rate times one tenth times sum over k equals i through i+9 of h subscript theta of x(k) minus y(k) times x(k)j. And so in this expression, where summing the gradient terms over my ten examples. So, that’s number ten, that’s, you know, my mini batch size and just i+9 again, the 9 comes from the choice of the parameter b, and then after this we will then increase, you know, i by tenth, we will go on to the next ten examples and then keep moving like this. So just to write out the entire algorithm in full. In order to simplify the indexing for this one at the right top, I’m going to assume we have a mini-batch size of ten and a training set size of a thousand, what we’re going to do is have this sort of form, for i equals 1 and that in 21’s the stepping, in steps of 10 because we look at 10 examples at a time. And then we perform this sort of gradient descent update using ten examples at a time so this 10 and this i+9 those are consequence of having chosen my mini-batch to be ten. And you know, this ultimate four-loop, this ends at 991 here because if I have 1000 training samples then I need 100 steps of size 10 in order to get through my training set. So this is mini-batch gradient descent. Compared to batch gradient descent, this also allows us to make progress much faster. So we have again our running example of, you know, U.S. Census data with 300 million training examples, then what we’re saying is after looking at just the first 10 examples we can start to make progress in improving the parameters theta so we don’t need to scan through the entire training set. We just need to look at the first 10 examples and this will start letting us make progress and then we can look at the second ten examples and modify the parameters a little bit again and so on. So, that is why Mini-batch gradient descent can be faster than batch gradient descent. Namely, you can start making progress in modifying the parameters after looking at just ten examples rather than needing to wait ‘till you’ve scan through every single training example of 300 million of them. So, how about Mini-batch gradient descent versus Stochastic gradient descent. So, why do we want to look at b examples at a time rather than look at just a single example at a time as the Stochastic gradient descent? The answer is in vectorization. In particular, Mini-batch gradient descent is likely to outperform Stochastic gradient descent only if you have a good vectorized implementation. In that case, the sum over 10 examples can be performed in a more vectorized way which will allow you to partially parallelize your computation over the ten examples. So, in other words, by using appropriate vectorization to compute the rest of the terms, you can sometimes partially use the good numerical algebra libraries and parallelize your gradient computations over the b examples, whereas if you were looking at just a single example of time with Stochastic gradient descent then, you know, just looking at one example at a time their isn’t much to parallelize over. At least there is less to parallelize over. One disadvantage of Mini-batch gradient descent is that there is now this extra parameter b, the Mini-batch size which you may have to fiddle with, and which may therefore take time. But if you have a good vectorized implementation this can sometimes run even faster that Stochastic gradient descent. So that was Mini-batch gradient descent which is an algorithm that in some sense does something that’s somewhat in between what Stochastic gradient descent does and what Batch gradient descent does. And if you choose their reasonable value of b. I usually use b equals 10, but, you know, other values, anywhere from say 2 to 100, would be reasonably common. So we choose value of b and if you use a good vectorized implementation, sometimes it can be faster than both Stochastic gradient descent and faster than Batch gradient descent. 04_stochastic-gradient-descent-convergenceYou now know about the stochastic gradient descent algorithm. But when you’re running the algorithm, how do you make sure that it’s completely debugged and is converging okay? Equally important, how do you tune the learning rate alpha with Stochastic Gradient Descent. In this video we’ll talk about some techniques for doing these things, for making sure it’s converging and for picking the learning rate alpha. Back when we were using batch gradient descent, our standard way for making sure that gradient descent was converging was we would plot the optimization cost function as a function of the number of iterations. So that was the cost function and we would make sure that this cost function is decreasing on every iteration. When the training set sizes were small, we could do that because we could compute the sum pretty efficiently. But when you have a massive training set size then you don’t want to have to pause your algorithm periodically. You don’t want to have to pause stochastic gradient descent periodically in order to compute this cost function since it requires a sum of your entire training set size. And the whole point of stochastic gradient was that you wanted to start to make progress after looking at just a single example without needing to occasionally scan through your entire training set right in the middle of the algorithm, just to compute things like the cost function of the entire training set. So for stochastic gradient descent, in order to check the algorithm is converging, here’s what we can do instead. Let’s take the definition of the cost that we had previously. So the cost of the parameters theta with respect to a single training example is just one half of the square error on that training example. Then, while stochastic gradient descent is learning, right before we train on a specific example. So, in stochastic gradient descent we’re going to look at the examples xi, yi, in order, and then sort of take a little update with respect to this example. And we go on to the next example, xi plus 1, yi plus 1, and so on, right? That’s what stochastic gradient descent does. So, while the algorithm is looking at the example xi, yi, but before it has updated the parameters theta using that an example, let’s compute the cost of that example. Just to say the same thing again, but using slightly different words. A stochastic gradient descent is scanning through our training set right before we have updated theta using a specific training example x(i) comma y(i) let’s compute how well our hypothesis is doing on that training example. And we want to do this before updating theta because if we’ve just updated theta using example, you know, that it might be doing better on that example than what would be representative. Finally, in order to check for the convergence of stochastic gradient descent, what we can do is every, say, every thousand iterations, we can plot these costs that we’ve been computing in the previous step. We can plot those costs average over, say, the last thousand examples processed by the algorithm. And if you do this, it kind of gives you a running estimate of how well the algorithm is doing. on, you know, the last 1000 training examples that your algorithm has seen. So, in contrast to computing Jtrain periodically which needed to scan through the entire training set. With this other procedure, well, as part of stochastic gradient descent, it doesn’t cost much to compute these costs as well right before updating to parameter theta. And all we’re doing is every thousand integrations or so, we just average the last 1,000 costs that we computed and plot that. And by looking at those plots, this will allow us to check if stochastic gradient descent is converging. So here are a few examples of what these plots might look like. Suppose you have plotted the cost average over the last thousand examples, because these are averaged over just a thousand examples, they are going to be a little bit noisy and so, it may not decrease on every single iteration. Then if you get a figure that looks like this, So the plot is noisy because it’s average over, you know, just a small subset, say a thousand training examples. If you get a figure that looks like this, you know that would be a pretty decent run with the algorithm, maybe, where it looks like the cost has gone down and then this plateau that looks kind of flattened out, you know, starting from around that point. look like, this is what your cost looks like then maybe your learning algorithm has converged. If you want to try using a smaller learning rate, something you might see is that the algorithm may initially learn more slowly so the cost goes down more slowly. But then eventually you have a smaller learning rate is actually possible for the algorithm to end up at a, maybe very slightly better solution. So the red line may represent the behavior of stochastic gradient descent using a slower, using a smaller leaning rate. And the reason this is the case is because, you remember, stochastic gradient descent doesn’t just converge to the global minimum, is that what it does is the parameters will oscillate a bit around the global minimum. And so by using a smaller learning rate, you’ll end up with smaller oscillations. And sometimes this little difference will be negligible and sometimes with a smaller than you can get a slightly better value for the parameters. Here are some other things that might happen. Let’s say you run stochastic gradient descent and you average over a thousand examples when plotting these costs. So, you know, here might be the result of another one of these plots. Then again, it kind of looks like it’s converged. If you were to take this number, a thousand, and increase to averaging over 5 thousand examples. Then it’s possible that you might get a smoother curve that looks more like this. And by averaging over, say 5,000 examples instead of 1,000, you might be able to get a smoother curve like this. And so that’s the effect of increasing the number of examples you average over. The disadvantage of making this too big of course is that now you get one date point only every 5,000 examples. And so the feedback you get on how well your learning learning algorithm is doing is, sort of, maybe it’s more delayed because you get one data point on your plot only every 5,000 examples rather than every 1,000 examples. Along a similar vein some times you may run a gradient descent and end up with a plot that looks like this. And with a plot that looks like this, you know, it looks like the cost just is not decreasing at all. It looks like the algorithm is just not learning. It’s just, looks like this here a flat curve and the cost is just not decreasing. But again if you were to increase this to averaging over a larger number of examples it is possible that you see something like this red line it looks like the cost actually is decreasing, it’s just that the blue line averaging over 2, 3 examples, the blue line was too noisy so you couldn’t see the actual trend in the cost actually decreasing and possibly averaging over 5,000 examples instead of 1,000 may help. Of course we averaged over a larger number examples that we’ve averaged here over 5,000 examples, I’m just using a different color, it is also possible that you that see a learning curve ends up looking like this. That it’s still flat even when you average over a larger number of examples. And as you get that, then that’s maybe just a more firm verification that unfortunately the algorithm just isn’t learning much for whatever reason. And you need to either change the learning rate or change the features or change something else about the algorithm. Finally, one last thing that you might see would be if you were to plot these curves and you see a curve that looks like this, where it actually looks like it’s increasing. And if that’s the case then this is a sign that the algorithm is diverging. And what you really should do is use a smaller value of the learning rate alpha. So hopefully this gives you a sense of the range of phenomena you might see when you plot these cost average over some range of examples as well as suggests the sorts of things you might try to do in response to seeing different plots. So if the plots looks too noisy, or if it wiggles up and down too much, then try increasing the number of examples you’re averaging over so you can see the overall trend in the plot better. And if you see that the errors are actually increasing, the costs are actually increasing, try using a smaller value of alpha. Finally, it’s worth examining the issue of the learning rate just a little bit more. We saw that when we run stochastic gradient descent, the algorithm will start here and sort of meander towards the minimum And then it won’t really converge, and instead it’ll wander around the minimum forever. And so you end up with a parameter value that is hopefully close to the global minimum that won’t be exact at the global minimum. In most typical implementations of stochastic gradient descent, the learning rate alpha is typically held constant. And so what you we end up is exactly a picture like this. If you want stochastic gradient descent to actually converge to the global minimum, there’s one thing which you can do which is you can slowly decrease the learning rate alpha over time. So, a pretty typical way of doing that would be to set alpha equals some constant 1 divided by iteration number plus constant 2. So, iteration number is the number of iterations you’ve run of stochastic gradient descent, so it’s really the number of training examples you’ve seen And const 1 and const 2 are additional parameters of the algorithm that you might have to play with a bit in order to get good performance. One of the reasons people tend not to use this is because you end up needing to spend time playing with these 2 extra parameters, constant 1 and constant 2, and so this makes the algorithm more finicky. You know, it’s just more parameters able to fiddle with in order to make the algorithm work well. But if you manage to tune the parameters well, then the picture you can get is that the algorithm will actually meander around towards the minimum, but as it gets closer because you’re decreasing the learning rate the meanderings will get smaller and smaller until it pretty much just to the global minimum. I hope this makes sense, right? And the reason this formula makes sense is because as the algorithm runs, the iteration number becomes large So alpha will slowly become small, and so you take smaller and smaller steps until it hopefully converges to the global minimum. So If you do slowly decrease alpha to zero you can end up with a slightly better hypothesis. But because of the extra work needed to fiddle with the constants and because frankly usually we’re pretty happy with any parameter value that is, you know, pretty close to the global minimum. Typically this process of decreasing alpha slowly is usually not done and keeping the learning rate alpha constant is the more common application of stochastic gradient descent although you will see people use either version. To summarize in this video we talk about a way for approximately monitoring how the stochastic gradient descent is doing in terms for optimizing the cost function. And this is a method that does not require scanning over the entire training set periodically to compute the cost function on the entire training set. But instead it looks at say only the last thousand examples or so. And you can use this method both to make sure the stochastic gradient descent is okay and is converging or to use it to tune the learning rate alpha. summaryLearning with Large DatasetsWe mainly benefit from a very large dataset when our algorithm has high variance when m is small. Recall that if our algorithm has high bias, more data will not have any benefit.Datasets can often approach such sizes as m = 100,000,000. In this case, our gradient descent step will have to make a summation over all one hundred million examples. We will want to try to avoid this – the approaches for doing so are described below. Stochastic Gradient DescentStochastic gradient descent is an alternative to classic (or batch) gradient descent and is more efficient and scalable to large data sets.Stochastic gradient descent is written out in a different but similar way:$$cost(\theta,(x^{(i)}, y^{(i)})) = \dfrac{1}{2}(h_{\theta}(x^{(i)}) - y^{(i)})^2$$The only difference in the above cost function is the elimination of the m constant within $\dfrac{1}{2}$.$$J_{train}(\theta) = \dfrac{1}{m} \displaystyle \sum_{i=1}^m cost(\theta, (x^{(i)}, y^{(i)}))$$$J_{train}$ is now just the average of the cost applied to all of our training examples.The algorithm is as follows Randomly ‘shuffle’ the dataset For $i = 1\dots m$, $\Theta_j := \Theta_j - \alpha (h_{\Theta}(x^{(i)}) - y^{(i)}) \cdot x^{(i)}_j$ This algorithm will only try to fit one training example at a time. This way we can make progress in gradient descent without having to scan all m training examples first. Stochastic gradient descent will be unlikely to converge at the global minimum and will instead wander around it randomly, but usually yields a result that is close enough. Stochastic gradient descent will usually take 1-10 passes through your data set to get near the global minimum. Mini-Batch Gradient DescentMini-batch gradient descent can sometimes be even faster than stochastic gradient descent. Instead of using all m examples as in batch gradient descent, and instead of using only 1 example as in stochastic gradient descent, we will use some in-between number of examples b.Typical values for b range from 2-100 or so.For example, with b=10 and m=1000:Repeat:For $i = 1,11,21,31,\dots,991$$$\theta_j := \theta_j - \alpha \dfrac{1}{10} \displaystyle \sum_{k=i}^{i+9} (h_\theta(x^{(k)}) - y^{(k)})x_j^{(k)}$$We’re simply summing over ten examples at a time. The advantage of computing more than one example at a time is that we can use vectorized implementations over the b examples. Stochastic Gradient Descent ConvergenceHow do we choose the learning rate α for stochastic gradient descent? Also, how do we debug stochastic gradient descent to make sure it is getting as close as possible to the global optimum?One strategy is to plot the average cost of the hypothesis applied to every 1000 or so training examples. We can compute and save these costs during the gradient descent iterations.With a smaller learning rate, it is possible that you may get a slightly better solution with stochastic gradient descent. That is because stochastic gradient descent will oscillate and jump around the global minimum, and it will make smaller random jumps with a smaller learning rate.If you increase the number of examples you average over to plot the performance of your algorithm, the plot’s line will become smoother.With a very small number of examples for the average, the line will be too noisy and it will be difficult to find the trend.One strategy for trying to actually converge at the global minimum is to slowly decrease α over time . For example $\alpha = \dfrac{const1}{iterationNumber + const2}$However, this is not often done because people don’t want to have to fiddle with even more parameters. 02_advanced-topics01_online-learningIn this video, I’d like to talk about a new large-scale machine learning setting called the online learning setting. The online learning setting allows us to model problems where we have a continuous flood or a continuous stream of data coming in and we would like an algorithm to learn from that. Today, many of the largest websites, or many of the largest website companies use different versions of online learning algorithms to learn from the flood of users that keep on coming to, back to the website. Specifically, if you have a continuous stream of data generated by a continuous stream of users coming to your website, what you can do is sometimes use an online learning algorithm to learn user preferences from the stream of data and use that to optimize some of the decisions on your website. Suppose you run a shipping service, so, you know, users come and ask you to help ship their package from location A to location B and suppose you run a website, where users repeatedly come and they tell you where they want to send the package from, and where they want to send it to (so the origin and destination) and your website offers to ship the package for some asking price, so I’ll ship your package for $50, I’ll ship it for $20. And based on the price that you offer to the users, the users sometimes chose to use a shipping service; that’s a positive example and sometimes they go away and they do not choose to purchase your shipping service. So let’s say that we want a learning algorithm to help us to optimize what is the asking price that we want to offer to our users. And specifically, let’s say we come up with some sort of features that capture properties of the users. If we know anything about the demographics, they capture, you know, the origin and destination of the package, where they want to ship the package. And what is the price that we offer to them for shipping the package. and what we want to do is learn what is the probability that they will elect to ship the package, using our shipping service given these features, and again just as a reminder these features X also captures the price that we’re asking for. And so if we could estimate the chance that they’ll agree to use our service for any given price, then we can try to pick a price so that they have a pretty high probability of choosing our website while simultaneously hopefully offering us a fair return, offering us a fair profit for shipping their package. So if we can learn this property of y equals 1 given any price and given the other features we could really use this to choose appropriate prices as new users come to us. So in order to model the probability of y equals 1, what we can do is use logistic regression or neural network or some other algorithm like that. But let’s start with logistic regression. Now if you have a website that just runs continuously, here’s what an online learning algorithm would do. I’m gonna write repeat forever. This just means that our website is going to, you know, keep on staying up. What happens on the website is occasionally a user will come and for the user that comes we’ll get some x,y pair corresponding to a customer or to a user on the website. So the features x are, you know, the origin and destination specified by this user and the price that we happened to offer to them this time around, and y is either one or zero depending one whether or not they chose to use our shipping service. Now once we get this {x,y} pair, what an online learning algorithm does is then update the parameters theta using just this example x,y, and in particular we would update my parameters theta as Theta j get updated as Theta j minus the learning rate alpha times my usual gradient descent rule for logistic regression. So we do this for j equals zero up to n, and that’s my close curly brace. So, for other learning algorithms instead of writing X-Y, right, I was writing things like Xi, Yi but in this online learning setting where actually discarding the notion of there being a fixed training set instead we have an algorithm. Now what happens as we get an example and then we learn using that example like so and then we throw that example away. We discard that example and we never use it again and so that’s why we just look at one example at a time. We learn from that example. We discard it. Which is why, you know, we’re also doing away with this notion of there being this sort of fixed training set indexed by i. And, if you really run a major website where you really have a continuous stream of users coming, then this sort of online learning algorithm is actually a pretty reasonable algorithm. Because of data is essentially free if you have so much data, that data is essentially unlimited then there is really may be no need to look at a training example more than once. Of course if we had only a small number of users then rather than using an online learning algorithm like this, you might be better off saving away all your data in a fixed training set and then running some algorithm over that training set. But if you really have a continuous stream of data, then an online learning algorithm can be very effective. I should mention also that one interesting effect of this sort of online learning algorithm is that it can adapt to changing user preferences. And in particular, if over time because of changes in the economy maybe users start to become more price sensitive and willing to pay, you know, less willing to pay high prices. Or if they become less price sensitive and they’re willing to pay higher prices. Or if different things become more important to users, if you start to have new types of users coming to your website. This sort of online learning algorithm can also adapt to changing user preferences and kind of keep track of what your changing population of users may be willing to pay for. And it does that because if your pool of users changes, then these updates to your parameters theta will just slowly adapt your parameters to whatever your latest pool of users looks like. Here’s another example of a sort of application to which you might apply online learning. this is an application in product search in which we want to apply learning algorithm to learn to give good search listings to a user. Let’s say you run an online store that sells phones - that sells mobile phones or sells cell phones. And you have a user interface where a user can come to your website and type in the query like “Android phone 1080p camera”. So 1080p is a type of a specification for a video camera that you might have on a phone, a cell phone, a mobile phone. Suppose, suppose we have a hundred phones in our store. And because of the way our website is laid out, when a user types in a query, if it was a search query, we would like to find a choice of ten different phones to show what to offer to the user. What we’d like to do is have a learning algorithm help us figure out what are the ten phones out of the 100 we should return the user in response to a user-search query like the one here. Here’s how we can go about the problem. For each phone and given a specific user query; we can construct a feature vector X. So the feature vector X might capture different properties of the phone. It might capture things like, how similar the user search query is in the phones. We capture things like how many words in the user search query match the name of the phone, how many words in the user search query match the description of the phone and so on. So the features x capture properties of the phone and it captures things about how similar or how well the phone matches the user query along different dimensions. What we like to do is estimate the probability that a user will click on the link for a specific phone, because we want to show the user phones that they are likely to want to buy, want to show the user phones that they have high probability of clicking on in the web browser. So I’m going to define y equals one if the user clicks on the link for a phone and y equals zero otherwise and what I would like to do is learn the probability the user will click on a specific phone given, you know, the features x, which capture properties of the phone and how well the query matches the phone. To give this problem a name in the language of people that run websites like this, the problem of learning this is actually called the problem of learning the predicted click-through rate, the predicted CTR. It just means learning the probability that the user will click on the specific link that you offer them, so CTR is an abbreviation for click through rate. And if you can estimate the predicted click-through rate for any particular phone, what we can do is use this to show the user the ten phones that are most likely to click on, because out of the hundred phones, we can compute this for each of the 100 phones and just select the 10 phones that the user is most likely to click on, and this will be a pretty reasonable way to decide what ten results to show to the user. Just to be clear, suppose that every time a user does a search, we return ten results what that will do is it will actually give us ten x,y pairs, this actually gives us ten training examples every time a user comes to our website because, because for the ten phone that we chose to show the user, for each of those 10 phones we get a feature vector X, and for each of those 10 phones we show the user we will also get a value for y, we will also observe the value of y, depending on whether or not we clicked on that url or not and so, one way to run a website like this would be to continuously show the user, you know, your ten best guesses for what other phones they might like and so, each time a user comes you would get ten examples, ten x,y pairs, and then use an online learning algorithm to update the parameters using essentially 10 steps of gradient descent on these 10 examples, and then you can throw the data away, and if you really have a continuous stream of users coming to your website, this would be a pretty reasonable way to learn parameters for your algorithm so as to show the ten phones to your users that may be most promising and the most likely to click on. So, this is a product search problem or learning to rank phones, learning to search for phones example. So, I’ll quickly mention a few others. One is, if you have a website and you’re trying to decide, you know, what special offer to show the user, this is very similar to phones, or if you have a website and you show different users different news articles. So, if you’re a news aggregator website, then you can again use a similar system to select, to show to the user, you know, what are the news articles that they are most likely to be interested in and what are the news articles that they are most likely to click on. Closely related to special offers, will we profit from recommendations. And in fact, if you have a collaborative filtering system, you can even imagine a collaborative filtering system giving you additional features to feed into a logistic regression classifier to try to predict the click through rate for different products that you might recommend to a user. Of course, I should say that any of these problems could also have been formulated as a standard machine learning problem, where you have a fixed training set. Maybe, you can run your website for a few days and then save away a training set, a fixed training set, and run a learning algorithm on that. But these are the actual sorts of problems, where you do see large companies get so much data, that there’s really maybe no need to save away a fixed training set, but instead you can use an online learning algorithm to just learn continuously. from the data that users are generating on your website. So, that was the online learning setting and as we saw, the algorithm that we apply to it is really very similar to this schotastic gradient descent algorithm, only instead of scanning through a fixed training set, we’re instead getting one example from a user, learning from that example, then discarding it and moving on**And if you have a continuous stream of data for some application, this sort of algorithm may be well worth considering for your application. And of course,one advantage of online learning is also that if you have a changing pool of users, or if the things you’re trying to predict are slowly changing like your user taste is slowly changing, the online learning algorithm can slowly adapt your learned hypothesis to whatever the latest sets of user behaviors are like as well. 02_map-reduce-and-data-parallelismIn the last few videos, we talked about stochastic gradient descent, and, you know, other variations of the stochastic gradient descent algorithm, including those adaptations to online learning, but all of those algorithms could be run on one machine, or could be run on one computer. And some machine learning problems are just too big to run on one machine, sometimes maybe you just so much data you just don’t ever want to run all that data through a single computer, no matter what algorithm you would use on that computer. So in this video I’d like to talk about different approach to large scale machine learning, called the map reduce approach. And even though we have quite a few videos on stochastic gradient descent and we’re going to spend relative less time on map reduce–don’t judge the relative importance of map reduce versus the gradient descent based on the amount amount of time I spend on these ideas in particular. Many people will say that map reduce is at least an equally important, and some would say an even more important idea compared to gradient descent, only it’s relatively simpler to explain, which is why I’m going to spend less time on it, but using these ideas you might be able to scale learning algorithms to even far larger problems than is possible using stochastic gradient descent. Here’s the idea. Let’s say we want to fit a linear regression model or a logistic regression model or some such, and let’s start again with batch gradient descent, so that’s our batch gradient descent learning rule. And to keep the writing on this slide tractable, I’m going to assume throughout that we have m equals 400 examples. Of course, by our standards, in terms of large scale machine learning, you know m might be pretty small and so, this might be more commonly applied to problems, where you have maybe closer to 400 million examples, or some such, but just to make the writing on the slide simpler, I’m going to pretend we have 400 examples. So in that case, the batch gradient descent learning rule has this 400 and the sum from i equals 1 through 400 through my 400 examples here, and if m is large, then this is a computationally expensive step. So, what the MapReduce idea does is the following, and I should say the map reduce idea is due to two researchers, Jeff Dean and Sanjay Gimawat. Jeff Dean, by the way, is one of the most legendary engineers in all of Silicon Valley and he kind of built a large fraction of the architectural infrastructure that all of Google runs on today. But here’s the map reduce idea. So, let’s say I have some training set, if we want to denote by this box here of X Y pairs, where it’s X1, Y1, down to my 400 examples, Xm, Ym. So, that’s my training set with 400 training examples. In the MapReduce idea, one way to do, is split this training set in to different subsets. I’m going to. assume for this example that I have 4 computers, or 4 machines to run in parallel on my training set, which is why I’m splitting this into 4 machines. If you have 10 machines or 100 machines, then you would split your training set into 10 pieces or 100 pieces or what have you. And what the first of my 4 machines is to do, say, is use just the first one quarter of my training set–so use just the first 100 training examples. And in particular, what it’s going to do is look at this summation, and compute that summation for just the first 100 training examples. So let me write that up I’m going to compute a variable temp 1 to superscript 1 the first machine J equals sum from equals 1 through 100, and then I’m going to plug in exactly that term there–so I have X-theta, Xi, minus Yi times Xij, right? So that’s just that gradient descent term up there. And then similarly, I’m going to take the second quarter of my data and send it to my second machine, and my second machine will use training examples 101 through 200 and you will compute similar variables of a temp to j which is the same sum for index from examples 101 through 200. And similarly machines 3 and 4 will use the third quarter and the fourth quarter of my training set. So now each machine has to sum over 100 instead of over 400 examples and so has to do only a quarter of the work and thus presumably it could do it about four times as fast. Finally, after all these machines have done this work, I am going to take these temp variables and put them back together. So I take these variables and send them all to a You know centralized master server and what the master will do is combine these results together. and in particular, it will update my parameters theta j according to theta j gets updated as theta j minus Of the learning rate alpha times one over 400 times temp, 1, J, plus temp 2j plus temp 3j plus temp 4j and of course we have to do this separately for J equals 0. You know, up to and within this number of features. So operating this equation into I hope it’s clear. So what this equation is doing is exactly the same is that when you have a centralized master server that takes the results, the ten one j the ten two j ten three j and ten four j and adds them up and so of course the sum of these four things. Right, that’s just the sum of this, plus the sum of this, plus the sum of this, plus the sum of that, and those four things just add up to be equal to this sum that we’re originally computing a batch stream descent. And then we have the alpha times 1 of 400, alpha times 1 of 100, and this is exactly equivalent to the batch gradient descent algorithm, only, instead of needing to sum over all four hundred training examples on just one machine, we can instead divide up the work load on four machines. So, here’s what the general picture of the MapReduce technique looks like. We have some training sets, and if we want to paralyze across four machines, we are going to take the training set and split it, you know, equally. Split it as evenly as we can into four subsets. Then we are going to take the 4 subsets of the training data and send them to 4 different computers. And each of the 4 computers can compute a summation over just one quarter of the training set, and then finally take each of the computers takes the results, sends them to a centralized server, which then combines the results together. So, on the previous line in that example, the bulk of the work in gradient descent, was computing the sum from i equals 1 to 400 of something. So more generally, sum from i equals 1 to m of that formula for gradient descent. And now, because each of the four computers can do just a quarter of the work, potentially you can get up to a 4x speed up. In particular, if there were no network latencies and no costs of the network communications to send the data back and forth, you can potentially get up to a 4x speed up. Of course, in practice, because of network latencies, the overhead of combining the results afterwards and other factors, in practice you get slightly less than a 4x speedup. But, none the less, this sort of macro juice approach does offer us a way to process much larger data sets than is possible using a single computer. If you are thinking of applying Map Reduce to some learning algorithm, in order to speed this up. By paralleling the computation over different computers, the key question to ask yourself is, can your learning algorithm be expressed as a summation over the training set? And it turns out that many learning algorithms can actually be expressed as computing sums of functions over the training set and the computational expense of running them on large data sets is because they need to sum over a very large training set. So, whenever your learning algorithm can be expressed as a sum of the training set and whenever the bulk of the work of the learning algorithm can be expressed as the sum of the training set, then map reviews might a good candidate for scaling your learning algorithms through very, very good data sets. Lets just look at one more example. Let’s say that we want to use one of the advanced optimization algorithm. So, things like, you know, L-BFGS constant gradient and so on, and let’s say we want to train a logistic regression of the algorithm. For that, we need to compute two main quantities. One is for the advanced optimization algorithms like, you know, LPF and constant gradient. We need to provide it a routine to compute the cost function of the optimization objective. And so for logistic regression, you remember that a cost function has this sort of sum over the training set, and so if youre paralizing over ten machines, you would split up the training set onto ten machines and have each of the ten machines compute the sum of this quantity over just one tenth of the training data. Then, the other thing that the advanced optimization algorithms need, is a routine to compute these partial derivative terms. Once again, these derivative terms, for which it’s a logistic regression, can be expressed as a sum over the training set, and so once again, similar to our earlier example, you would have each machine compute that summation over just some small fraction of your training data. And finally, having computed all of these things, they could then send their results to a centralized server, which can then add up the partial sums. This corresponds to adding up those tenth i or tenth ij variables, which were computed locally on machine number i, and so the centralized server can sum these things up and get the overall cost function and get the overall partial derivative, which you can then pass through the advanced optimization algorithm. So, more broadly, by taking other learning algorithms and expressing them in sort of summation form or by expressing them in terms of computing sums of functions over the training set, you can use the MapReduce technique to parallelize other learning algorithms as well, and scale them to very large training sets. Finally, as one last comment, so far we have been discussing MapReduce algorithms as allowing you to parallelize over multiple computers, maybe multiple computers in a computer cluster or over multiple computers in the data center. _It turns out that sometimes even if you have just a single computer, MapReduce can also be applicable._In particular, on many single computers now, you can have multiple processing cores. You can have multiple CPUs, and within each CPU you can have multiple proc cores. If you have a large training set, what you can do if, say, you have a computer with 4 computing cores, what you can do is, even on a single computer you can split the training sets into pieces and send the training set to different cores within a single box, like within a single desktop computer or a single server and use MapReduce this way to divvy up work load. Each of the cores can then carry out the sum over, say, one quarter of your training set, and then they can take the partial sums and combine them, in order to get the summation over the entire training set. The advantage of thinking about MapReduce this way, as paralyzing over cause within a single machine, rather than parallelizing over multiple machines is that, this way you don’t have to worry about network latency, because all the communication, all the sending of the [xx] back and forth, all that happens within a single machine. And so network latency becomes much less of an issue compared to if you were using this to over different computers within the data sensor. Finally, one last caveat on parallelizing within a multi-core machine. Depending on the details of your implementation, if you have a multi-core machine and if you have certain numerical linear algebra libraries. It turns out that the sum numerical linear algebra libraries that can automatically parallelize their linear algebra operations across multiple cores within the machine. So if you’re fortunate enough to be using one of those numerical linear algebra libraries and certainly this does not apply to every single library. If you’re using one of those libraries and. If you have a very good vectorizing implementation of the learning algorithm. Sometimes you can just implement you standard learning algorithm in a vectorized fashion and not worry about parallelization and numerical linear algebra libararies could take care of some of it for you. So you don’t need to implement [xx] but. for other any problems, taking advantage of this sort of map reducing commentation, finding and using this MapReduce formulation and to paralelize a cross coarse except yourself might be a good idea as well and could let you speed up your learning algorithm. In this video, we talked about the MapReduce approach to parallelizing machine learning by taking a data and spreading them across many computers in the data center. Although these ideas are critical to paralysing across multiple cores within a single computer as well. Today there are some good open source implementations of MapReduce, so there are many users in open source system called Hadoop and using either your own implementation or using someone else’s open source implementation, you can use these ideas to parallelize learning algorithms and get them to run on much larger data sets than is possible using just a single machine. summaryOnline LearninWith a continuous stream of users to a website, we can run an endless loop that gets (x,y), where we collect some user actions for the features in x to predict some behavior y.You can update θ for each individual (x,y) pair as you collect them. This way, you can adapt to new pools of users, since you are continuously updating theta. Map Reduce and Data ParallelismWe can divide up batch gradient descent and dispatch the cost function for a subset of the data to many different machines so that we can train our algorithm in parallel.You can split your training set into z subsets corresponding to the number of machines you have. On each of those machines calculate $\displaystyle \sum_{i=p}^{q}(h_{\theta}(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)}$, where we’ve split the data starting at p and ending at q.MapReduce will take all these dispatched (or ‘mapped’) jobs and ‘reduce’ them by calculating:$$\Theta_j := \Theta_j - \alpha \dfrac{1}{z}(temp_j^{(1)} + temp_j^{(2)} + \cdots + temp_j^{(z)})$$For all $j = 0, \dots, n$.This is simply taking the computed cost from all the machines, calculating their average, multiplying by the learning rate, and updating theta.Your learning algorithm is MapReduceable if it can be expressed as computing sums of functions over the training set . Linear regression and logistic regression are easily parallelizable.For neural networks, you can compute forward propagation and back propagation on subsets of your data on many machines. Those machines can report their derivatives back to a ‘master’ server that will combine them.]]></content>
      <categories>
        <category>英文</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[16_recommender-systems note16]]></title>
    <url>%2F2018%2F01%2F16%2F16_recommender-systems%2F</url>
    <content type="text"><![CDATA[01_predicting-movie-ratings01_predicting-movie-ratingsIn this next set of videos, I would like to tell you about recommender systems. There are two reasons, I had two motivations for why I wanted to talk about recommender systems. The first is just that it is an important application of machine learning. Over the last few years, occasionally I visit different, you know, technology companies here in Silicon Valley and I often talk to people working on machine learning applications there and so I’ve asked people what are the most important applications of machine learning or what are the machine learning applications that you would most like to get an improvement in the performance of. And one of the most frequent answers I heard was that there are many groups out in Silicon Valley now, trying to build better recommender systems. So, if you think about what the websites are like Amazon, or what Netflix or what eBay, or what iTunes Genius, made by Apple does, there are many websites or systems that try to recommend new products to use. So, Amazon recommends new books to you, Netflix try to recommend new movies to you, and so on. And these sorts of recommender systems, that look at what books you may have purchased in the past, or what movies you have rated in the past, but these are the systems that are responsible for today, a substantial fraction of Amazon’s revenue and for a company like Netflix, the recommendations that they make to the users is also responsible for a substantial fraction of the movies watched by their users. And so an improvement in performance of a recommender system can have a substantial and immediate impact on the bottom line of many of these companies. Recommender systems is kind of a funny problem, within academic machine learning so that we could go to an academic machine learning conference, the problem of recommender systems, actually receives relatively little attention, or at least it’s sort of a smaller fraction of what goes on within Academia. But if you look at what’s happening, many technology companies, the ability to build these systems seems to be a high priority for many companies. And that’s one of the reasons why I want to talk about them in this class. The second reason that I want to talk about recommender systems is that as we approach the last few sets of videos of this class I wanted to talk about a few of the big ideas in machine learning and share with you, you know, some of the big ideas in machine learning. And we’ve already seen in this class that features are important for machine learning, the features you choose will have a big effect on the performance of your learning algorithm. So there’s this big idea in machine learning, which is that for some problems, maybe not all problems, but some problems, there are algorithms that can try to automatically learn a good set of features for you. So rather than trying to hand design, or hand code the features, which is mostly what we’ve been doing so far, there are a few settings where you might be able to have an algorithm, just to learn what feature to use, and the recommender systems is just one example of that sort of setting. There are many others, but engraved through recommender systems, will be able to go a little bit into this idea of learning the features and you’ll be able to see at least one example of this, I think, big idea in machine learning as well. So, without further ado, let’s get started, and talk about the recommender system problem formulation. As my running example, I’m going to use the modern problem of predicting movie ratings. So, here’s a problem. Imagine that you’re a website or a company that sells or rents out movies, or what have you. And so, you know, Amazon, and Netflix, and I think iTunes are all examples of companies that do this, and let’s say you let your users rate different movies, using a 1 to 5 star rating. So, users may, you know, something one, two, three, four or five stars. In order to make this example just a little bit nicer, I’m going to allow 0 to 5 stars as well, because that just makes some of the math come out just nicer. Although most of these websites use the 1 to 5 star scale. So here, I have 5 movies. You know, Love That Lasts, Romance Forever, Cute Puppies of Love, Nonstop Car Chases, and Swords vs. Karate. And we have 4 users, which, calling, you know, Alice, Bob, Carol, and Dave, with initials A, B, C, and D, we’ll call them users 1, 2, 3, and 4. So, let’s say Alice really likes Love That Lasts and rates that 5 stars, likes Romance Forever, rates it 5 stars. She did not watch Cute Puppies of Love, and did rate it, so we don’t have a rating for that, and Alice really did not like Nonstop Car Chases or Swords vs. Karate. And a different user Bob, user two, maybe rated a different set of movies, maybe she likes to Love at Last, did not to watch Romance Forever, just have a rating of 4, a 0, a 0, and maybe our 3rd user, rates this 0, did not watch that one, 0, 5, 5, and, you know, let’s just fill in some of the numbers. And so just to introduce a bit of notation, this notation that we’ll be using throughout, I’m going to use NU to denote the number of users. So in this example, NU will be equal to 4. So the u-subscript stands for users and Nm, going to use to denote the number of movies, so here I have five movies so Nm equals equals 5. And you know for this example, I have for this example, I have loosely 3 maybe romantic or romantic comedy movies and 2 action movies and you know, if you look at this small example, it looks like Alice and Bob are giving high ratings to these romantic comedies or movies about love, and giving very low ratings about the action movies, and for Carol and Dave, it’s the opposite, right? Carol and Dave, users three and four, really like the action movies and give them high ratings, but don’t like the romance and love- type movies as much. Specifically, in the recommender system problem, we are given the following data. Our data comprises the following: we have these values r(i, j), and r(i, j) is 1 if user J has rated movie I. So our users rate only some of the movies, and so, you know, we don’t have ratings for those movies. And whenever r(i, j) is equal to 1, whenever user j has rated movie i, we also get this number y(i, j), which is the rating given by user j to movie i. And so, y(i, j) would be a number from zero to five, depending on the star rating, zero to five stars that user gave that particular movie. So, the recommender system problem is given this data that has give these r(i, j)’s and the y(i, j)’s to look through the data and look at all the movie ratings that are missing and to try to predict what these values of the question marks should be. In the particular example, I have a very small number of movies and a very small number of users and so most users have rated most movies but in the realistic settings your users each of your users may have rated only a minuscule fraction of your movies but looking at this data, you know, if Alice and Bob both like the romantic movies maybe we think that Alice would have given this a five. Maybe we think Bob would have given this a 4.5 or some high value, as we think maybe Carol and Dave were doing these very low ratings. And Dave, well, if Dave really likes action movies, maybe he would have given Swords and Karate a 4 rating or maybe a 5 rating, okay? And so, our job in developing a recommender system is to come up with a learning algorithm that can automatically go fill in these missing values for us so that we can look at, say, the movies that the user has not yet watched, and recommend new movies to that user to watch. You try to predict what else might be interesting to a user. So that’s the formalism of the recommender system problem. In the next video we’ll start to develop a learning algorithm to address this problem. summaryProblem Formulation Recommendation is currently a very popular application of machine learning.Say we are trying to recommend movies to customers. We can use the following definitions $n_u =$ number of users $n_m =$ number of movies $r(i,j) = 1$ if user j has rated movie i $y(i,j) =$ rating given by user j to movie i (defined only if r(i,j)=1) 02_content-based-recommendationsIn the last video, we talked about the recommender systems problem where for example you might have a set of movies and you may have a set of users, each who have rated some subset of the movies. They’ve rated the movies one to five stars or zero to five stars. And what we would like to do is look at these users and predict how they would have rated other movies that they have not yet rated. In this video I’d like to talk about our first approach to building a recommender system. This approach is called content based recommendations. Here’s our data set from before and just to remind you of a bit of notation, I was using nu to denote the number of users and so that’s equal to 4, and nm to denote the number of movies, I have 5 movies. So, how do I predict what these missing values would be? Let’s suppose that for each of these movies I have a set of features for them. In particular, let’s say that for each of the movies have two features which I’m going to denote x1 and x2. Where x1 measures the degree to which a movie is a romantic movie and x2 measures the degree to which a movie is an action movie. So, if you take a movie, Love at last, you know it’s 0.9 rating on the romance scale. This is a highly romantic movie, but zero on the action scale. So, almost no action in that movie. Romance forever is a 1.0, lot of romance and 0.01 action. I don’t know, maybe there’s a minor car crash in that movie or something. So there’s a little bit of action. Skipping one, let’s do Swords vs karate, maybe that has a 0 romance rating and no romance at all in that but plenty of action. And Nonstop car chases, maybe again there’s a tiny bit of romance in that movie but mainly action. And Cute puppies of love mainly a romance movie with no action at all. So if we have features like these, then each movie can be represented with a feature vector. Let’s take movie one. So let’s call these movies 1, 2, 3, 4, and 5. But my first movie, Love at last, I have my two features, 0.9 and 0. And so these are features x1 and x2. And let’s add an extra feature as usual, which is my interceptor feature x0 = 1. And so putting these together I would then have a feature x1. The superscript 1 denotes it’s the feature vector for my first movie, and this feature vector is equal to 1. The first 1 there is this interceptor. And then my two feature is 0.90 like so. So for Love at last I would have a feature vector x1, for the movie Romance forever I may have a software feature of vector x2, and so on, and for Swords vs karate I would have a different feature vector x superscript 5. Also, consistence with our earlier node notation that we were using, we’re going to set n to be the number of features not counting this x0 interceptor. So n is equal to 2 because it’s we have two features x1 and x2 capturing the degree of romance and the degree of action in each movie. Now in order to make predictions here’s one thing that we do which is that we could treat predicting the ratings of each user as a separate linear regression problem. So specifically, let’s say that for each user j, we’re going to learn the parameter vector theta j, which would be an R3 in this case. More generally, theta (j) would be an R (n+1), where n is the number of features not counting the set term. And we’re going to predict user j as rating movie i with just the inner product between parameters vectors theta and the features xi. So let’s take a specific example. Let’s take user 1, so that would be Alice. And associated with Alice would be some parameter vector theta 1. And our second user, Bob, will be associated a different parameter vector theta 2. Carol will be associated with a different parameter vector theta 3 and Dave a different parameter vector theta 4. So let’s say you want to make a prediction for what Alice will think of the movie Cute puppies of love. Well that movie is going to have some parameter vector x3 where we have that x3 is going to be equal to 1, which is my intercept term and then 0.99 and then 0. And let’s say, for this example, let’s say that we’ve somehow already gotten a parameter vector theta 1 for Alice. We’ll say it later exactly how we come up with this parameter vector. But let’s just say for now that some unspecified learning algorithm has learned the parameter vector theta 1 and is equal to this 0,5,0. So our prediction for this entry is going to be equal to theta 1, that is Alice’s parameter vector, transpose x3, that is the feature vector for the Cute puppies of love movie, number 3. And so the inner product between these two vectors is gonna be 5 times 0.99, which is equal to 4.95. And so my prediction for this value over here is going to be 4.95. And maybe that seems like a reasonable value if indeed this is my parameter vector theta 1. So, all we’re doing here is we’re applying a different copy of this linear regression for each user, and we’re saying that what Alice does is Alice has some parameter vector theta 1 that she uses, that we use to predict her ratings as a function of how romantic and how action packed a movie is. And Bob and Carol and Dave, each of them have a different linear function of the romanticness and actionness, or degree of romance and degree of action in a movie and that that’s how we’re gonna predict that their star ratings. More formally, here’s how we can write down the problem. Our notation is that r(i,j) is equal to 1 if user j has rated movie i and y(i,j) is the rating of that movie, if that rating exists. That is, if that user has actually rated that movie. And, on the previous slide we also defined these, theta j, which is a parameter for the user xi, which is a feature vector for a specific movie. And for each user and each movie, we predict that rating as follows. So let me introduce just temporarily introduce one extra bit of notation mj. We’re gonna use mj to denote the number of users rated by movie j. We don’t need this notation only for this line. Now in order to learn the parameter vector for theta j, well how do we do so. This is basically a linear regression problem. So what we can do is just choose a parameter vector theta j so that the predicted values here are as close as possible to the values that we observed in our training sets and the values we observed in our data. So let’s write that down. In order to learn the parameter vector theta j, let’s minimize over the parameter vector theta j of sum, and I want to sum over all movies that user j has rated. So we write it as sum over all values of i. That’s a :r(i,j) equals 1. So the way to read this summation syntax is this is summation over all the values of i, so the r(i.j) is equal to 1. So you’ll be summing over all the movies that user j has rated. And then I’m going to compute theta j, transpose x i. So that’s the prediction of using j’s rating on movie i,- y (i,j). So that’s the actual observed rating squared. And then, let me just divide by the number of movies that user j has actually rated. So let’s just divide by 1 over 2m j. And so this is just like the least squares regressions. It’s just like linear regression, where we want to choose the parameter vector theta j to minimize this type of squared error term. And if you want, you can also add in irregularization terms so plus lambda over 2m and this is really 2mj because we have mj examples. User j has rated that many movies, it’s not like we have that many data points with which to fit the parameters of theta j. And then let me add in my usual regularization term here of theta j k squared. As usual, this sum is from k equals 1 through n, so here, theta j is going to be an n plus 1 dimensional vector, where in our early example n was equal to 2. But more broadly, more generally n is the number of features we have per movie. And so as usual we don’t regularize over theta 0. We don’t regularize over the bias terms. The sum is from k equals 1 through n. So if you minimize this as a function of theta j you get a good solution, you get a pretty good estimate of a parameter vector theta j with which to make predictions for user j’s movie ratings. For recommender systems, I’m gonna change this notation a little bit. So to simplify the subsequent math, I with to get rid of this term mj. So that’s just a constant, right? So I can delete it without changing the value of theta j that I get out of this optimization. So if you imagine taking this whole equation, taking this whole expression and multiplying it by mj, get rid of that constant. And when I minimize this, I should still get the same value of theta j as before. So just to repeat what we wrote on the previous slide, here’s our optimization objective. In order to learn theta j which is the parameter for user j, we’re going to minimize over theta j of this optimization objectives. So this is our usual squared error term and then this is our regularizations term. Now of course in building a recommender system, we don’t just want to learn parameters for a single user. We want to learn parameters for all of our users. I have n subscript u users, so I want to learn all of these parameters. And so, what I’m going to do is take this optimization objective and just add the mixture summation there. So this expression here with the one half on top of this is exactly the same as what we had on top. Except that now instead of just doing this for a specific user theta j, I’m going to sum my objective over all of my users and then minimize this overall optimization objective, minimize this overall cost on. And when I minimize this as a function of theta 1, theta 2, up to theta nu, I will get a separate parameter vector for each user. And I can then use that to make predictions for all of my users, for all of my n subscript users. So putting everything together, this was our optimization objective on top. And to give this thing a name, I’ll just call this J(theta1, …, theta nu). So j as usual is my optimization objective, which I’m trying to minimize. Next, in order to actually do the minimization, if you were to derive the gradient descent update, these are the equations that you would get. So you take theta j, k, and subtract from an alpha, which is the learning rate, times these terms over here on the right. So there’s slightly different cases when k equals 0 and when k does not equal 0. Because our regularization term here regularizes only the values of theta jk for k not equal to 0, so we don’t regularize theta 0, so with slightly different updates when k equals 0 and k is not equal to 0. And this term over here, for example, is just the partial derivative with respect to your parameter, that of your optimization objective. Right and so this is just gradient descent and I’ve already computed the derivatives and plugged them into here. And if this gradient descent update look a lot like what we have here for linear regression. That’s because these are essentially the same as linear regression. The only minor difference is that for linear regression we have these 1 over m terms, this really would’ve been 1 over mj. But because earlier when we are deriving the optimization objective, we got rid of this, that’s why we don’t have this 1 over m term. But otherwise, it’s really some of my training examples of the ever times xk plus that regularization term, plus that term of regularization contributes to the derivative. And so if you’re using gradient descent here’s how you can minimize the cost function j to learn all the parameters. And using these formulas for the derivative if you want, you can also plug them into a more advanced optimization algorithm, like conjugate gradient or LBFGS or what have you. And use that to try to minimize the cost function j as well. So hopefully you now know how you can apply essentially a deviation on linear regression in order to predict different movie ratings by different users. This particular algorithm is called a content based recommendations, or a content based approach, because we assume that we have available to us features for the different movies. And so where features that capture what is the content of these movies, of how romantic is this movie, how much action is in this movie. And we’re really using features of a content of the movies to make our predictions. But for many movies, we don’t actually have such features. Or maybe very difficult to get such features for all of our movies, for all of whatever items we’re trying to sell. And so, in the next video, we’ll start to talk about an approach to recommender systems that isn’t content based and does not assume that we have someone else giving us all of these features for all of the movies in our data set. summaryContent Based RecommendationsWe can introduce two features, $x_1$ and $x_2$ which represents how much romance or how much action a movie may have (on a scale of 0−1).One approach is that we could do linear regression for every single user. For each user j, learn a parameter $\theta^{(j)} \in \mathbb{R}^3$. Predict user j as rating movie i with $(\theta^{(j)})^Tx^{(i)}$ stars.$\theta^{(j)} =$ parameter vector for user j$x^{(i)} =$ feature vector for movie iFor user j, movie i, predicted rating: $(\theta^{(j)})^T(x^{(i)})$$m^{(j)} =$ number of movies rated by user jTo learn $\theta^{(j)}$, we do the following$$min_{\theta^{(j)}} = \dfrac{1}{2}\displaystyle \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{k=1}^n(\theta_k^{(j)})^2$$This is our familiar linear regression. The base of the first summation is choosing all i such that $r(i,j) = 1$.To get the parameters for all our users, we do the following:$$min_{\theta^{(1)},\dots,\theta^{(n_u)}} = \dfrac{1}{2}\displaystyle \sum_{j=1}^{n_u} \sum_{i:r(i,j)=1} ((\theta^{(j)})^T(x^{(i)}) - y^{(i,j)})^2 + \dfrac{\lambda}{2} \sum_{j=1}^{n_u} \sum_{k=1}^n(\theta_k^{(j)})^2$$We can apply our linear regression gradient descent update using the above cost function.The only real difference is that we eliminate the constant $\dfrac{1}{m}$. 02_collaborative-filtering01_collaborative-filteringIn this video we’ll talk about an approach to building a recommender system that’s called collaborative filtering. The algorithm that we’re talking about has a very interesting property that it does what is called feature learning and by that I mean that this will be an algorithm that can start to learn for itself what features to use. Here was the data set that we had and we had assumed that for each movie, someone had come and told us how romantic that movie was and how much action there was in that movie. But as you can imagine it can be very difficult and time consuming and expensive to actually try to get someone to, you know, watch each movie and tell you how romantic each movie and how action packed is each movie, and often you’ll want even more features than just these two. So where do you get these features from? So let’s change the problem a bit and suppose that we have a data set where we do not know the values of these features. So we’re given the data set of movies and of how the users rated them, but we have no idea how romantic each movie is and we have no idea how action packed each movie is so I’ve replaced all of these things with question marks. But now let’s make a slightly different assumption. Let’s say we’ve gone to each of our users, and each of our users has told has told us how much they like the romantic movies and how much they like action packed movies. So Alice has associated a current of theta 1. Bob theta 2. Carol theta 3. Dave theta 4. And let’s say we also use this and that Alice tells us that she really likes romantic movies and so there’s a five there which is the multiplier associated with X1 and lets say that Alice tells us she really doesn’t like action movies and so there’s a 0 there. And Bob tells us something similar so we have theta 2 over here. Whereas Carol tells us that she really likes action movies which is why there’s a 5 there, that’s the multiplier associated with X2, and remember there’s also X0 equals 1 and let’s say that Carol tells us she doesn’t like romantic movies and so on, similarly for Dave. So let’s assume that somehow we can go to users and each user J just tells us what is the value of theta J for them. And so basically specifies to us of how much they like different types of movies. If we can get these parameters theta from our users then it turns out that it becomes possible to try to infer what are the values of x1 and x2 for each movie. Let’s look at an example. Let’s look at movie 1. So that movie 1 has associated with it a feature vector x1. And you know this movie is called Love at last but let’s ignore that. Let’s pretend we don’t know what this movie is about, so let’s ignore the title of this movie. All we know is that Alice loved this move. Bob loved this movie. Carol and Dave hated this movie. So what can we infer? Well, we know from the feature vectors that Alice and Bob love romantic movies because they told us that there’s a 5 here. Whereas Carol and Dave, we know that they hate romantic movies and that they love action movies. So because those are the parameter vectors that you know, uses 3 and 4, Carol and Dave, gave us. And so based on the fact that movie 1 is loved by Alice and Bob and hated by Carol and Dave, we might reasonably conclude that this is probably a romantic movie, it is probably not much of an action movie. this example is a little bit mathematically simplified but what we’re really asking is what feature vector should X1 be so that theta 1 transpose x1 is approximately equal to 5, that’s Alice’s rating, and theta 2 transpose x1 is also approximately equal to 5, and theta 3 transpose x1 is approximately equal to 0, so this would be Carol’s rating, and theta 4 transpose X1 is approximately equal to 0. And from this it looks like, you know, X1 equals one that’s the intercept term, and then 1.0, 0.0, that makes sense given what we know of Alice, Bob, Carol, and Dave’s preferences for movies and the way they rated this movie. And so more generally, we can go down this list and try to figure out what might be reasonable features for these other movies as well. Let’s formalize this problem of learning the features XI. Let’s say that our users have given us their preferences. So let’s say that our users have come and, you know, told us these values for theta 1 through theta of NU and we want to learn the feature vector XI for movie number I. What we can do is therefore pose the following optimization problem. So we want to sum over all the indices J for which we have a rating for movie I because we’re trying to learn the features for movie I that is this feature vector XI. So and then what we want to do is minimize this squared error, so we want to choose features XI, so that, you know, the predictive value of how user J rates movie I will be similar, will be not too far in the squared error sense of the actual value YIJ that we actually observe in the rating of user j on movie I. So, just to summarize what this term does is it tries to choose features XI so that for all the users J that have rated that movie, the algorithm also predicts a value for how that user would have rated that movie that is not too far, in the squared error sense, from the actual value that the user had rated that movie. So that’s the squared error term. As usual, we can also add this sort of regularization term to prevent the features from becoming too big. So this is how we would learn the features for one specific movie but what we want to do is learn all the features for all the movies and so what I’m going to do is add this extra summation here so I’m going to sum over all Nm movies, N subscript m movies, and minimize this objective on top that sums of all movies. And if you do that, you end up with the following optimization problem. And if you minimize this, you have hopefully a reasonable set of features for all of your movies. So putting everything together, what we, the algorithm we talked about in the previous video and the algorithm that we just talked about in this video. In the previous video, what we showed was that you know, if you have a set of movie ratings, so if you have the data the rij’s and then you have the yij’s that will be the movie ratings. Then given features for your different movies we can learn these parameters theta. So if you knew the features, you can learn the parameters theta for your different users. And what we showed earlier in this video is that if your users are willing to give you parameters, then you can estimate features for the different movies. So this is kind of a chicken and egg problem. Which comes first? You know, do we want if we can get the thetas, we can know the Xs. If we have the Xs, we can learn the thetas. And what you can do is, and then this actually works, what you can do is in fact randomly guess some value of the thetas. Now based on your initial random guess for the thetas, you can then go ahead and use the procedure that we just talked about in order to learn features for your different movies. Now given some initial set of features for your movies you can then use this first method that we talked about in the previous video to try to get an even better estimate for your parameters theta. Now that you have a better setting of the parameters theta for your users, we can use that to maybe even get a better set of features and so on. We can sort of keep iterating, going back and forth and optimizing theta, x theta, x theta, nd this actually works and if you do this, this will actually cause your album to converge to a reasonable set of features for you movies and a reasonable set of parameters for your different users. So this is a basic collaborative filtering algorithm. This isn’t actually the final algorithm that we’re going to use. In the next video we are going to be able to improve on this algorithm and make it quite a bit more computationally efficient. But, hopefully this gives you a sense of how you can formulate a problem where you can simultaneously learn the parameters and simultaneously learn the features from the different movies. And for this problem, for the recommender system problem, this is possible only because each user rates multiple movies and hopefully each movie is rated by multiple users. And so you can do this back and forth process to estimate theta and x. So to summarize, in this video we’ve seen an initial collaborative filtering algorithm. The term collaborative filtering refers to the observation that when you run this algorithm with a large set of users, what all of these users are effectively doing are sort of collaboratively–or collaborating to get better movie ratings for everyone because with every user rating some subset with the movies, every user is helping the algorithm a little bit to learn better features, and then by helping– by rating a few movies myself, I will be helping the system learn better features and then these features can be used by the system to make better movie predictions for everyone else. And so there is a sense of collaboration where every user is helping the system learn better features for the common good. This is this collaborative filtering. And, in the next video what we going to do is take the ideas that have worked out, and try to develop a better an even better algorithm, a slightly better technique for collaborative filtering. summaryIt can be very difficult to find features such as “amount of romance” or “amount of action” in a movie. To figure this out, we can use feature finders .We can let the users tell us how much they like the different genres, providing their parameter vector immediately for us.To infer the features from given parameters, we use the squared error function with regularization over all the users:$$min_{x^{(1)},\dots,x^{(n_m)}} \dfrac{1}{2} \displaystyle \sum_{i=1}^{n_m} \sum_{j:r(i,j)=1} ((\theta^{(j)})^T x^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2$$You can also randomly guess the values for theta to guess the features repeatedly. You will actually converge to a good set of features. 02_collaborative-filtering-algorithmIn the last couple videos, we talked about the ideas of how, first, if you’re given features for movies, you can use that to learn parameters data for users. And second, if you’re given parameters for the users, you can use that to learn features for the movies. In this video we’re going to take those ideas and put them together to come up with a collaborative filtering algorithm. So one of the things we worked out earlier is that if you have features for the movies then you can solve this minimization problem to find the parameters theta for your users. And then we also worked that out, if you are given the parameters theta, you can also use that to estimate the features x, and you can do that by solving this minimization problem. So one thing you could do is actually go back and forth. Maybe randomly initialize the parameters and then solve for theta, solve for x, solve for theta, solve for x. But, it turns out that there is a more efficient algorithm that doesn’t need to go back and forth between the x’s and the thetas, but that can solve for theta and x simultaneously. And here it is. What we are going to do, is basically take both of these optimization objectives, and put them into the same objective. So I’m going to define the new optimization objective j, which is a cost function, that is a function of my features x and a function of my parameters theta. And, it’s basically the two optimization objectives I had on top, but I put together. So, in order to explain this, first, I want to point out that this term over here, this squared error term, is the same as this squared error term and the summations look a little bit different, but let’s see what the summations are really doing. The first summation is sum over all users J and then sum over all movies rated by that user. So, this is really summing over all pairs IJ, that correspond to a movie that was rated by a user. Sum over J says, for every user, the sum of all the movies rated by that user. This summation down here, just does things in the opposite order. This says for every movie I, sum over all the users J that have rated that movie and so, you know these summations, both of these are just summations over all pairs ij for which r of i J is equal to 1. It’s just something over all the user movie pairs for which you have a rating. and so those two terms up there is just exactly this first term, and I’ve just written the summation here explicitly, where I’m just saying the sum of all pairs IJ, such that RIJ is equal to 1. So what we’re going to do is define a combined optimization objective that we want to minimize in order to solve simultaneously for x and theta. And then the other terms in the optimization objective are this, which is a regularization in terms of theta. So that came down here and the final piece is this term which is my optimization objective for the x’s and that became this. And this optimization objective j actually has an interesting property that if you were to hold the x’s constant and just minimize with respect to the thetas then you’d be solving exactly this problem, whereas if you were to do the opposite, if you were to hold the thetas constant, and minimize j only with respect to the x’s, then it becomes equivalent to this. Because either this term or this term is constant if you’re minimizing only the respective x’s or only respective thetas. So here’s an optimization objective that puts together my cost functions in terms of x and in terms of theta. And in order to come up with just one optimization problem, what we’re going to do, is treat this cost function, as a function of my features x and of my user pro user parameters data and just minimize this whole thing, as a function of both the Xs and a function of the thetas. And really the only difference between this and the older algorithm is that, instead of going back and forth, previously we talked about minimizing with respect to theta then minimizing with respect to x, whereas minimizing with respect to theta, minimizing with respect to x and so on. In this new version instead of sequentially going between the 2 sets of parameters x and theta, what we are going to do is just minimize with respect to both sets of parameters simultaneously. Finally one last detail is that when we’re learning the features this way. Previously we have been using this convention that we have a feature x0 equals one that corresponds to an interceptor. When we are using this sort of formalism where we’re are actually learning the features, we are actually going to do away with this convention. And so the features we are going to learn x, will be in Rn. Whereas previously we had features x and Rn + 1 including the intercept term. By getting rid of x0 we now just have x in Rn. And so similarly, because the parameters theta is in the same dimension, we now also have theta in RN because if there’s no x0, then there’s no need parameter theta 0 as well. And the reason we do away with this convention is because we’re now learning all the features, right? So there is no need to hard code the feature that is always equal to one. Because if the algorithm really wants a feature that is always equal to 1, it can choose to learn one for itself. So if the algorithm chooses, it can set the feature X1 equals 1. So there’s no need to hard code the feature of 001, the algorithm now has the flexibility to just learn it by itself. $$J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2$$ So, putting everything together, here is our collaborative filtering algorithm. First we are going to initialize x and theta to small random values. And this is a little bit like neural network training, where there we were also initializing all the parameters of a neural network to small random values. Next we’re then going to minimize the cost function using great descent or one of the advance optimization algorithms. So, if you take derivatives you find that the great descent like these and so this term here is the partial derivative of the cost function, I’m not going to write that out, with respect to the feature value Xik and similarly this term here is also a partial derivative value of the cost function with respect to the parameter theta that we’re minimizing. And just as a reminder, in this formula that we no longer have this X0 equals 1 and so we have that x is in Rn and theta is a Rn. In this new formalism, we’re regularizing every one of our perimeters theta, you know, every one of our parameters Xn. There’s no longer the special case theta zero, which was regularized differently, or which was not regularized compared to the parameters theta 1 down to theta. So there is now no longer a theta 0, which is why in these updates, I did not break out a special case for k equals 0. So we then use gradient descent to minimize the cost function j with respect to the features x and with respect to the parameters theta. And finally, given a user, if a user has some parameters, theta, and if there’s a movie with some sort of learned features x, we would then predict that that movie would be given a star rating by that user of theta transpose j. Or just to fill those in, then we’re saying that if user J has not yet rated movie I, then what we do is predict that user J is going to rate movie I according to theta J transpose Xi. So that’s the collaborative filtering algorithm and if you implement this algorithm you actually get a pretty decent algorithm that will simultaneously learn good features for hopefully all the movies as well as learn parameters for all the users and hopefully give pretty good predictions for how different users will rate different movies that they have not yet rated. summaryTo speed things up, we can simultaneously minimize our features and our parameters:$$J(x,\theta) = \dfrac{1}{2} \displaystyle \sum_{(i,j):r(i,j)=1}((\theta^{(j)})^Tx^{(i)} - y^{(i,j)})^2 + \dfrac{\lambda}{2}\sum_{i=1}^{n_m} \sum_{k=1}^{n} (x_k^{(i)})^2 + \dfrac{\lambda}{2}\sum_{j=1}^{n_u} \sum_{k=1}^{n} (\theta_k^{(j)})^2$$It looks very complicated, but we’ve only combined the cost function for theta and the cost function for x.Because the algorithm can learn them itself, the bias units where $x_0=1$ have been removed, therefore $x∈ℝ^n$ and $θ∈ℝ^n$.These are the steps in the algorithm: Initialize $x^{(i)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)}$ to small random values. This serves to break symmetry and ensures that the algorithm learns features $x^{(i)},…,x^{(n_m)}$ that are different from each other. Minimize $J(x^{(i)},…,x^{(n_m)},\theta^{(1)},…,\theta^{(n_u)})$ using gradient descent (or an advanced optimization algorithm).E.g. for every $j=1,…,n_u,i=1,…n_m$:$$x_k^{(i)} := x_k^{(i)} - \alpha\left (\displaystyle \sum_{j:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) \theta_k^{(j)}} + \lambda x_k^{(i)} \right)$$$$\theta_k^{(j)} := \theta_k^{(j)} - \alpha\left (\displaystyle \sum_{i:r(i,j)=1}{((\theta^{(j)})^T x^{(i)} - y^{(i,j)}) x_k^{(i)}} + \lambda \theta_k^{(j)} \right)$$ For a user with parameters θ and a movie with (learned) features x, predict a star rating of $\theta^Tx$. 03_low-rank-matrix-factorization01_vectorization-low-rank-matrix-factorizationIn the last few videos, we talked about a collaborative filtering algorithm. In this video I’m going to say a little bit about the vectorization implementation of this algorithm. And also talk a little bit about other things you can do with this algorithm. For example, one of the things you can do is, given one product can you find other products that are related to this so that for example, a user has recently been looking at one product. Are there other related products that you could recommend to this user? So let’s see what we could do about that. What I’d like to do is work out an alternative way of writing out the predictions of the collaborative filtering algorithm. To start, here is our data set with our five movies and what I’m going to do is take all the ratings by all the users and group them into a matrix. So, here we have five movies and four users, and so this matrix y is going to be a 5 by 4 matrix. It’s just you know, taking all of the elements, all of this data. Including question marks, and grouping them into this matrix. And of course the elements of this matrix of the (i, j) element of this matrix is really what we were previously writing as y superscript i, j. It’s the rating given to movie i by user j. Given this matrix y of all the ratings that we have, there’s an alternative way of writing out all the predictive ratings of the algorithm. And, in particular if you look at what a certain user predicts on a certain movie, what user j predicts on movie i is given by this formula. And so, if you have a matrix of the predicted ratings, what you would have is the following matrix where the i, j entry. So this corresponds to the rating that we predict using j will give to movie i is exactly equal to that theta j transpose XI, and so, you know, this is a matrix where this first element the one-one element is a predictive rating of user one or movie one and this element, this is the one-two element is the predicted rating of user two on movie one, and so on, and this is the predicted rating of user one on the last movie and if you want, you know, this rating is what we would have predicted for this value and this rating is what we would have predicted for that value, and so on. Now, given this matrix of predictive ratings there is then a simpler or vectorized way of writing these out. In particular if I define the matrix x, and this is going to be just like the matrix we had earlier for linear regression to be sort of x1 transpose x2 transpose down to x of nm transpose. So I’m take all the features for my movies and stack them in rows. So if you think of each movie as one example and stack all of the features of the different movies and rows. And if we also to find a matrix capital theta, and what I’m going to do is take each of the per user parameter vectors, and stack them in rows, like so. So that’s theta 1, which is the parameter vector for the first user. And, you know, theta 2, and so, you must stack them in rows like this to define a matrix capital theta and so I have nu parameter vectors all stacked in rows like this. Now given this definition for the matrix x and this definition for the matrix theta in order to have a vectorized way of computing the matrix of all the predictions you can just compute x times the matrix theta transpose, and that gives you a vectorized way of computing this matrix over here. To give the collaborative filtering algorithm that you’ve been using another name. The algorithm that we’re using is also called low rank matrix factorization. And so if you hear people talk about low rank matrix factorization that’s essentially exactly the algorithm that we have been talking about. And this term comes from the property that this matrix x times theta transpose has a mathematical property in linear algebra called that this is a low rank matrix and so that’s what gives rise to this name low rank matrix factorization for these algorithms, because of this low rank property of this matrix x theta transpose. In case you don’t know what low rank means or in case you don’t know what a low rank matrix is, don’t worry about it. You really don’t need to know that in order to use this algorithm. But if you’re an expert in linear algebra, that’s what gives this algorithm, this other name of low rank matrix factorization. Finally, having run the collaborative filtering algorithm here’s something else that you can do which is use the learned features in order to find related movies. Specifically for each product i really for each movie i, we’ve learned a feature vector xi. So, you know, when you learn a certain features without really know that can the advance what the different features are going to be, but if you run the algorithm and perfectly the features will tend to capture what are the important aspects of these different movies or different products or what have you. What are the important aspects that cause some users to like certain movies and cause some users to like different sets of movies. So maybe you end up learning a feature, you know, where x1 equals romance, x2 equals action similar to an earlier video and maybe you learned a different feature x3 which is a degree to which this is a comedy. Then some feature x4 which is, you know, some other thing. And you have N features all together and after you have learned features it’s actually often pretty difficult to go in to the learned features and come up with a human understandable interpretation of what these features really are. But in practice, you know, the features even though these features can be hard to visualize. It can be hard to figure out just what these features are. Usually, it will learn features that are very meaningful for capturing whatever are the most important or the most salient properties of a movie that causes you to like or dislike it. And so now let’s say we want to address the following problem. Say you have some specific movie i and you want to find other movies j that are related to that movie. And so well, why would you want to do this? Right, maybe you have a user that’s browsing movies, and they’re currently watching movie j, than what’s a reasonable movie to recommend to them to watch after they’re done with movie j? Or if someone’s recently purchased movie j, well, what’s a different movie that would be reasonable to recommend to them for them to consider purchasing. So, now that you have learned these feature vectors, this gives us a very convenient way to measure how similar two movies are. In particular, movie i has a feature vector xi. and so if you can find a different movie, j, so that the distance between xi and xj is small, then this is a pretty strong indication that, you know, movies j and i are somehow similar. At least in the sense that some of them likes movie i, maybe more likely to like movie j as well. So, just to recap, if your user is looking at some movie i and if you want to find the 5 most similar movies to that movie in order to recommend 5 new movies to them, what you do is find the five movies j, with the smallest distance between the features between these different movies. And this could give you a few different movies to recommend to your user. So with that, hopefully, you now know how to use a vectorized implementation to compute all the predicted ratings of all the users and all the movies, and also how to do things like use learned features to find what might be movies and what might be products that aren’t related to each other. summaryVectorization: Low Rank Matrix FactorizationGiven matrices X (each row containing features of a particular movie) and Θ (each row containing the weights for those features for a given user), then the full matrix Y of all predicted ratings of all movies by all users is given simply by: $$Y = X\Theta^T$$.Predicting how similar two movies i and j are can be done using the distance between their respective feature vectors x. Specifically, we are looking for a small value of $||x^{(i)} - x^{(j)}||$. 02_implementational-detail-mean-normalizationBy now you’ve seen all of the main pieces of the recommender system algorithm or the collaborative filtering algorithm. In this video I want to just share one last implementational detail, namely mean normalization, which can sometimes just make the algorithm work a little bit better. To motivate the idea of mean normalization, let’s consider an example of where there’s a user that has not rated any movies. So, in addition to our four users, Alice, Bob, Carol, and Dave, I’ve added a fifth user, Eve, who hasn’t rated any movies. Let’s see what our collaborative filtering algorithm will do on this user. Let’s say that n is equal to 2 and so we’re going to learn two features and we are going to have to learn a parameter vector theta 5, which is going to be in R2, remember this is now vectors in Rn not Rn+1, we’ll learn the parameter vector theta 5 for our user number 5, Eve. So if we look in the first term in this optimization objective, well the user Eve hasn’t rated any movies, so there are no movies for which Rij is equal to one for the user Eve and so this first term plays no role at all in determining theta 5 because there are no movies that Eve has rated. And so the only term that effects theta 5 is this term. And so we’re saying that we want to choose vector theta 5 so that the last regularization term is as small as possible. In other words we want to minimize this lambda over 2 theta 5 subscript 1 squared plus theta 5 subscript 2 squared so that’s the component of the regularization term that corresponds to user 5, and of course if your goal is to minimize this term, then what you’re going to end up with is just theta 5 equals 0 0. Because a regularization term is encouraging us to set parameters close to 0 and if there is no data to try to pull the parameters away from 0, because this first term doesn’t effect theta 5, we just end up with theta 5 equals the vector of all zeros. And so when we go to predict how user 5 would rate any movie, we have that theta 5 transpose xi, for any i, that’s just going to be equal to zero. Because theta 5 is 0 for any value of x, this inner product is going to be equal to 0. And what we’re going to have therefore, is that we’re going to predict that Eve is going to rate every single movie with zero stars. But this doesn’t seem very useful does it? I mean if you look at the different movies, Love at Last, this first movie, a couple people rated it 5 stars. And for even the Swords vs. Karate, someone rated it 5 stars. So some people do like some movies. It seems not useful to just predict that Eve is going to rate everything 0 stars. And in fact if we’re predicting that eve is going to rate everything 0 stars, we also don’t have any good way of recommending any movies to her, because you know all of these movies are getting exactly the same predicted rating for Eve so there’s no one movie with a higher predicted rating that we could recommend to her, so, that’s not very good. The idea of mean normalization will let us fix this problem. So here’s how it works. As before let me group all of my movie ratings into this matrix Y, so just take all of these ratings and group them into matrix Y. And this column over here of all question marks corresponds to Eve’s not having rated any movies. Now to perform mean normalization what I’m going to do is compute the average rating that each movie obtained. And I’m going to store that in a vector that we’ll call mu. So the first movie got two 5-star and two 0-star ratings, so the average of that is a 2.5-star rating. The second movie had an average of 2.5-stars and so on. And the final movie that has 0, 0, 5, 0. And the average of 0, 0, 5, 0, that averages out to an average of 1.25 rating. And what I’m going to do is look at all the movie ratings and I’m going to subtract off the mean rating. So this first element 5 I’m going to subtract off 2.5 and that gives me 2.5. And the second element 5 subtract off of 2.5, get a 2.5. And then the 0, 0, subtract off 2.5 and you get -2.5, -2.5. In other words, what I’m going to do is take my matrix of movie ratings, take this wide matrix, and subtract form each row the average rating for that movie. So, what I’m doing is just normalizing each movie to have an average rating of zero. And so just one last example. If you look at this last row, 0 0 5 0. We’re going to subtract 1.25, and so I end up with these values over here. So now and of course the question marks stay a question mark. So each movie in this new matrix Y has an average rating of 0. What I’m going to do then, is take this set of ratings and use it with my collaborative filtering algorithm. So I’m going to pretend that this was the data that I had gotten from my users, or pretend that these are the actual ratings I had gotten from the users, and I’m going to use this as my data set with which to learn my parameters theta J and my features XI - from these mean normalized movie ratings. When I want to make predictions of movie ratings, what I’m going to do is the following: for user J on movie I, I’m gonna predict theta J transpose XI, where X and theta are the parameters that I’ve learned from this mean normalized data set. But, because on the data set, I had subtracted off the means in order to make a prediction on movie i, I’m going to need to add back in the mean, and so i’m going to add back in mu i. And so that’s going to be my prediction where in my training data subtracted off all the means and so when we make predictions and we need to add back in these means mu i for movie i. And so specifically if you user 5 which is Eve, the same argument as the previous slide still applies in the sense that Eve had not rated any movies and so the learned parameter for user 5 is still going to be equal to 0, 0. And so what we’re going to get then is that on a particular movie i we’re going to predict for Eve theta 5, transpose xi plus add back in mu i and so this first component is going to be equal to zero, if theta five is equal to zero. And so on movie i, we are going to end a predicting mu i. And, this actually makes sense. It means that on movie 1 we’re going to predict Eve rates it 2.5. On movie 2 we’re gonna predict Eve rates it 2.5. On movie 3 we’re gonna predict Eve rates it at 2 and so on. This actually makes sense, because it says that if Eve hasn’t rated any movies and we just don’t know anything about this new user Eve, what we’re going to do is just predict for each of the movies, what are the average rating that those movies got. Finally, as an aside, in this video we talked about mean normalization, where we normalized each row of the matrix y, to have mean 0. In case you have some movies with no ratings, so it is analogous to a user who hasn’t rated anything, but in case you have some movies with no ratings, you can also play with versions of the algorithm, where you normalize the different columns to have means zero, instead of normalizing the rows to have mean zero, although that’s maybe less important, because if you really have a movie with no rating, maybe you just shouldn’t recommend that movie to anyone, anyway. And so, taking care of the case of a user who hasn’t rated anything might be more important than taking care of the case of a movie that hasn’t gotten a single rating. So to summarize, that’s how you can do mean normalization as a sort of pre-processing step for collaborative filtering. Depending on your data set, this might some times make your implementation work just a little bit better. summaryIf the ranking system for movies is used from the previous lectures, then new users (who have watched no movies), will be assigned new movies incorrectly. Specifically, they will be assigned θ with all components equal to zero due to the minimization of the regularization term. That is, we assume that the new user will rank all movies 0, which does not seem intuitively correct.We rectify this problem by normalizing the data relative to the mean. First, we use a matrix Y to store the data from previous ratings, where the ith row of Y is the ratings for the ith movie and the jth column corresponds to the ratings for the jth user.We can now define a vector $\mu = [\mu_1, \mu_2, \dots , \mu_{n_m}]$ such that $\mu_i = \frac{\sum_{j:r(i,j)=1}{Y_{i,j}}}{\sum_{j}{r(i,j)}}$Which is effectively the mean of the previous ratings for the ith movie (where only movies that have been watched by users are counted). We now can normalize the data by subtracting u, the mean rating, from the actual ratings for each user (column in matrix Y):As an example, consider the following matrix Y and mean ratings μ:$$Y = \begin{bmatrix} 5 &amp; 5 &amp; 0 &amp; 0 \\ 4 &amp; ? &amp; ? &amp; 0 \\ 0 &amp; 0 &amp; 5 &amp; 4 \\ 0 &amp; 0 &amp; 5 &amp; 0 \\ \end{bmatrix}, \quad \mu = \begin{bmatrix} 2.5 \\ 2 \\ 2.25 \\ 1.25 \\ \end{bmatrix}$$The resulting Y′ vector is:$$Y’ = \begin{bmatrix} 2.5 &amp; 2.5 &amp; -2.5 &amp; -2.5 \\ 2 &amp; ? &amp; ? &amp; -2 \\ -.2.25 &amp; -2.25 &amp; 3.75 &amp; 1.25 \\ -1.25 &amp; -1.25 &amp; 3.75 &amp; -1.25 \end{bmatrix}$$Now we must slightly modify the linear regression prediction to include the mean normalization term:$$(\theta^{(j)})^T x^{(i)} + \mu_i$$Now, for a new user, the initial predicted values will be equal to the μ term instead of simply being initialized to zero, which is more accurate.]]></content>
      <categories>
        <category>英文</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[15_anomaly-detection note15]]></title>
    <url>%2F2018%2F01%2F15%2F15_anomaly-detection%2F</url>
    <content type="text"><![CDATA[NoteThis personal note is written after studying the opening course on the coursera website, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. 01_density-estimation01_problem-motivationIn this next set of videos, I’d like to tell you about a problem called Anomaly Detection. This is a reasonably commonly use you type machine learning. And one of the interesting aspects is that it’s mainly for unsupervised problem, that there’s some aspects of it that are also very similar to sort of the supervised learning problem. So, what is anomaly detection? example To explain it. Let me use the motivating example of: Imagine that you’re a manufacturer of aircraft engines, and let’s say that as your aircraft engines roll off the assembly line, you’re doing, you know, QA or quality assurance testing, and as part of that testing you measure features of your aircraft engine, like maybe, you measure the heat generated, things like the vibrations and so on. I share some friends that worked on this problem a long time ago, and these were actually the sorts of features that they were collecting off actual aircraft engines so you now have a data set of X1 through Xm, if you have manufactured m aircraft engines, and if you plot your data, maybe it looks like this. So, each point here, each cross here as one of your unlabeled examples. So, the anomaly detection problem is the following. Let’s say that on, you know, the next day, you have a new aircraft engine that rolls off the assembly line and your new aircraft engine has some set of features x-test. What the anomaly detection problem is, we want to know if this aircraft engine is anomalous in any way, in other words, we want to know if, maybe, this engine should undergo further testing because, or if it looks like an okay engine, and so it’s okay to just ship it to a customer without further testing. So, if your new aircraft engine looks like a point over there, well, you know, that looks a lot like the aircraft engines we’ve seen before, and so maybe we’ll say that it looks okay. Whereas, if your new aircraft engine, if x-test, you know, were a point that were out here, so that if X1 and X2 are the features of this new example. If x-tests were all the way out there, then we would call that an anomaly. and maybe send that aircraft engine for further testing before we ship it to a customer, since it looks very different than the rest of the aircraft engines we’ve seen before. Desity_estimation More formally in the anomaly detection problem, we’re give some data sets, x1 through Xm of examples, and we usually assume that these end examples are normal or non-anomalous examples, and we want an algorithm to tell us if some new example x-test is anomalous. The approach that we’re going to take is that given this training set, given the unlabeled training set, we’re going to build a model for p of x. In other words, we’re going to build a model for the probability of x, where x are these features of, say, aircraft engines. And so, having built a model of the probability of x we’re then going to say that for the new aircraft engine, if p of x-test is less than some epsilon then we flag this as an anomaly. So we see a new engine that, you know, has very low probability under a model p of x that we estimate from the data, then we flag this anomaly, whereas if p of x-test is, say, greater than or equal to some small threshold. Then we say that, you know, okay, it looks okay. And so, given the training set, like that plotted here, if you build a model, hopefully you will find that aircraft engines, or hopefully the model p of x will say that points that lie, you know, somewhere in the middle, that’s pretty high probability, whereas points a little bit further out have lower probability. Points that are even further out have somewhat lower probability, and the point that’s way out here, the point that’s way out there, would be an anomaly. Whereas the point that’s way in there, right in the middle, this would be okay because p of x right in the middle of that would be very high cause we’ve seen a lot of points in that region. Here are some examples of applications of anomaly detection. Perhaps the most common application of anomaly detection is actually for detection if you have many users, and if each of your users take different activities, you know maybe on your website or in the physical plant or something, you can compute features of the different users activities. And what you can do is build a model to say, you know, what is the probability of different users behaving different ways. What is the probability of a particular vector of features of a users behavior so you know examples of features of a users activity may be on the website it’d be things like, maybe x1 is how often does this user log in, x2, you know, maybe the number of what pages visited, or the number of transactions, maybe x3 is, you know, the number of posts of the users on the forum, feature x4 could be what is the typing speed of the user and some websites can actually track that was the typing speed of this user in characters per second. And so you can model p of x based on this sort of data. And finally having your model p of x, you can try to identify users that are behaving very strangely on your website by checking which ones have probably effects less than epsilon and maybe send the profiles of those users for further review. Or demand additional identification from those users, or some such to guard against you know, strange behavior or fraudulent behavior on your website. This sort of technique will tend of flag the users that are behaving unusually, not just users that maybe behaving fraudulently. So not just constantly having stolen or users that are trying to do funny things, or just find unusual users. But this is actually the technique that is used by many online websites that sell things to try identify users behaving strangely that might be indicative of either fraudulent behavior or of computer accounts that have been stolen. Another example of anomaly detection is manufacturing. So, already talked about the aircraft engine thing where you can find unusual, say, aircraft engines and send those for further review. A third application would be monitoring computers in a data center. I actually have some friends who work on this too. So if you have a lot of machines in a computer cluster or in a data center, we can do things like compute features at each machine. So maybe some features capturing you know, how much memory used, number of disc accesses, CPU load. As well as more complex features like what is the CPU load on this machine divided by the amount of network traffic on this machine? Then given the dataset of how your computers in your data center usually behave, you can model the probability of x, so you can model the probability of these machines having different amounts of memory use or probability of these machines having different numbers of disc accesses or different CPU loads and so on. And if you ever have a machine whose probability of x, p of x, is very small then you know that machine is behaving unusually and maybe that machine is about to go down, and you can flag that for review by a system administrator. And this is actually being used today by various data centers to watch out for unusual things happening on their machines. So, that’s anomaly detection. In the next video, I’ll talk a bit about the Gaussian distribution and review properties of the Gaussian probability distribution, and in videos after that, we will apply it to develop an anomaly detection algorithm. summaryProblem MotivationJust like in other learning problems, we are given a dataset ${x^{(1)}, x^{(2)},\dots,x^{(m)}}$.We are then given a new example, $x_{test}$, and we want to know whether this new example is abnormal/anomalous.We define a “model” $p(x)$ that tells us the probability the example is not anomalous. We also use a threshold $ϵ$ (epsilon) as a dividing line so we can say which examples are anomalous and which are not.A very common application of anomaly detection is detecting fraud: $x^{(i)} =$ features of user i’s activities Model $p(x)$ from the data. Identify unusual users by checking which have $p(x)&lt;ϵ$. If our anomaly detector is flagging too many anomalous examples, then we need to decrease our threshold $ϵ$ 02_gaussian-distributionIn this video, I’d like to talk aboutthe Gaussian distribution which is also called the normal distribution. In case you’re already intimately familiar with the Gaussian distribution, it’s probably okay to skip this video,but if you’re not sure or if it has been a while since you’ve worked with the Gaussian distribution or normal distribution then please do watch this video all the way to the end. And in the video after this we’ll start applying the Gaussian distribution to developing an anomaly detection algorithm. Let’s say x is a row value’s random variable, so x is a row number. If the probability distribution of x is Gaussian with mean mu and variance sigma squared. Then, we’ll write this as x, the random variable. Tilde, this little tilde, this is distributed as. And then to denote a Gaussian distribution, sometimes I’m going to write script N parentheses mu comma sigma script. So this script N stands for normal since Gaussian and normal they mean the thing are synonyms. And the Gaussian distribution is parametarized by two parameters, by a mean parameter which we denote mu and a variance parameter which we denote via sigma squared. If we plot the Gaussian distribution or Gaussian probability density. It’ll look like the bell shaped curve which you may have seen before. And so this bell shaped curve is paramafied by those two parameters, mu and sequel. And the location of the center of this bell shaped curve is the mean mu. And the width of this bell shaped curve, roughly that, is this parameter, sigma, is also called one standard deviation, and so this specifies the probability of x taking on different values. So, x taking on values here in the middle here it’s pretty high, since the Gaussian density here is pretty high, whereas x taking on values further, and further away will be diminishing in probability. Finally just for completeness let me write out the formula for the Gaussian distribution. So the probability of x, and I’ll sometimes write this as the p (x) when we write this as P ( x ; mu, sigma squared), and so this denotes that the probability of X is parameterized by the two parameters mu and sigma squared. And the formula for the Gaussian density is this 1/ root 2 pi, sigma e (-(x-mu/g) squared/2 sigma squared. So there’s no need to memorize this formula. This is just the formula for the bell-shaped curve over here on the left. There’s no need to memorize it, and if you ever need to use this, you can always look this up. And so that figure on the left, that is what you get if you take a fixed value of mu and take a fixed value of sigma, and you plot P(x) so this curve here. This is really p(x) plotted as a function of X for a fixed value of Mu and of sigma squared. And by the way sometimes it’s easier to think in terms of sigma squared that’s called the variance. And sometimes is easier to think in terms of sigma. So sigma is called the standard deviation, and so it specifies the width of this Gaussian probability density, where as the square sigma, or sigma squared, is called the variance. Let’s look at some examples of what the Gaussian distribution looks like. If mu equals zero, sigma equals one. Then we have a Gaussian distribution that’s centered around zero, because that’s mu and the width of this Gaussian, so that’s one standard deviation is sigma over there. Let’s look at some examples of Gaussians. If mu is equal to zero and sigma equals one, then that corresponds to a Gaussian distribution that is centered at zero, since mu is zero, and the width of this Gaussian is is controlled by sigma by that variance parameter sigma. Here’s another example. That same mu is equal to 0 and sigma is equal to .5 so the standard deviation is .5 and the variance sigma squared would therefore be the square of 0.5 would be 0.25 and in that case the Gaussian distribution, the Gaussian probability density goes like this. Is also sent as zero. But now the width of this is much smaller because the smaller the area is, the width of this Gaussian density is roughly half as wide. But because this is a probability distribution, the area under the curve, that’s the shaded area there. That area must integrate to one this is a property of probability distributing. So this is a much taller Gaussian density because this half is Y but half the standard deviation but it twice as tall. Another example is sigma is equal to 2 then you get a much fatter a much wider Gaussian density and so here the sigma parameter controls that Gaussian distribution has a wider width. And once again, the area under the curve, that is the shaded area, will always integrate to one, that’s the property of probability distributions and because it’s wider it’s also half as tall in order to still integrate to the same thing. And finally one last example would be if we now change the mu parameters as well. Then instead of being centered at 0 we now have a Gaussian distribution that’s centered at 3 because this shifts over the entire Gaussian distribution. Next, let’s talk about the Parameter estimation problem. So what’s the parameter estimation problem? Let’s say we have a dataset of m examples so exponents x m and lets say each of this example is a row number. Here in the figure I’ve plotted an example of the dataset so the horizontal axis is the x axis and either will have a range of examples of x, and I’ve just plotted them on this figure here. And the parameter estimation problem is, let’s say I suspect that these examples came from a Gaussian distribution. So let’s say I suspect that each of my examples, x i, was distributed. That’s what this tilde thing means. Let’s not suspect that each of these examples were distributed according to a normal distribution, or Gaussian distribution, with some parameter mu and some parameter sigma square. But I don’t know what the values of these parameters are. The problem of parameter estimation is, given my data set, I want to try to figure out, well I want to estimate what are the values of mu and sigma squared. So if you’re given a data set like this, it looks like maybe if I estimate what Gaussian distribution the data came from, maybe that might be roughly the Gaussian distribution it came from. With mu being the center of the distribution, sigma standing for the deviation controlling the width of this Gaussian distribution. Seems like a reasonable fit to the data. Because, you know, looks like the data has a very high probability of being in the central region, and a low probability of being further out, even though probability of being further out, and so on. So maybe this is a reasonable estimate of mu and sigma squared. That is, if it corresponds to a Gaussian distribution function that looks like this. So what I’m going to do is just write out the formula the standard formulas for estimating the parameters Mu and sigma squared. Our estimate or the way we’re going to estimate mu is going to be just the average of my example. So mu is the mean parameter. Just take my training set, take my m examples and average them. And that just means the center of this distribution. How about sigma squared? Well, the variance, I’ll just write out the standard formula again, I’m going to estimate as sum over one through m of x i minus mu squared. And so this mu here is actually the mu that I compute over here using this formula. And what the variance is, or one interpretation of the variance is that if you look at this term, that’s the square difference between the value I got in my example minus the mean. Minus the center, minus the mean of the distribution. And so in the variance I’m gonna estimate as just the average of the square differences between my examples, minus the mean. And as a side comment, only for those of you that are experts in statistics. If you’re an expert in statistics, and if you’ve heard of maximum likelihood estimation, then these parameters, these estimates, are actually the maximum likelihood estimates of the parameters of mu and sigma squared but if you haven’t heard of that before don’t worry about it, all you need to know is that these are the two standard formulas for how to figure out what are mu and Sigma squared given the data set. Finally one last side comment again only for those of you that have maybe taken the statistics class before but if you’ve taken statistics This class before. Some of you may have seen the formula here where this is M-1 instead of M so this first term becomes 1/M-1 instead of 1/M. In machine learning people tend to learn 1/M formula but in practice whether it is 1/M or 1/M-1 it makes essentially no difference assuming M is reasonably large. a reasonably large training set size. So just in case you’ve seen this other version before. In either version it works just about equally well but in machine learning most people tend to use 1/M in this formula.And the two versions have slightly different theoretical properties like these are different math properties. Bit of practice it really makes makes very little difference, if any. So, hopefully you now have a good sense of what the Gaussian distribution looks like, as well as how to estimate the parameters mu and sigma squared of Gaussian distribution if you’re given a training set, that is if you’re given a set of data that you suspect comes from a Gaussian distribution with unknown parameters, mu and sigma squared. In the next video, we’ll start to take this and apply it to develop an anomaly detection algorithm. summaryThe Gaussian Distribution is a familiar bell-shaped curve that can be described by a function $\mathcal{N}(\mu,\sigma^2)$Let x∈ℝ. If the probability distribution of x is Gaussian with mean μ, variance $\sigma^2$, then:$$x \sim \mathcal{N}(\mu, \sigma^2)$$The little ∼ or ‘tilde’ can be read as “distributed as.”The Gaussian Distribution is parameterized by a mean and a variance.Mu, or μ, describes the center of the curve, called the mean. The width of the curve is described by sigma, or σ, called the standard deviation.The full function is as follows:$$\large p(x;\mu,\sigma^2) = \dfrac{1}{\sigma\sqrt{(2\pi)}}e^{-\dfrac{1}{2}(\dfrac{x - \mu}{\sigma})^2}$$We can estimate the parameter μ from a given dataset by simply taking the average of all the examples:$$\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}$$We can estimate the other parameter, $\sigma^2$, with our familiar squared error formula:$$\sigma^2 = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x^{(i)} - \mu)^2$$ 03_algorithmIn the last video, we talked about the Gaussian distribution. In this video lets apply that to develop an anomaly detection algorithm. Let’s say that we have an unlabeled training set of M examples, and each of these examples is going to be a feature in Rn so your training set could be, feature vectors from the last M aircraft engines being manufactured. Or it could be features from m users or something else. The way we are going to address anomaly detection, is we are going to model p of x from the data sets. We’re going to try to figure out what are high probability features, what are lower probability types of features. So, x is a vector and what we are going to do is model p of x, as probability of x1, that is of the first component of x, times the probability of x2, that is the probability of the second feature, times the probability of the third feature, and so on up to the probability of the final feature of Xn. Now I’m leaving space here cause I’ll fill in something in a minute. So, how do we model each of these terms, p of X1, p of X2, and so on. What we’re going to do, is assume that the feature, X1, is distributed according to a Gaussian distribution, with some mean, which you want to write as mu1 and some variance, which I’m going to write as sigma squared 1, and so p of X1 is going to be a Gaussian probability distribution, with mean mu1 and variance sigma squared 1. And similarly I’m going to assume that X2 is distributed, Gaussian, that’s what this little tilda stands for, that means distributed Gaussian with mean mu2 and Sigma squared 2, so it’s distributed according to a different Gaussian, which has a different set of parameters, mu2 sigma square 2. And similarly, you know, X3 is yet another Gaussian, so this can have a different mean and a different standard deviation than the other features, and so on, up to XN. And so that’s my model. Just as a side comment for those of you that are experts in statistics, it turns out that this equation that I just wrote out actually corresponds to an independence assumption on the values of the features x1 through xn. But in practice it turns out that the algorithm of this fragment, it works just fine, whether or not these features are anywhere close to independent and even if independence assumption doesn’t hold true this algorithm works just fine. But in case you don’t know those terms I just used independence assumptions and so on, don’t worry about it. You’ll be able to understand it and implement this algorithm just fine and that comment was really meant only for the experts in statistics. Finally, in order to wrap this up, let me take this expression and write it a little bit more compactly. So, we’re going to write this is a product from J equals one through N, of P of XJ parameterized by mu j comma sigma squared j. So this funny symbol here, there is capital Greek alphabet pi, that funny symbol there corresponds to taking the product of a set of values. And so, you’re familiar with the summation notation, so the sum from i equals one through n, of i. This means 1 + 2 + 3 plus dot dot dot, up to n. Where as this funny symbol here, this product symbol, right product from i equals 1 through n of i. Then this means that, it’s just like summation except that we’re now multiplying. This becomes 1 times 2 times 3 times up to N. And so using this product notation, this product from j equals 1 through n of this expression. It’s just more compact, it’s just shorter way for writing out this product of of all of these terms up there. Since we’re are taking these p of x j given mu j comma sigma squared j terms and multiplying them together. And, by the way the problem of estimating this distribution p of x, they’re sometimes called the problem of density estimation. Hence the title of the slide. So putting everything together, here is our anomaly detection algorithm. The first step is to choose features, or come up with features xi that we think might be indicative of anomalous examples. So what I mean by that, is, try to come up with features, so that when there’s an unusual user in your system that may be doing fraudulent things, or when the aircraft engine examples, you know there’s something funny, something strange about one of the aircraft engines. Choose features X I, that you think might take on unusually large values, or unusually small values, for what an anomalous example might look like. But more generally, just try to choose features that describe general properties of the things that you’re collecting data on. Next, given a training set, of M, unlabled examples, X1 through X M, we then fit the parameters, mu 1 through mu n, and sigma squared 1 through sigma squared n, and so these were the formulas similar to the formulas we have in the previous video, that we’re going to use the estimate each of these parameters, and just to give some interpretation, mu J, that’s my average value of the j feature. Mu j goes in this term p of xj. which is parametrized by mu J and sigma squared J. And so this says for the mu J just take the mean over my training set of the values of the j feature. And, just to mention, that you do this, you compute these formulas for j equals one through n. So use these formulas to estimate mu 1, to estimate mu 2, and so on up to mu n, and similarly for sigma squared, and it’s also possible to come up with vectorized versions of these. So if you think of mu as a vector, so mu if is a vector there’s mu 1, mu 2, down to mu n, then a vectorized version of that set of parameters can be written like so sum from 1 equals one through n xi. So, this formula that I just wrote out estimates this xi as the feature vectors that estimates mu for all the values of n simultaneously. And it’s also possible to come up with a vectorized formula for estimating sigma squared j. Finally, when you’re given a new example, so when you have a new aircraft engine and you want to know is this aircraft engine anomalous. What we need to do is then compute p of x, what’s the probability of this new example? So, p of x is equal to this product, and what you implement, what you compute, is this formula and where over here, this thing here this is just the formula for the Gaussian probability, so you compute this thing, and finally if this probability is very small, then you flag this thing as an anomaly. Here’s an example of an application of this method. Let’s say we have this data set plotted on the upper left of this slide. if you look at this, well, lets look the feature of x1. If you look at this data set, it looks like on average, the features x1 has a mean of about 5 and the standard deviation, if you only look at just the x1 values of this data set has the standard deviation of maybe 2. So that sigma 1 and looks like x2 the values of the features as measured on the vertical axis, looks like it has an average value of about 3, and a standard deviation of about 1. So if you take this data set and if you estimate mu1, mu2, sigma1, sigma2, this is what you get. And again, I’m writing sigma here, I’m think about standard deviations, but the formula on the previous 5 actually gave the estimates of the squares of theses things, so sigma squared 1 and sigma squared 2. So, just be careful whether you are using sigma 1, sigma 2, or sigma squared 1 or sigma squared 2. So, sigma squared 1 of course would be equal to 4, for example, as the square of 2. And in pictures what p of x1 parametrized by mu1 and sigma squared 1 and p of x2, parametrized by mu 2 and sigma squared 2, that would look like these two distributions over here. And, turns out that if were to plot of p of x, right, which is the product of these two things, you can actually get a surface plot that looks like this. This is a plot of p of x, where the height above of this, where the height of this surface at a particular point, so given a particular x1 x2 values of x2 if x1 equals 2, x equal 2, that’s this point. And the height of this 3-D surface here, that’s p of x. So p of x, that is the height of this plot, is literally just p of x1 parametrized by mu 1 sigma squared 1, times p of x2 parametrized by mu 2 sigma squared 2. Now, so this is how we fit the parameters to this data. Let’s see if we have a couple of new examples. Maybe I have a new example there. Is this an anomaly or not? Or, maybe I have a different example, maybe I have a different second example over there. So, is that an anomaly or not? They way we do that is, we would set some value for Epsilon, let’s say I’ve chosen Epsilon equals 0.02. I’ll say later how we choose Epsilon. But let’s take this first example, let me call this example X1 test. And let me call the second example X2 test. What we do is, we then compute p of X1 test, so we use this formula to compute it and this looks like a pretty large value. In particular, this is greater than, or greater than or equal to epsilon. And so this is a pretty high probability at least bigger than epsilon, so we’ll say that X1 test is not an anomaly. Whereas, if you compute p of X2 test, well that is just a much smaller value. So this is less than epsilon and so we’ll say that that is indeed an anomaly, because it is much smaller than that epsilon that we then chose. And in fact, I’d improve it here. What this is really saying is that, you look through the 3d surface plot. It’s saying that all the values of x1 and x2 that have a high height above the surface, corresponds to an a non-anomalous example of an OK or normal example. Whereas all the points far out here, all the points out here, all of those points have very low probability, so we are going to flag those points as anomalous, and so it’s gonna define some region, that maybe looks like this, so that everything outside this, it flags as anomalous, whereas the things inside this ellipse I just drew, if it considers okay, or non-anomalous, not anomalous examples. And so this example x2 test lies outside that region, and so it has very small probability, and so we consider it an anomalous example. In this video we talked about how to estimate p of x, the probability of x, for the purpose of developing an anomaly detection algorithm. And in this video, we also stepped through an entire process of giving data set, we have, fitting the parameters, doing parameter estimations. We get mu and sigma parameters, and then taking new examples and deciding if the new examples are anomalous or not. In the next few videos we will delve deeper into this algorithm, and talk a bit more about how to actually get this to work well. summaryGiven a training set of examples, $\lbrace x^{(1)},\dots,x^{(m)}\rbrace$ where each example is a vector, $x \in \mathbb{R}^n$.$$p(x) = p(x_1;\mu_1,\sigma_1^2)p(x_2;\mu_2,\sigma^2_2)\cdots p(x_n;\mu_n,\sigma^2_n)$$In statistics, this is called an “independence assumption” on the values of the features inside training example x.More compactly, the above expression can be written as follows:$$= \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2)$$The algorithmChoose features $x_i$ that you think might be indicative of anomalous examples.Fit parameters $$\mu_1,\dots,\mu_n,\sigma_1^2,\dots,\sigma_n^2$$Calculate $\mu_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x_j^{(i)}$Calculate $\sigma^2_j = \dfrac{1}{m}\displaystyle \sum_{i=1}^m(x_j^{(i)} - \mu_j)^2$Given a new example x, compute p(x):$$p(x) = \displaystyle \prod^n_{j=1} p(x_j;\mu_j,\sigma_j^2) = \prod\limits^n_{j=1} \dfrac{1}{\sqrt{2\pi}\sigma_j}exp(-\dfrac{(x_j - \mu_j)^2}{2\sigma^2_j})$$Anomaly if p(x)&lt;ϵA vectorized version of the calculation for μ is $\mu = \dfrac{1}{m}\displaystyle \sum_{i=1}^m x^{(i)}$. You can vectorize $\sigma^2$ similarly. 02_building-an-anomaly-detection-system01_developing-and-evaluating-an-anomaly-detection-systemIn the last video, we developed an anomaly detection algorithm. In this video, I like to talk about the process of how to go about developing a specific application of anomaly detection to a problem and in particular this will focus on the problem of how to evaluate an anomaly detection algorithm. In previous videos, we’ve already talked about the importance of real number evaluation and this captures the idea that when you’re trying to develop a learning algorithm for a specific application, you need to often make a lot of choices like, you know, choosing what features to use and then so on. And making decisions about all of these choices is often much easier, and if you have a way to evaluate your learning algorithm that just gives you back a number. So if you’re trying to decide, you know, I have an idea for one extra feature, do I include this feature or not. If you can run the algorithm with the feature, and run the algorithm without the feature, and just get back a number that tells you, you know, did it improve or worsen performance to add this feature? Then it gives you a much better way, a much simpler way, with which to decide whether or not to include that feature. So in order to be able to develop an anomaly detection system quickly, it would be a really helpful to have a way of evaluating an anomaly detection system. In order to do this, in order to evaluate an anomaly detection system, we’re actually going to assume have some labeled data. So, so far, we’ll be treating anomaly detection as an unsupervised learning problem, using unlabeled data. But if you have some labeled data that specifies what are some anomalous examples, and what are some non-anomalous examples, then this is how we actually think of as the standard way of evaluating an anomaly detection algorithm. So taking the aircraft engine example again. Let’s say that, you know, we have some label data of just a few anomalous examples of some aircraft engines that were manufactured in the past that turns out to be anomalous. Turned out to be flawed or strange in some way. Let’s say we use we also have some non-anomalous examples, so some perfectly okay examples. I’m going to use y equals 0 to denote the normal or the non-anomalous example and y equals 1 to denote the anomalous examples. The process of developing and evaluating an anomaly detection algorithm is as follows. We’re going to think of it as a training set and talk about the cross validation in test sets later, but the training set we usually think of this as still the unlabeled training set. And so this is our large collection of normal, non-anomalous or not anomalous examples. And usually we think of this as being as non-anomalous, but it’s actually okay even if a few anomalies slip into your unlabeled training set. And next we are going to define a cross validation set and a test set, with which to evaluate a particular anomaly detection algorithm. So, specifically, for both the cross validation test sets we’re going to assume that, you know, we can include a few examples in the cross validation set and the test set that contain examples that are known to be anomalous. So the test sets say we have a few examples with y equals 1 that correspond to anomalous aircraft engines. So here’s a specific example. Let’s say that, altogether, this is the data that we have. We have manufactured 10,000 examples of engines that, as far as we know we’re perfectly normal, perfectly good aircraft engines. And again, it turns out to be okay even if a few flawed engine slips into the set of 10,000 is actually okay, but we kind of assumed that the vast majority of these 10,000 examples are, you know, good and normal non-anomalous engines. And let’s say that, you know, historically, however long we’ve been running on manufacturing plant, let’s say that we end up getting features, getting 24 to 28 anomalous engines as well. And for a pretty typical application of anomaly detection, you know, the number non-anomalous examples, that is with y equals 1, we may have anywhere from, you know, 20 to 50. It would be a pretty typical range of examples, number of examples that we have with y equals 1. And usually we will have a much larger number of good examples. So, given this data set, a fairly typical way to split it into the training set, cross validation set and test set would be as follows. Let’s take 10,000 good aircraft engines and put 6,000 of that into the unlabeled training set. So, I’m calling this an unlabeled training set but all of these examples are really ones that correspond to y equals 0, as far as we know. And so, we will use this to fit p of x, right. So, we will use these 6000 engines to fit p of x, which is that p of x one parametrized by Mu 1, sigma squared 1, up to p of Xn parametrized by Mu N sigma squared n. And so it would be these 6,000 examples that we would use to estimate the parameters Mu 1, sigma squared 1, up to Mu N, sigma squared N. And so that’s our training set of all, you know, good, or the vast majority of good examples. Next we will take our good aircraft engines and put some number of them in a cross validation set plus some number of them in the test sets. So 6,000 plus 2,000 plus 2,000, that’s how we split up our 10,000 good aircraft engines. And then we also have 20 flawed aircraft engines, and we’ll take that and maybe split it up, you know, put ten of them in the cross validation set and put ten of them in the test sets. And in the next slide we will talk about how to actually use this to evaluate the anomaly detection algorithm. So what I have just described here is a you know probably the recommend a good way of splitting the labeled and unlabeled example. The good and the flawed aircraft engines. Where we use like a 60, 20, 20% split for the good engines and we take the flawed engines, and we put them just in the cross validation set, and just in the test set, then we’ll see in the next slide why that’s the case. Just as an aside, if you look at how people apply anomaly detection algorithms, sometimes you see other peoples’ split the data differently as well. So, another alternative, this is really not a recommended alternative, but some people want to take off your 10,000 good engines, maybe put 6000 of them in your training set and then put the same 4000 in the cross validation set and the test set. And so, you know, we like to think of the cross validation set and the test set as being completely different data sets to each other. But you know, in anomaly detection, you know, for sometimes you see people, sort of, use the same set of good engines in the cross validation sets, and the test sets, and sometimes you see people use exactly the same sets of anomalous engines in the cross validation set and the test set. And so, all of these are considered, you know, less good practices and definitely less recommended. Certainly using the same data in the cross validation set and the test set, that is not considered a good machine learning practice. But, sometimes you see people do this too. So, given the training cross validation and test sets, here’s how you evaluate or here is how you develop and evaluate an algorithm. First, we take the training sets and we fit the model p of x. So, we fit, you know, all these Gaussians to my m unlabeled examples of aircraft engines, and these, I am calling them unlabeled examples, but these are really examples that we’re assuming our goods are the normal aircraft engines. Then imagine that your anomaly detection algorithm is actually making prediction. So, on the cross validation of the test set, given that, say, test example X, think of the algorithm as predicting that y is equal to 1, p of x is less than epsilon, we must be taking zero, if p of x is greater than or equal to epsilon. So, given x, it’s trying to predict, what is the label, given y equals 1 corresponding to an anomaly or is it y equals 0 corresponding to a normal example? So given the training, cross validation, and test sets. How do you develop an algorithm? And more specifically, how do you evaluate an anomaly detection algorithm? Well, to this whole, the first step is to take the unlabeled training set, and to fit the model p of x lead training data. So you take this, you know on I’m coming, unlabeled training set, but really, these are examples that we are assuming, vast majority of which are normal aircraft engines, not because they’re not anomalies and it will fit the model p of x. It will fit all those parameters for all the Gaussians on this data. Next on the cross validation of the test set, we’re going to think of the anomaly detention algorithm as trying to predict the value of y. So in each of like say test examples. We have these X-I tests, Y-I test, where y is going to be equal to 1 or 0 depending on whether this was an anomalous example. So given input x in my test set, my anomaly detection algorithm think of it as predicting the y as 1 if p of x is less than epsilon. So predicting that it is an anomaly, it is probably is very low. And we think of the algorithm is predicting that y is equal to 0. If p of x is greater then or equals epsilon. So predicting those normal example if the p of x is reasonably large. And so we can now think of the anomaly detection algorithm as making predictions for what are the values of these y labels in the test sets or on the cross validation set. And this puts us somewhat more similar to the supervised learning setting, right? Where we have label test set and our algorithm is making predictions on these labels and so we can evaluate it you know by seeing how often it gets these labels right. Of course these labels are will be very skewed because y equals zero, that is normal examples, usually be much more common than y equals 1 than anomalous examples. But, you know, this is much closer to the source of evaluation metrics we can use in supervised learning. So what’s a good evaluation metric to use. Well, because the data is very skewed, because y equals 0 is much more common, classification accuracy would not be a good the evaluation metrics. So, we talked about this in the earlier video. So, if you have a very skewed data set, then predicting y equals 0 all the time, will have very high classification accuracy. Instead, we should use evaluation metrics, like computing the fraction of true positives, false positives, false negatives, true negatives or compute the position of the v curve of this algorithm or do things like compute the f1 score, right, which is a single real number way of summarizing the position and the recall numbers. And so these would be ways to evaluate an anomaly detection algorithm on your cross validation set or on your test set. Finally, earlier in the anomaly detection algorithm, we also had this parameter epsilon, right? So, epsilon is this threshold that we would use to decide when to flag something as an anomaly. And so, if you have a cross validation set, another way to and to choose this parameter epsilon, would be to try a different, try many different values of epsilon, and then pick the value of epsilon that, let’s say, maximizes f1 score, or that otherwise does well on your cross validation set. And more generally, the way to reduce the training, testing, and cross validation sets, is that when we are trying to make decisions, like what features to include, or trying to, you know, tune the parameter epsilon, we would then continually evaluate the algorithm on the cross validation sets and make all those decisions like what features did you use, you know, how to set epsilon, use that, evaluate the algorithm on the cross validation set, and then when we’ve picked the set of features, when we’ve found the value of epsilon that we’re happy with, we can then take the final model and evaluate it, you know, do the final evaluation of the algorithm on the test sets. So, in this video, we talked about the process of how to evaluate an anomaly detection algorithm, and again, having being able to evaluate an algorithm, you know, with a single real number evaluation, with a number like an F1 score that often allows you to much more efficient use of your time when you are trying to develop an anomaly detection system. And we try to make these sorts of decisions. I have to chose epsilon, what features to include, and so on. In this video, we started to use a bit of labeled data in order to evaluate the anomaly detection algorithm and this takes us a little bit closer to a supervised learning setting. In the next video, I’m going to say a bit more about that. And in particular we’ll talk about when should you be using an anomaly detection algorithm and when should we be thinking about using supervised learning instead, and what are the differences between these two formalisms. summaryTo evaluate our learning algorithm, we take some labeled data, categorized into anomalous and non-anomalous examples ( y = 0 if normal, y = 1 if anomalous).Among that data, take a large proportion of good , non-anomalous data for the training set on which to train p(x).Then, take a smaller proportion of mixed anomalous and non-anomalous examples (you will usually have many more non-anomalous examples) for your cross-validation and test sets.For example, we may have a set where 0.2% of the data is anomalous. We take 60% of those examples, all of which are good (y=0) for the training set. We then take 20% of the examples for the cross-validation set (with 0.1% of the anomalous examples) and another 20% from the test set (with another 0.1% of the anomalous).In other words, we split the data 60/20/20 training/CV/test and then split the anomalous examples 50/50 between the CV and test sets.Algorithm evaluation:Fit model p(x) on training set $$\lbrace x^{(1)},\dots,x^{(m)} \rbrace$$On a cross validation/test example x, predict: If $p(x) &lt; ϵ$ ( anomaly ), then $y = 1$ If $p(x) ≥ ϵ$ ( normal ), then $y = 0$ Possible evaluation metrics (see “Machine Learning System Design” section): True positive, false positive, false negative, true negative. Precision/recall $F_1$ score Note that we use the cross-validation set to choose parameter $ϵ$ 02_anomaly-detection-vs-supervised-learningIn the last video we talked about the process of evaluating an anomaly detection algorithm. And there we started to use some label data with examples that we knew were either anomalous or not anomalous with Y equals one, or Y equals 0. And so, the question then arises of, and if we have the label data, that we have some examples and know the anomalies, and some of them will not be anomalies. Why don’t we just use a supervisor on half of them? So why don’t we just use logistic regression, or a neuro network to try to learn directly from our labeled data to predict whether Y equals one or Y equals 0. In this video, I’ll try to share with you some of the thinking and some guidelines for when you should probably use an anomaly detection algorithm, and whether it might be more fruitful instead of using a supervisor in the algorithm. This slide shows what are the settings under which you should maybe use anomaly detection versus when supervised learning might be more fruitful. If you have a problem with a very small number of positive examples, and remember the examples of y equals one are the anomaly examples. Then you might consider using an anomaly detection algorithm instead. So, having 0 to 20, it may be up to 50 positive examples, might be pretty typical. And usually we have such a small positive, set of positive examples, we’re going to save the positive examples just for the cross validation set in the test set. And in contrast, in a typical normal anomaly detection setting, we will often have a relatively large number of negative examples of the normal examples of normal aircraft engines. And we can then use this very large number of negative examples With which to fit the model p(x). And so there’s this idea that in many anomaly detection applications, you have very few positive examples and lots of negative examples. And when we’re doing the process of estimating p(x), affecting all those Gaussian parameters, we need only negative examples to do that. So if you have a lot negative data, we can still fit p(x) pretty well. In contrast, for supervised learning, more typically we would have a reasonably large number of both positive and negative examples. And so this is one way to look at your problem and decide if you should use an anomaly detection algorithm or a supervised. Here’s another way that people often think about anomaly detection. So for anomaly detection applications, often there are very different types of anomalies. So think about so many different ways for go wrong. There are so many things that could go wrong that could the aircraft engine. And so if that’s the case, and if you have a pretty small set of positive examples, then it can be hard for an algorithm, difficult for an algorithm to learn from your small set of positive examples what the anomalies look like. And in particular, you know future anomalies may look nothing like the ones you’ve seen so far. So maybe in your set of positive examples, maybe you’ve seen 5 or 10 or 20 different ways that an aircraft engine could go wrong. But maybe tomorrow, you need to detect a totally new set, a totally new type of anomaly. A totally new way for an aircraft engine to be broken, that you’ve just never seen before. And if that’s the case, it might be more promising to just model the negative examples with this sort of calcium model p of x rather than try to hard to model the positive examples. Because tomorrow’s anomaly may be nothing like the ones you’ve seen so far. In contrast, in some other problems, you have enough positive examples for an algorithm to get a sense of what the positive examples are like. In particular, if you think that future positive examples are likely to be similar to ones in the training set; then in that setting, it might be more reasonable to have a supervisor in the algorithm that looks at all of the positive examples, looks at all of the negative examples, and uses that to try to distinguish between positives and negatives. Hopefully, this gives you a sense of if you have a specific problem, should you think about using an anomaly detection algorithm, or a supervised learning algorithm. And a key difference really is that in anomaly detection, often we have such a small number of positive examples that it is not possible for a learning algorithm to learn that much from the positive examples. And so what we do instead is take a large set of negative examples and have it just learn a lot, learn p(x) from just the negative examples. Of the normal aircraft engines and we’ve reserved the small number of positive examples for evaluating our algorithms to use in the either the transvalidation set or the test set. And just as a side comment about this many different types of easier. In some earlier videos we talked about the email spam examples. In those examples, there are actually many different types of spam email, right? There’s spam email that’s trying to sell you things. Spam email trying to steal your passwords, this is called fishing emails and many different types of spam emails. But for the spam problem we usually have enough examples of spam email to see most of these different types of spam email because we have a large set of examples of spam. And that’s why we usually think of spam as a supervised learning setting even though there are many different types of. And so if we look at some applications of anomaly detection versus supervised learning we’ll find fraud detection. If you have many different types of ways for people to try to commit fraud and a relatively small number of fraudulent users on your website, then I use an anomaly detection algorithm. I should say, if you have, if you’re a very major online retailer and if you actually have had a lot of people commit fraud on your website, so you actually have a lot of examples of y=1, then sometimes fraud detection could actually shift over to the supervised learning algorithm. But, if you haven’t seen that many examples of users doing strange things on your website, then more frequently fraud detection is actually treated as an anomaly detection algorithm rather than a supervised learning algorithm. Other examples, we’ve talked about manufacturing already. Hopefully, you see more and more examples are not that many anomalies but if again for some manufacturing processes, if you manufacture in very large volumes and you see a lot of bad examples, maybe manufacturing can shift to the supervised learning column as well. But if you haven’t seen that many bad examples of so to do the anomaly detection monitoring machines in a data center [INAUDIBLE] similar source of apply. Whereas, you must have classification, weather prediction, and classifying cancers. If you have equal numbers of positive and negative examples. Your positive and your negative examples, then we would tend to treat all of these as supervisor problems. So hopefully, that gives you a sense of one of the properties of a learning problem that would cause you to treat it as an anomaly detection problem versus a supervisory problem. And for many other problems that are faced by various technology companies and so on, we actually are in the settings where we have very few or sometimes zero positive training examples. There’s just so many different types of anomalies that we’ve never seen them before. And for those sorts of problems, very often the algorithm that is used is an anomaly detection algorithm. summaryWhen do we use anomaly detection and when do we use supervised learning? Use anomaly detection when… We have a very small number of positive examples (y=1 … 0-20 examples is common) and a large number of negative (y=0) examples. We have many different “types” of anomalies and it is hard for any algorithm to learn from positive examples what the anomalies look like; future anomalies may look nothing like any of the anomalous examples we’ve seen so far. Use supervised learning when… We have a large number of both positive and negative examples. In other words, the training set is more evenly divided into classes. We have enough positive examples for the algorithm to get a sense of what new positives examples look like. The future positive examples are likely similar to the ones in the training set. 03_choosing-what-features-to-useBy now you’ve seen the anomaly detection algorithm and we’ve also talked about how to evaluate an anomaly detection algorithm. It turns out, that when you’re applying anomaly detection, one of the things that has a huge effect on how well it does, is _what features you use, and what features you choose, to give the anomaly detection algorithm._ So in this video, what I’d like to do is say a few words, give some suggestions and guidelines for how to go about designing or selecting features give to an anomaly detection algorithm. In our anomaly detection algorithm, one of the things we did was model the features using this sort of Gaussian distribution. With xi to mu i, sigma squared i, lets say. And so one thing that I often do would be to plot the data or the histogram of the data, to make sure that the data looks vaguely Gaussian before feeding it to my anomaly detection algorithm. And, it’ll usually work okay, even if your data isn’t Gaussian, but this is sort of a nice sanitary check to run. And by the way, in case your data looks non-Gaussian, the algorithms will often work just find. But, concretely if I plot the data like this, and if it looks like a histogram like this, and the way to plot a histogram is to use the HIST, or the HIST command in Octave, but it looks like this, this looks vaguely Gaussian, so if my features look like this, I would be pretty happy feeding into my algorithm. But if i were to plot a histogram of my data, and it were to look like this well, this doesn’t look at all like a bell shaped curve, this is a very asymmetric distribution, it has a peak way off to one side. If this is what my data looks like, what I’ll often do is play with different transformations of the data in order to make it look more Gaussian. And again the algorithm will usually work okay, even if you don’t. But if you use these transformations to make your data more gaussian, it might work a bit better. So given the data set that looks like this, what I might do is take a log transformation of the data and if i do that and re-plot the histogram, what I end up with in this particular example, is a histogram that looks like this. And this looks much more Gaussian, right? This looks much more like the classic bell shaped curve, that we can fit with some mean and variance paramater sigma. So what I mean by taking a log transform, is really that if I have some feature x1 and then the histogram of x1 looks like this then I might take my feature x1 and replace it with log of x1 and this is my new x1 that I’ll plot to the histogram over on the right, and this looks much more Guassian. Rather than just a log transform some other things you can do, might be, let’s say I have a different feature x2, maybe I’ll replace that will log x plus 1, or more generally with log x with x2 and some constant c and this constant could be something that I play with, to try to make it look as Gaussian as possible. Or for a different feature x3, maybe I’ll replace it with x3, I might take the square root. The square root is just x3 to the power of one half, right? And this one half is another example of a parameter I can play with. So, I might have x4 and maybe I might instead replace that with x4 to the power of something else, maybe to the power of 1/3. And these, all of these, this one, this exponent parameter, or the C parameter, all of these are examples of parameters that you can play with in order to make your data look a little bit more Gaussian. live demoSo, let me show you a live demo of how I actually go about playing with my data to make it look more Gaussian. So, I have already loaded in to octave here a set of features x I have a thousand examples loaded over there. So let’s pull up the histogram of my data. Use the hist x command. So there’s my histogram. By default, I think this uses 10 bins of histograms, but I want to see a more fine grid histogram. So we do hist to the x, 50, so, this plots it in 50 different bins. Okay, that looks better. Now, this doesn’t look very Gaussian, does it? So, lets start playing around with the data. Lets try a hist of x to the 0.5. So we take the square root of the data, and plot that histogram. And, okay, it looks a little bit more Gaussian, but not quite there, so let’s play at the 0.5 parameter. Let’s see. Set this to 0.2. Looks a little bit more Gaussian. Let’s reduce a little bit more 0.1. Yeah, that looks pretty good. I could actually just use 0.1. Well, let’s reduce it to 0.05. And, you know? Okay, this looks pretty Gaussian, so I can define a new feature which is x mu equals x to the 0.05, and now my new feature x Mu looks more Gaussian than my previous one and then I might instead use this new feature to feed into my anomaly detection algorithm. And of course, there is more than one way to do this. You could also have hist of log of x, that’s another example of a transformation you can use. And, you know, that also look pretty Gaussian. So, I can also define x mu equals log of x. and that would be another pretty good choice of a feature to use. So to summarize, if you plot a histogram with the data, and find that it looks pretty non-Gaussian, it’s worth playing around a little bit with different transformations like these, to see if you can make your data look a little bit more Gaussian, before you feed it to your learning algorithm, although even if you don’t, it might work okay. But I usually do take this step. Now, the second thing I want to talk about is, how do you come up with features for an anomaly detection algorithm. And the way I often do so, is via an error analysis procedure. So what I mean by that, is that this is really similar to the error analysis procedure that we have for supervised learning, where we would train a complete algorithm, and run the algorithm on a cross validation set, and look at the examples it gets wrong, and see if we can come up with extra features to help the algorithm do better on the examples that it got wrong in the cross-validation set. So lets try to reason through an example of this process. In anomaly detection, we are hoping that p of x will be large for the normal examples and it will be small for the anomalous examples. And so a pretty common problem would be if p of x is comparable, maybe both are large for both the normal and the anomalous examples. Lets look at a specific example of that. Let’s say that this is my unlabeled data. So, here I have just one feature, x1 and so I’m gonna fit a Gaussian to this. And maybe my Gaussian that I fit to my data looks like that. And now let’s say I have an anomalous example, and let’s say that my anomalous example takes on an x value of 2.5. So I plot my anomalous example there. And you know, it’s kind of buried in the middle of a bunch of normal examples, and so, just this anomalous example that I’ve drawn in green, it gets a pretty high probability, where it’s the height of the blue curve, and the algorithm fails to flag this as an anomalous example. Now, if this were maybe aircraft engine manufacturing or something, what I would do is, I would actually look at my training examples and look at what went wrong with that particular aircraft engine, and see, if looking at that example can inspire me to come up with a new feature x2, that helps to distinguish between this bad example, compared to the rest of my red examples, compared to all of my normal aircraft engines. And if I managed to do so, the hope would be then, that, if I can create a new feature, X2, so that when I re-plot my data, if I take all my normal examples of my training set, hopefully I find that all my training examples are these red crosses here. And hopefully, if I find that for my anomalous example, the feature x2 takes on the the unusual value. So for my green example here, this anomaly, right, my X1 value, is still 2.5. Then maybe my X2 value, hopefully it takes on a very large value like 3.5 over there, or a very small value. But now, if I model my data, I’ll find that my anomaly detection algorithm gives high probability to data in the central regions, slightly lower probability to that, sightly lower probability to that. An example that’s all the way out there, my algorithm will now give very low probability to. And so, the process of this is, really look at the mistakes that it is making. Look at the anomaly that the algorithm is failing to flag, and see if that inspires you to create some new feature. So find something unusual about that aircraft engine and use that to create a new feature, so that with this new feature it becomes easier to distinguish the anomalies from your good examples. And so that’s the process of error analysis and using that to create new features for anomaly detection. Finally, let me share with you my thinking on how I usually go about choosing features for anomaly detection. So, usually, the way I think about choosing features is I want to choose features that will take on either very, very large values, or very, very small values, for examples that I think might turn out to be anomalies. So let’s use our example again of monitoring the computers in a data center. And so you have lots of machines, maybe thousands, or tens of thousands of machines in a data center. And we want to know if one of the machines, one of our computers is acting up, so doing something strange. So here are examples of features you may choose, maybe memory used, number of disc accesses, CPU load, network traffic. But now, lets say that I suspect one of the failure cases, let’s say that in my data set I think that CPU load the network traffic tend to grow linearly with each other. Maybe I’m running a bunch of web servers, and so, here if one of my servers is serving a lot of users, I have a very high CPU load, and have a very high network traffic. But let’s say, I think, let’s say I have a suspicion, that one of the failure cases is if one of my computers has a job that gets stuck in some infinite loop. So if I think one of the failure cases, is one of my machines, one of my web servers–server code– gets stuck in some infinite loop, and so the CPU load grows, but the network traffic doesn’t because it’s just spinning it’s wheels and doing a lot of CPU work, you know, stuck in some infinite loop. In that case, to detect that type of anomaly, I might create a new feature, X5, which might be CPU load divided by network traffic. And so here X5 will take on a unusually large value if one of the machines has a very large CPU load but not that much network traffic and so this will be a feature that will help your anomaly detection capture, a certain type of anomaly. And you can also get creative and come up with other features as well. Like maybe I have a feature x6 thats CPU load squared divided by network traffic. And this would be another variant of a feature like x5 to try to capture anomalies where one of your machines has a very high CPU load, that maybe doesn’t have a commensurately large network traffic. And by creating features like these, you can start to capture anomalies that correspond to unusual combinations of values of the features. So in this video we talked about how to and take a feature, and maybe transform it a little bit, so that it becomes a bit more Gaussian, before feeding into an anomaly detection algorithm. And also the error analysis in this process of creating features to try to capture different types of anomalies. And with these sorts of guidelines hopefully that will help you to choose good features, to give to your anomaly detection algorithm, to help it capture all sorts of anomalies. summaryThe features will greatly affect how well your anomaly detection algorithm works.We can check that our features are gaussian by plotting a histogram of our data and checking for the bell-shaped curve.Some transforms we can try on an example feature x that does not have the bell-shaped curve are: $log(x)$ $log(x+1)$ $log(x+c)$ for some constant $\sqrt{x}$ $x^{1/3}$ We can play with each of these to try and achieve the gaussian shape in our data.There is an error analysis procedure for anomaly detection that is very similar to the one in supervised learning.Our goal is for $p(x)$ to be large for normal examples and small for anomalous examples.One common problem is when $p(x)$ is similar for both types of examples. In this case, you need to examine the anomalous examples that are giving high probability in detail and try to figure out new features that will better distinguish the data.In general, choose features that might take on unusually large or small values in the event of an anomaly. 03_multivariate-gaussian-distribution-optional01_multivariate-gaussian-distributionIn this and the next video, I’d like to tell you about one possible extension to the anomaly detection algorithm that we’ve developed so far. This extension uses something called the multivariate Gaussian distribution, and it has some advantages, and some disadvantages, and it can sometimes catch some anomalies that the earlier algorithm didn’t. To motivate this, let’s start with an example. Let’s say that so our unlabeled data looks like what I have plotted here. And I’m going to use the example of monitoring machines in the data center, monitoring computers in the data center. So my two features are x1 which is the CPU load and x2 which is maybe the memory use. So if I take my two features, x1 and x2, and I model them as Gaussians then here’s a plot of my X1 features, here’s a plot of my X2 features, and so if I fit a Gaussian to that, maybe I’ll get a Gaussian like this, so here’s P of X 1, which depends on the parameters mu 1, and sigma squared 1, and here’s my memory used, and, you know, maybe I’ll get a Gaussian that looks like this, and this is my P of X 2, which depends on mu 2 and sigma squared 2. And so this is how the anomaly detection algorithm models X1 and X2. Now let’s say that in the test sets I have an example that looks like this. The location of that green cross, so the value of X 1 is about 0.4, and the value of X 2 is about 1.5. Now, if you look at the data, it looks like, yeah, most of the data data lies in this region, and so that green cross is pretty far away from any of the data I’ve seen. It looks like that should be raised as an anomaly. So, in my data, in my, in the data of my good examples, it looks like, you know, the CPU load, and the memory use, they sort of grow linearly with each other. So if I have a machine using lots of CPU, you know memory use will also be high, whereas this example, this green example it looks like here, the CPU load is very low, but the memory use is very high, and I just have not seen that before in my training set. It looks like that should be an anomaly. But let’s see what the anomaly detection algorithm will do. Well, for the CPU load, it puts it at around there 0.5 and this reasonably high probability is not that far from other examples we’ve seen, maybe, whereas, for the memory use, this appointment, 0.5, whereas for the memory use, it’s about 1.5, which is there. Again, you know, it’s all to us, it’s not terribly Gaussian, but the value here and the value here is not that different from many other examples we’ve seen, and so P of X 1, will be pretty high, reasonably high. P of X 2 reasonably high. I mean, if you look at this plot right, this point here, it doesn’t look that bad, and if you look at this plot, you know across here, doesn’t look that bad. I mean, I have had examples with even greater memory used, or with even less CPU use, and so this example doesn’t look that anomalous. And so, an anomaly detection algorithm will fail to flag this point as an anomaly. And it turns out what our anomaly detection algorithm is doing is that it is not realizing that this blue ellipse shows the high probability region, is that, one of the thing is that, examples here, a high probability, and the examples, the next circle of from a lower probably, and examples here are even lower probability, and somehow, here are things that are, green cross there, it’s pretty high probability, and in particular, it tends to think that, you know, everything in this region, everything on the line that I’m circling over, has, you know, about equal probability, and it doesn’t realize that something out here actually has much lower probability than something over there. So, in order to fix this, we can, we’re going to develop a modified version of the anomaly detection algorithm, using something called the multivariate Gaussian distribution also called the multivariate normal distribution. So here’s what we’re going to do. We have features x which are in Rn and instead of P of X 1, P of X 2, separately, we’re going to model P of X, all in one go, so model P of X, you know, all at the same time. So the parameters of the multivariate Gaussian distribution are mu, which is a vector, and sigma, which is an n by n matrix, called a covariance matrix, and this is similar to the covariance matrix that we saw when we were working with the PCA, with the principal components analysis algorithm. For the second complete is, let me just write out the formula for the multivariate Gaussian distribution. So we say that probability of X, and this is parameterized by my parameters mu and sigma that the probability of x is equal to once again there’s absolutely no need to memorize this formula. You know, you can look it up whenever you need to use it, but this is what the probability of X looks like. Transverse, 2nd inverse, X minus mu. And this thing here, the absolute value of sigma, this thing here when you write this symbol, this is called the determent of sigma and this is a mathematical function of a matrix and you really don’t need to know what the determinant of a matrix is, but really all you need to know is that you can compute it in octave by using the octave command DET of sigma. Okay, and again, just be clear, alright? In this expression, these sigmas here, these are just n by n matrix. This is not a summation and you know, the sigma there is an n by n matrix. So that’s the formula for P of X, but it’s more interestingly, or more importantly, what does P of X actually looks like? Lets look at some examples of multivariate Gaussian distributions. $$p(x)=∏_{j=1}^{n}p(x_j;μ_j,σ^2_j)=∏_{j=1}^{n}\frac{1}{\sqrt{2π}σ_j}exp(-\frac{(x_j-μ_j)^2}{2σ_j^2}), μ=\frac{1}{m}\sum_{i=1}^{m}x^{(i)} \\ p(x)=\frac{1}{(2π)^{\frac{n}{2}} |Σ|^{\frac{1}{2}}}exp(-\frac{1}{2}(x-μ)^TΣ^{-1}(x-μ)), Σ=\frac{1}{m}(X-μ)^T(X-μ)$$ So let’s take a two dimensional example, say if I have N equals 2, I have two features, X1 and X2. Lets say I set MU to be equal to 0 and sigma to be equal to this matrix here. With 1s on the diagonals and 0s on the off-diagonals, this matrix is sometimes also called the identity matrix. In that case, p of x will look like this, and what I’m showing in this figure is, you know, for a specific value of X1 and for a specific value of X2, the height of this surface the value of p of x. And so with this setting the parameters p of x is highest when X1 and X2 equal zero 0, so that’s the peak of this Gaussian distribution, and the probability falls off with this sort of two dimensional Gaussian or this bell shaped two dimensional bell-shaped surface. Down below is the same thing but plotted using a contour plot instead, or using different colors, and so this heavy intense red in the middle, corresponds to the highest values, and then the values decrease with the yellow being slightly lower values the cyan being lower values and this deep blue being the lowest values so this is really the same figure but plotted viewed from the top instead, using colors instead. And so, with this distribution, you see that it faces most of the probability near 0,0 and then as you go out from 0,0 the probability of X1 and X2 goes down. Now lets try varying some of the parameters and see what happens. So let’s take sigma and change it so let’s say sigma shrinks a little bit. Sigma is a covariance matrix and so it measures the variance or the variability of the features X1 X2. So if the shrink sigma then what you get is what you get is that the width of this bump diminishes and the height also increases a bit, because the area under the surface is equal to 1. So the integral of the volume under the surface is equal to 1, because probability distribution must integrate to one. But, if you shrink the variance, it’s kinda like shrinking sigma squared, you end up with a narrower distribution, and one that’s a little bit taller. And so you see here also the concentric ellipsis has shrunk a little bit. Whereas in contrast if you were to increase sigma to 2 2 on the diagonals, so it is now two times the identity then you end up with a much wider and much flatter Gaussian. And so the width of this is much wider. This is hard to see but this is still a bell shaped bump, it’s just flattened down a lot, it has become much wider and so the variance or the variability of X1 and X2 just becomes wider. Here are a few more examples. Now lets try varying one of the elements of sigma at the time. Let’s say I send sigma to 0.6 there, and 1 over there. What this does, is this reduces the variance of the first feature, X 1, while keeping the variance of the second feature X 2, the same. And so with this setting of parameters, you can model things like that. X 1 has smaller variance, and X 2 has larger variance. Whereas if I do this, if I set this matrix to 2, 1 then you can also model examples where you know here we’ll say X1 can have take on a large range of values whereas X2 takes on a relatively narrower range of values. And that’s reflected in this figure as well, you know where, the distribution falls off more slowly as X 1 moves away from 0, and falls off very rapidly as X 2 moves away from 0. And similarly if we were to modify this element of the matrix instead, then similar to the previous slide, except that here where you know playing around here saying that X2 can take on a very small range of values and so here if this is 0.6, we notice now X2 tends to take on a much smaller range of values than the original example, whereas if we were to set sigma to be equal to 2 then that’s like saying X2 you know, has a much larger range of values. Now, one of the cool things about the multivariate Gaussian distribution is that you can also use it to model correlations between the data. That is we can use it to model the fact that X1 and X2 tend to be highly correlated with each other for example. So specifically if you start to change the off diagonal entries of this covariance matrix you can get a different type of Gaussian distribution. And so as I increase the off-diagonal entries from .5 to .8, what I get is this distribution that is more and more thinly peaked along this sort of x equals y line. And so here the contour says that x and y tend to grow together and the things that are with large probability are if either X1 is large and Y2 is large or X1 is small and Y2 is small. Or somewhere in between. And as this entry, 0.8 gets large, you get a Gaussian distribution, that’s sort of where all the probability lies on this sort of narrow region, where x is approximately equal to y. This is a very tall, thin distribution you know line mostly along this line central region where x is close to y. So this is if we set these entries to be positive entries. In contrast if we set these to negative values, as I decreases it to -.5 down to -.8, then what we get is a model where we put most of the probability in this sort of negative X one in the next 2 correlation region, and so, most of the probability now lies in this region, where X 1 is about equal to -X 2, rather than X 1 equals X 2. And so this captures a sort of negative correlation between x1 and x2. And so this is a hopefully this gives you a sense of the different distributions that the multivariate Gaussian distribution can capture. So follow up in varying, the covariance matrix sigma, the other thing you can do is also, vary the mean parameter mu, and so operationally, we have mu equal 0 0, and so the distribution was centered around X 1 equals 0, X2 equals 0, so the peak of the distribution is here, whereas, if we vary the values of mu, then that varies the peak of the distribution and so, if mu equals 0, 0.5, the peak is at, you know, X1 equals zero, and X2 equals 0.5, and so the peak or the center of this distribution has shifted, and if mu was 1.5 minus 0.5 then OK, and similarly the peak of the distribution has now shifted to a different location, corresponding to where, you know, X1 is 1.5 and X2 is -0.5, and so varying the mu parameter, just shifts around the center of this whole distribution. So, hopefully, looking at all these different pictures gives you a sense of the sort of probability distributions that the Multivariate Gaussian Distribution allows you to capture. And the key advantage of it is it allows you to capture, when you’d expect two different features to be positively correlated, or maybe negatively correlated. In the next video, we’ll take this multivariate Gaussian distribution and apply it to anomaly detection. summaryThe multivariate gaussian distribution is an extension of anomaly detection and may (or may not) catch more anomalies.Instead of modeling $p(x_1),p(x_2),\dots$ separately, we will model p(x) all in one go. Our parameters will be: $\mu \in \mathbb{R}^n$ and $\Sigma \in \mathbb{R}^{n \times n}$$$p(x;\mu,\Sigma) = \dfrac{1}{(2\pi)^{n\over 2} |\Sigma|^{1\over 2}} exp(-{1\over 2}(x-\mu)^T\Sigma^{-1}(x-\mu))$$The important effect is that we can model oblong gaussian contours, allowing us to better fit data that might not fit into the normal circular contours. Varying Σ changes the shape, width, and orientation of the contours. Changing μ will move the center of the distribution. Check also: The Multivariate Gaussian Distribution http://cs229.stanford.edu/section/gaussians.pdf Chuong B. Do, October 10, 2008. 02_anomaly-detection-using-the-multivariate-gaussian-distributionIn the last video we talked about the Multivariate Gaussian Distribution and saw some examples of the sorts of distributions you can model, as you vary the parameters, mu and sigma. In this video, let’s take those ideas, and apply them to develop a different anomaly detection algorithm. To recap the multivariate Gaussian distribution and the multivariate normal distribution has two parameters, mu and sigma. Where mu this an n dimensional vector and sigma, the covariance matrix, is an n by n matrix. And here’s the formula for the probability of X, as parameterized by mu and sigma, and as you vary mu and sigma, you can get a range of different distributions, like, you know, these are three examples of the ones that we saw in the previous video. So let’s talk about the parameter fitting or the parameter estimation problem. The question, as usual, is if I have a set of examples X1 through XM and here each of these examples is an n dimensional vector and I think my examples come from a multivariate Gaussian distribution. How do I try to estimate my parameters mu and sigma? Well the standard formulas for estimating them is you set mu to be just the average of your training examples. And you set sigma to be equal to this. And this is actually just like the sigma that we had written out, when we were using the PCA or the Principal Components Analysis algorithm. So you just plug in these two formulas and this would give you your estimated parameter mu and your estimated parameter sigma. So given the data set here is how you estimate mu and sigma. Let’s take this method and just plug it into an anomaly detection algorithm. So how do we put all of this together to develop an anomaly detection algorithm? Here ‘s what we do. First we take our training set, and we fit the model, we fit P of X, by, you know, setting mu and sigma as described on the previous slide. Next when you are given a new example X. So if you are given a test example, lets take an earlier example to have a new example out here. And that is my test example. Given the new example X, what we are going to do is compute P of X, using this formula for the multivariate Gaussian distribution. And then, if P of X is very small, then we flagged it as an anomaly, whereas, if P of X is greater than that parameter epsilon, then we don’t flag it as an anomaly. So it turns out, if we were to fit a multivariate Gaussian distribution to this data set, so just the red crosses, not the green example, you end up with a Gaussian distribution that places lots of probability in the central region, slightly less probability here, slightly less probability here, slightly less probability here, and very low probability at the point that is way out here. And so, if you apply the multivariate Gaussian distribution to this example, it will actually correctly flag that example. as an anomaly. Finally it’s worth saying a few words about what is the relationship between the multivariate Gaussian distribution model, and the original model, where we were modeling P of X as a product of this P of X1, P of X2, up to P of Xn. It turns out that you can prove mathematically, I’m not going to do the proof here, but you can prove mathematically that this relationship, between the multivariate Gaussian model and this original one. And in particular, it turns out that the original model corresponds to multivariate Gaussians, where the contours of the Gaussian are always axis aligned. So all three of these are examples of Gaussian distributions that you can fit using the original model. It turns out that that corresponds to multivariate Gaussian, where, you know, the ellipsis here, the contours of this distribution–it turns out that this model actually corresponds to a special case of a multivariate Gaussian distribution. And in particular, this special case is defined by constraining the distribution of p of x, the multivariate a Gaussian distribution of p of x, so that the contours of the probability density function, of the probability distribution function, are axis aligned. And so you can get a p of x with a multivariate Gaussian that looks like this, or like this, or like this. And you notice, that in all 3 of these examples, these ellipses, or these ovals that I’m drawing, have their axes aligned with the X1 X2 axes. And what we do not have, is a set of contours that are at an angle, right? And this corresponded to examples where sigma is equal to 1 1, 0.8, 0.8. Let’s say, with non-0 elements on the off diagonals. So, it turns out that it’s possible to show mathematically that this model actually is the same as a multivariate Gaussian distribution but with a constraint. And the constraint is that the covariance matrix sigma must have 0’s on the off diagonal elements. In particular, the covariance matrix sigma, this thing here, it would be sigma squared 1, sigma squared 2, down to sigma squared n, and then everything on the off diagonal entries, all of these elements above and below the diagonal of the matrix, all of those are going to be zero. And in fact if you take these values of sigma, sigma squared 1, sigma squared 2, down to sigma squared n, and plug them into here, and you know, plug them into this covariance matrix, then the two models are actually identical. That is, this new model, using a multivariate Gaussian distribution, corresponds exactly to the old model, if the covariance matrix sigma, has only 0 elements off the diagonals, and in pictures that corresponds to having Gaussian distributions, where the contours of this distribution function are axis aligned. So you aren’t allowed to model the correlations between the diffrent features. So in that sense the original model is actually a special case of this multivariate Gaussian model. So when would you use each of these two models? So when would you the original model and when would you use the multivariate Gaussian model? The original model is probably used somewhat more often, and whereas the multivariate Gaussian distribution is used somewhat less but it has the advantage of being able to capture correlations between features. So suppose you want to capture anomalies where you have different features say where features x1, x2 take on unusual combinations of values so in the earlier example, we had that example where the anomaly was with the CPU load and the memory use taking on unusual combinations of values, if you want to use the original model to capture that, then what you need to do is create an extra feature, such as X3 equals X1/X2, you know equals maybe the CPU load divided by the memory used, or something, and you need to create extra features if there’s unusual combinations of values where X1 and X2 take on an unusual combination of values even though X1 by itself and X2 by itself looks like it’s taking a perfectly normal value. But if you’re willing to spend the time to manually create an extra feature like this, then the original model will work fine. Whereas in contrast, the multivariate Gaussian model can automatically capture correlations between different features. But the original model has some other more significant advantages, too, and one huge advantage of the original model is that it is computationally cheaper, and another view on this is that is scales better to very large values of n and very large numbers of features, and so even if n were ten thousand, or even if n were equal to a hundred thousand, the original model will usually work just fine. Whereas in contrast for the multivariate Gaussian model notice here, for example, that we need to compute the inverse of the matrix sigma where sigma is an n by n matrix and so computing sigma if sigma is a hundred thousand by a hundred thousand matrix that is going to be very computationally expensive. And so the multivariate Gaussian model scales less well to large values of N. And finally for the original model, it turns out to work out ok even if you have a relatively small training set this is the small unlabeled examples that we use to model p of x of course, and this works fine, even if M is, you know, maybe 50, 100, works fine. Whereas for the multivariate Gaussian, it is sort of a mathematical property of the algorithm that you must have m greater than n, so that the number of examples is greater than the number of features you have. And there’s a mathematical property of the way we estimate the parameters that if this is not true, so if m is less than or equal to n, then this matrix isn’t even invertible, that is this matrix is singular, and so you can’t even use the multivariate Gaussian model unless you make some changes to it. But a typical rule of thumb that I use is, I will use the multivariate Gaussian model only if m is much greater than n, so this is sort of the narrow mathematical requirement, but in practice, I would use the multivariate Gaussian model, only if m were quite a bit bigger than n. So if m were greater than or equal to 10 times n, let’s say, might be a reasonable rule of thumb, and if it doesn’t satisfy this, then the multivariate Gaussian model has a lot of parameters, right, so this covariance matrix sigma is an n by n matrix, so it has, you know, roughly n squared parameters, because it’s a symmetric matrix, it’s actually closer to n squared over 2 parameters, but this is a lot of parameters, so you need make sure you have a fairly large value for m, make sure you have enough data to fit all these parameters. And m greater than or equal to 10 n would be a reasonable rule of thumb to make sure that you can estimate this covariance matrix sigma reasonably well. So in practice the original model shown on the left that is used more often. And if you suspect that you need to capture correlations between features what people will often do is just manually design extra features like these to capture specific unusual combinations of values. But in problems where you have a very large training set or m is very large and n is not too large, then the multivariate Gaussian model is well worth considering and may work better as well, and can save you from having to spend your time to manually create extra features in case the anomalies turn out to be captured by unusual combinations of values of the features. Finally I just want to briefly mention one somewhat technical property, but if you’re fitting multivariate Gaussian model, and if you find that the covariance matrix sigma is singular, or you find it’s non-invertible, they’re usually 2 cases for this. One is if it’s failing to satisfy this m greater than n condition, and the second case is if you have redundant features. So by redundant features, I mean, if you have 2 features that are the same. Somehow you accidentally made two copies of the feature, so your x1 is just equal to x2. Or if you have redundant features like maybe your features X3 is equal to feature X4, plus feature X5. Okay, so if you have highly redundant features like these, you know, where if X3 is equal to X4 plus X5, well X3 doesn’t contain any extra information, right? You just take these 2 other features, and add them together. And if you have this sort of redundant features, duplicated features, or this sort of features, than sigma may be non-invertible. And so there’s a debugging set– this should very rarely happen, so you probably won’t run into this, it is very unlikely that you have to worry about this– but in case you implement a multivariate Gaussian model you find that sigma is non-invertible. What I would do is first make sure that M is quite a bit bigger than N, and if it is then, the second thing I do, is just check for redundant features. And so if there are 2 features that are equal, just get rid of one of them, or if you have redundant if these , X3 equals X4 plus X5, just get rid of the redundant feature, and then it should work fine again. As an aside for those of you who are experts in linear algebra, by redundant features, what I mean is the formal term is features that are linearly dependent. But in practice what that really means is one of these problems tripping up the algorithm if you just make you features non-redundant., that should solve the problem of sigma being non-invertable. But once again the odds of your running into this at all are pretty low so chances are, you can just apply the multivariate Gaussian model, without having to worry about sigma being non-invertible, so long as m is greater than or equal to n. So that’s it for anomaly detection, with the multivariate Gaussian distribution. And if you apply this method you would be able to have an anomaly detection algorithm that automatically captures positive and negative correlations between your different features and flags an anomaly if it sees is unusual combination of the values of the features. summaryWhen doing anomaly detection with multivariate gaussian distribution, we compute $μ$ and $Σ$ normally. We then compute $p(x)$ using the new formula in the previous section and flag an anomaly if $p(x) &lt; ϵ$.The original model for p(x) corresponds to a multivariate Gaussian where the contours of $p(x;\mu,\Sigma)$ are axis-aligned.The multivariate Gaussian model can automatically capture correlations between different features of $x$.However, the original model maintains some advantages: it is computationally cheaper (no matrix to invert, which is costly for large number of features) and it performs well even with small training set size (in multivariate Gaussian model, it should be greater than the number of features for $Σ$ to be invertible]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[14_dimensionality-reduction note14]]></title>
    <url>%2F2018%2F01%2F14%2F14_dimensionality-reduction%2F</url>
    <content type="text"><![CDATA[NoteThis personal note is written after studying the opening course on the coursera website, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. 01_motivation01_motivation-i-data-compressionIn this video, I’d like to start talking about a second type of unsupervised learning problem called dimensionality reduction. There are a couple of different reasons why one might want to do dimensionality reduction. One is data compression, and as we’ll see later, a few videos later, data compression not only allows us to compress the data and have it therefore use up less computer memory or disk space, but it will also allow us to speed up our learning algorithms. But first, let’s start by talking about what is dimensionality reduction. As a motivating example, let’s say that we’ve collected a data set with many, many, many features, and I’ve plotted just two of them here. And let’s say that unknown to us two of the features were actually the length of something in centimeters, and a different feature, x2, is the length of the same thing in inches. So, this gives us a highly redundant representation and maybe instead of having two separate features x1 then x2, both of which basically measure the length, maybe what we want to do is reduce the data to one-dimensional and just have one number measuring this length. In case this example seems a bit contrived, this centimeter and inches example is actually not that unrealistic, and not that different from things that I see happening in industry. If you have hundreds or thousands of features, it is often this easy to lose track of exactly what features you have. And sometimes may have a few different engineering teams, maybe one engineering team gives you two hundred features, a second engineering team gives you another three hundred features, and a third engineering team gives you five hundred features so you have a thousand features all together, and it actually becomes hard to keep track of you know, exactly which features you got from which team, and it’s actually not that want to have highly redundant features like these. And so if the length in centimeters were rounded off to the nearest centimeter and lengthened inches was rounded off to the nearest inch. Then, that’s why these examples don’t lie perfectly on a straight line, because of, you know, round-off error to the nearest centimeter or the nearest inch. And if we can reduce the data to one dimension instead of two dimensions, that reduces the redundancy. For a different example, again maybe when there seems fairly less contrives. For may years I’ve been working with autonomous helicopter pilots. Or I’ve been working with pilots that fly helicopters. And so. If you were to measure–if you were to, you know, do a survey or do a test of these different pilots–you might have one feature, x1, which is maybe the skill of these helicopter pilots, and maybe “x2” could be the pilot enjoyment. That is, you know, how much they enjoy flying, and maybe these two features will be highly correlated. And what you really care about might be this sort of this sort of, this direction, a different feature that really measures pilot aptitude. And I’m making up the name aptitude of course, but again, if you highly correlated features, maybe you really want to reduce the dimension. So, let me say a little bit more about what it really means to reduce the dimension of the data from 2 dimensions down from 2D to 1 dimensional or to 1D. Let me color in these examples by using different colors. And in this case by reducing the dimension what I mean is that I would like to find maybe this line, this, you know, direction on which most of the data seems to lie and project all the data onto that line which is true, and by doing so, what I can do is just measure the position of each of the examples on that line. And what I can do is come up with a new feature, z1, and to specify the position on the line I need only one number, so it says z1 is a new feature that specifies the location of each of those points on this green line. And what this means, is that where as previously if i had an example x1, maybe this was my first example, x1. So in order to represent x1 originally x1. I needed a two dimensional number, or a two dimensional feature vector. Instead now I can represent z1. I could use just z1 to represent my first example, and that’s going to be a real number. And similarly x2 you know, if x2 is my second example there, then previously, whereas this required two numbers to represent if I instead compute the projection of that black cross onto the line. And now I only need one real number which is z2 to represent the location of this point z2 on the line. And so on through my M examples. So, just to summarize, if we allow ourselves to approximate the original data set by projecting all of my original examples onto this green line over here, then I need only one number, I need only real number to specify the position of a point on the line, and so what I can do is therefore use just one number to represent the location of each of my training examples after they’ve been projected onto that green line. So this is an approximation to the original training self because I have projected all of my training examples onto a line. But now, I need to keep around only one number for each of my examples. And so this halves the memory requirement, or a space requirement, or what have you, for how to store my data. And perhaps more interestingly, more importantly, what we’ll see later, in the later video as well is that this will allow us to make our learning algorithms run more quickly as well. And that is actually, perhaps, even the more interesting application of this data compression rather than reducing the memory or disk space requirement for storing the data. On the previous slide we showed an example of reducing data from 2D to 1D. On this slide, I’m going to show another example of reducing data from three dimensional 3D to two dimensional 2D. By the way, in the more typical example of dimensionality reduction we might have a thousand dimensional data or 1000D data that we might want to reduce to let’s say a hundred dimensional or 100D, but because of the limitations of what I can plot on the slide. I’m going to use examples of 3D to 2D, or 2D to 1D. So, let’s have a data set like that shown here. And so, I would have a set of examples x(i) which are points in r3. So, I have three dimension examples. I know it might be a little bit hard to see this on the slide, but I’ll show a 3D point cloud in a little bit. And it might be hard to see here, but all of this data maybe lies roughly on the plane, like so. And so what we can do with dimensionality reduction, is take all of this data and project the data down onto a two dimensional plane. So, here what I’ve done is, I’ve taken all the data and I’ve projected all of the data, so that it all lies on the plane. Now, finally, in order to specify the location of a point within a plane, we need two numbers, right? We need to, maybe, specify the location of a point along this axis, and then also specify it’s location along that axis. So, we need two numbers, maybe called z1 and z2 to specify the location of a point within a plane. And so, what that means, is that we can now represent each example, each training example, using two numbers that I’ve drawn here, z1, and z2. So, our data can be represented using vector z which are in r2. And these subscript, z subscript 1, z subscript 2, what I just mean by that is that my vectors here, z, you know, are two dimensional vectors, z1, z2. And so if I have some particular examples, z(i), or that’s the two dimensional vector, z(i)1, z(i)2. And on the previous slide when I was reducing data to one dimensional data then I had only z1, right? And that is what a z1 subscript 1 on the previous slide was, but here I have two dimensional data, so I have z1 and z2 as the two components of the data. Now, let me just make sure that these figures make sense. So let me just reshow these exact three figures again but with 3D plots. So the process we went through was that shown in the lab is the optimal data set, in the middle the data set projects on the 2D, and on the right the 2D data sets with z1 and z2 as the axis. Let’s look at them a little bit further. Here’s my original data set, shown on the left, and so I had started off with a 3D point cloud like so, where the axis are labeled x1, x2, x3, and so there’s a 3D point but most of the data, maybe roughly lies on some, you know, not too far from some 2D plain. So, what we can do is take this data and here’s my middle figure. I’m going to project it onto 2D. So, I’ve projected this data so that all of it now lies on this 2D surface. As you can see all the data lies on a plane, ‘cause we’ve projected everything onto a plane, and so what this means is that now I need only two numbers, z1 and z2, to represent the location of point on the plane. And so that’s the process that we can go through to reduce our data from three dimensional to two dimensional. So that’s dimensionality reduction and how we can use it to compress our data. And as we’ll see later this will allow us to make some of our learning algorithms run much later as well, but we’ll get to that only in a later video. summary We may want to reduce the dimension of our features if we have a lot of redundant data. To do this, we find two highly correlated features, plot them, and make a new line that seems to describe both features accurately. We place all the new features on this single line. Doing dimensionality reduction will reduce the total data we have to store in computer memory and will speed up our learning algorithm. Note: in dimensionality reduction, we are reducing our features rather than our number of examples. Our variable m will stay the same size; n, the number of features each example from $x^{(1)}$ to $x^{(m)}$ carries, will be reduced. 02_motivation-ii-visualizationIn the last video, we talked about dimensionality reduction for the purpose of compressing the data. In this video, I’d like to tell you about a second application of dimensionality reduction and that is to visualize the data. For a lot of machine learning applications, it really helps us to develop effective learning algorithms, if we can understand our data better. If there is some way of visualizing the data better, and so, dimensionality reduction offers us, often, another useful tool to do so. Let’s start with an example. Let’s say we’ve collected a large data set of many statistics and facts about different countries around the world. So, maybe the first feature, X1 is the country’s GDP, or the Gross Domestic Product, and X2 is a per capita, meaning the per person GDP, X3 human development index, life expectancy, X5, X6 and so on. And we may have a huge data set like this, where, you know, maybe 50 features for every country, and we have a huge set of countries. So is there something we can do to try to understand our data better? I’ve given this huge table of numbers. How do you visualize this data? If you have 50 features, it’s very difficult to plot 50-dimensional data. What is a good way to examine this data? Using dimensionality reduction, what we can do is, instead of having each country represented by this featured vector, xi, which is 50-dimensional, so instead of, say, having a country like Canada, instead of having 50 numbers to represent the features of Canada, let’s say we can come up with a different feature representation that is these z vectors, that is in R2. If that’s the case, if we can have just a pair of numbers, z1 and z2 that somehow, summarizes my 50 numbers, maybe what we can do [xx] is to plot these countries in R2 and use that to try to understand the space in [xx] of features of different countries [xx] the better and so, here, what you can do is reduce the data from 50 D, from 50 dimensions to 2D, so you can plot this as a 2 dimensional plot, and, when you do that, it turns out that, if you look at the output of the Dimensionality Reduction algorithms, It usually doesn’t astride a physical meaning to these new features you want $z_1,z_2$. It’s often up to us to figure out you know, roughly what these features means. But, And if you plot those features, here is what you might find. So, here, every country is represented by a point ZI, which is an R2 and so each of those. Dots, and this figure represents a country, and so, here’s Z1 and here’s Z2, and a couple of these. So, you might find, for example, That the horizontial axis the Z1 axis corresponds roughly to the overall country size, or the overall economic activity of a country. So the overall GDP, overall economic size of a country. Whereas the vertical axis in our data might correspond to the per person GDP. Or the per person well being, or the per person economic activity, and, you might find that, given these 50 features, you know, these are really the 2 main dimensions of the deviation, and so, out here you may have a country like the U.S.A., which is a relatively large GDP, you know, is a very large GDP and a relatively high per-person GDP as well. Whereas here you might have a country like Singapore, which actually has a very high per person GDP as well, but because Singapore is a much smaller country the overall economy size of Singapore is much smaller than the US. And, over here, you would have countries where individuals are unfortunately some are less well off, maybe shorter life expectancy, less health care, less economic maturity that’s why smaller countries, whereas a point like this will correspond to a country that has a fair, has a substantial amount of economic activity, but where individuals tend to be somewhat less well off. So you might find that the axes Z1 and Z2 can help you to most succinctly capture really what are the two main dimensions of the variations amongst different countries. Such as the overall economic activity of the country projected by the size of the country’s overall economy as well as the per-person individual well-being, measured by per-person GDP, per-person healthcare, and things like that. So that’s how you can use dimensionality reduction, in order to reduce data from 50 dimensions or whatever, down to two dimensions, or maybe down to three dimensions, so that you can plot it and understand your data better. In the next video, we’ll start to develop a specific algorithm, called PCA, or Principal Component Analysis, which will allow us to do this and also do the earlier application I talked about of compressing the data. summaryMotivation II: VisualizationIt is not easy to visualize data that is more than three dimensions. We can reduce the dimensions of our data to 3 or less in order to plot it.We need to find new features, $z_1,z_2$ (and perhaps $z_3$ ) that can effectively summarize all the other features.Example: hundreds of features related to a country’s economic system may all be combined into one feature that you call “Economic Activity.” 02_principal-component-analysis01_principal-component-analysis-problem-formulationFor the problem of dimensionality reduction, by far the most popular, by far the most commonly used algorithm is something called principle components analysis, or PCA. In this video, I’d like to start talking about the problem formulation for PCA. In other words, let’s try to formulate, precisely, exactly what we would like PCA to do. Let’s say we have a data set like this. So, this is a data set of examples x and R2 and let’s say I want to reduce the dimension of the data from two-dimensional to one-dimensional. In other words, I would like to find a line onto which to project the data. So what seems like a good line onto which to project the data, it’s a line like this, might be a pretty good choice. And the reason we think this might be a good choice is that if you look at where the projected versions of the point scales, so I take this point and project it down here. Get that, this point gets projected here, to here, to here, to here. What we find is that the distance between each point and the projected version is pretty small. That is, these blue line segments are pretty short. So what PCA does formally is it tries to find a lower dimensional surface, really a line in this case, onto which to project the data so that the sum of squares of these little blue line segments is minimized. The length of those blue line segments, that’s sometimes also called the projection error. And so what PCA does is it tries to find a surface onto which to project the data so as to minimize that. As an aside, before applying PCA, it’s standard practice to first perform mean normalization at feature scaling so that the features x1 and x2 should have zero mean, and should have comparable ranges of values. I’ve already done this for this example, but I’ll come back to this later and talk more about feature scaling and the normalization in the context of PCA later. But coming back to this example, in contrast to the red line that I just drew, here’s a different line onto which I could project my data, which is this magenta line. And, as we’ll see, this magenta line is a much worse direction onto which to project my data, right? So if I were to project my data onto the magenta line, we’d get a set of points like that. And the projection errors, that is these blue line segments, will be huge. So these points have to move a huge distance in order to get projected onto the magenta line. And so that’s why PCA, principal components analysis, will choose something like the red line rather than the magenta line down here. Let’s write out the PCA problem a little more formally. The goal of PCA, if we want to reduce data from two-dimensional to one-dimensional is, we’re going to try find a vector that is a vector u1, which is going to be an Rn, so that would be an R2 in this case. I’m gonna find the direction onto which to project the data, so it’s to minimize the projection error. So, in this example I’m hoping that PCA will find this vector, which l wanna call u(1), so that when I project the data onto the line that I define by extending out this vector, I end up with pretty small reconstruction errors. And that reference of data that looks like this. And by the way, I should mention that where the PCA gives me u(1) or -u(1), doesn’t matter. So if it gives me a positive vector in this direction, that’s fine. If it gives me the opposite vector facing in the opposite direction, so that would be like minus u(1). Let’s draw that in blue instead, right? But it gives a positive u(1) or negative u(1), it doesn’t matter because each of these vectors defines the same red line onto which I’m projecting my data. So this is a case of reducing data from two-dimensional to one-dimensional. In the more general case we have n-dimensional data and we’ll want to reduce it to k-dimensions. In that case we want to find not just a single vector onto which to project the data but we want to find k-dimensions onto which to project the data. So as to minimize this projection error. So here’s the example. If I have a 3D point cloud like this, then maybe what I want to do is find vectors. So find a pair of vectors. And I’m gonna call these vectors. Let’s draw these in red. I’m going to find a pair of vectors, sustained from the origin. Here’s u(1), and plane, or they define a 2D surface, right? Like this with a 2D surface onto which I am going to project my data. For those of you that are familiar with linear algebra, for this year they’re really experts in linear algebra, the formal definition of this is that we are going to find the set of vectors u(1), u(2), maybe up to u(k). And what we’re going to do is project the data onto the linear subspace spanned by this set of k vectors. But if you’re not familiar with linear algebra, just think of it as finding k directions instead of just one direction onto which to project the data. So finding a k-dimensional surface is really finding a 2D plane in this case, shown in this figure, where we can define the position of the points in a plane using k directions. And that’s why for PCA we want to find k vectors onto which to project the data. And so more formally in PCA, what we want to do is find this way to project the data so as to minimize the sort of projection distance, which is the distance between the points and the projections. And so in this 3D example too. Given a point we would take the point and project it onto this 2D surface. We are done with that. And so the projection error would be, the distance between the point and where it gets projected down to my 2D surface. And so what PCA does is I try to find the line, or a plane, or whatever, onto which to project the data, to try to minimize that square projection, that 90 degree or that orthogonal projection error. Finally, one question I sometimes get asked is how does PCA relate to linear regression? Because when explaining PCA, I sometimes end up drawing diagrams like these and that looks a little bit like linear regression. It turns out PCA is not linear regression and despite some cosmetic similarity, these are actually totally different algorithms. If we were doing linear regression, what we would do would be, on the left we would be trying to predict the value of some variable y given some info features x. And so linear regression, what we’re doing is we’re fitting a straight line so as to minimize the square error between point and this straight line. And so what we’re minimizing would be the squared magnitude of these blue lines. And notice that I’m drawing these blue lines vertically. That these blue lines are the vertical distance between the point and the value predicted by the hypothesis. Whereas in contrast, in PCA, what it does is it tries to minimize the magnitude of these blue lines, which are drawn at an angle. These are really the shortest orthogonal distances. The shortest distance between the point x and this red line. And this gives very different effects depending on the dataset. And more generally, when you’re doing linear regression, there is this distinguished variable y they we’re trying to predict. All that linear regression as well as taking all the values of x and try to use that to predict y. Whereas in PCA, there is no distinguish, or there is no special variable y that we’re trying to predict. And instead, we have a list of features, x1, x2, and so on, up to xn, and all of these features are treated equally,so no one of them is special. As one last example, if I have three-dimensional data and I want to reduce data from 3D to 2D, so maybe I wanna find two directions, u(1) and u(2), onto which to project my data. Then what I have is I have three features, x1, x2, x3, and all of these are treated alike. All of these are treated symmetrically and there’s no special variable y that I’m trying to predict. And so PCA is not a linear regression, and even though at some cosmetic level they might look related, these are actually very different algorithms. So hopefully you now understand what PCA is doing. It’s trying to find a lower dimensional surface onto which to project the data, so as to minimize this squared projection error. To minimize the square distance between each point and the location of where it gets projected. In the next video, we’ll start to talk about how to actually find this lower dimensional surface onto which to project the data. summaryPrincipal Component Analysis Problem Formulation The most popular dimensionality reduction algorithm is Principal Component Analysis (PCA)Problem formulationGiven two features, $x_1$ and $x_2$ we want to find a single line that effectively describes both features at once. We then map our old features onto this new line to get a new single feature.The same can be done with three features, where we map them to a plane.The goal of PCA is to reduce the average of all the distances of every feature to the projection line. This is the projection error.Reduce from 2d to 1d: find a direction (a vector $u^{(1)} \in \mathbb{R}^n$) onto which to project the data so as to minimize the projection error.The more general case is as follows:Reduce from n-dimension to k-dimension: Find k vectors $u^{(1)}, u^{(2)}, \dots, u^{(k)}$ onto which to project the data so as to minimize the projection error.If we are converting from 3d to 2d, we will project our data onto two directions (a plane), so k will be 2.PCA is not linear regressionIn linear regression, we are minimizing the squared error from every point to our predictor line. These are vertical distances.In PCA, we are minimizing the shortest distance , or shortest orthogonal distances, to our data points.More generally, in linear regression we are taking all our examples in x and applying the parameters in Θ to predict y.In PCA, we are taking a number of features $x_1, x_2, \dots, x_n$, and finding a closest common dataset among them. We aren’t trying to predict any result and we aren’t applying any theta weights to the features. 02_principal-component-analysis-algorithmIn this video I’d like to tell you about the principle components analysis algorithm. And by the end of this video you know to implement PCA for yourself. And use it reduce the dimension of your data. Before applying PCA,there is a data pre-processing step which you should always do. Given the trading sets of the examples is important to always perform mean normalization, and then depending on your data, maybe perform feature scaling as well. this is very similar to the mean normalization and feature scaling process that we have for supervised learning. In fact it’s exactly the same procedure except that we’re doing it now to our unlabeled data, X1 through Xm. So for mean normalization we first compute the mean of each feature and then we replace each feature, X, with X minus its mean, and so this makes each feature now have exactly zero mean The different features have very different scales. So for example, if x1 is the size of a house, and x2 is the number of bedrooms, to use our earlier example, we then also scale each feature to have a comparable range of values. And so, similar to what we had with supervised learning, we would take x, i substitute j, that’s the j feature and so we would subtract of the mean, now that’s what we have on top, and then divide by sj. Here, sj is some measure of the beta values of feature j. So, it could be the max minus min value, or more commonly, it is the standard deviation of feature j. Having done this sort of data pre-processing, here’s what the PCA algorithm does. We saw from the previous video that what PCA does is, it tries to find a lower dimensional sub-space onto which to project the data, so as to minimize the squared projection errors, sum of the squared projection errors, as the square of the length of those blue lines that and so what we wanted to do specifically is find a vector, u1, which specifies that direction or in the 2D case we want to find two vectors, u1 and u2, to define this surface onto which to project the data. So, just as a quick reminder of what reducing the dimension of the data means, for this example on the left we were given the examples xI, which are in r2. And what we like to do is find a set of numbers zI in r push to represent our data. So that’s what from reduction from 2D to 1D means. So specifically by projecting data onto this red line there. We need only one number to specify the position of the points on the line. So i’m going to call that number z or z1. Z here [xx] real number, so that’s like a one dimensional vector. So z1 just refers to the first component of this, you know, one by one matrix, or this one dimensional vector. And so we need only one number to specify the position of a point. So if this example here was my example X1, then maybe that gets mapped here. And if this example was X2 maybe that example gets mapped And so this point here will be Z1 and this point here will be Z2, and similarly we would have those other points for These, maybe X3, X4, X5 get mapped to Z1, Z2, Z3. So What PCA has to do is we need to come up with a way to compute two things. One is to compute these vectors, u1, and in this case u1 and u2. And the other is how do we compute these numbers, Z. So on the example on the left we’re reducing the data from 2D to 1D. In the example on the right, we would be reducing data from 3 dimensional as in r3, to zi, which is now two dimensional. So these z vectors would now be two dimensional. So it would be z1 z2 like so, and so we need to give away to compute these new representations, the z1 and z2 of the data as well. So how do you compute all of these quantities? It turns out that a mathematical derivation, also the mathematical proof, for what is the right value U1, U2, Z1, Z2, and so on. That mathematical proof is very complicated and beyond the scope of the course. But once you’ve done [xx] it turns out that the procedure to actually find the value of u1 that you want is not that hard, even though so that the mathematical proof that this value is the correct value is someone more involved and more than i want to get into. But let me just describe the specific procedure that you have to implement in order to compute all of these things, the vectors, u1, u2, the vector z. Here’s the procedure. Let’s say we want to reduce the data to n dimensions to k dimension What we’re going to do is first compute something called the covariance matrix, and the covariance matrix is commonly denoted by this Greek alphabet which is the capital Greek alphabet sigma. It’s a bit unfortunate that the Greek alphabet sigma looks exactly like the summation symbols. So this is the Greek alphabet Sigma is used to denote a matrix and this here is a summation symbol. So hopefully in these slides there won’t be ambiguity about which is Sigma Matrix, the matrix, which is a summation symbol, and hopefully it will be clear from context when I’m using each one. How do you compute this matrix let’s say we want to store it in an octave variable called sigma. What we need to do is compute something called the eigenvectors of the matrix sigma. And an octave, the way you do that is you use this command, u s v equals s v d of sigma. SVD, by the way, stands for singular value decomposition. This is a Much more advanced single value composition. It is much more advanced linear algebra than you actually need to know but now It turns out that when sigma is equal to matrix there is a few ways to compute these are high in vectors and If you are an expert in linear algebra and if you’ve heard of high in vectors before you may know that there is another octet function called I, which can also be used to compute the same thing. and It turns out that the SVD function and the I function it will give you the same vectors, although SVD is a little more numerically stable. So I tend to use SVD, although I have a few friends that use the I function to do this as wellbut when you apply this to a covariance matrix sigma it gives you the same thing. This is because the covariance matrix always satisfies a mathematical Property called symmetric positive definite You really don’t need to know what that means, but the SVD and I-functions are different functions but when they are applied to a covariance matrix which can be proved to always satisfy this mathematical property; they’ll always give you the same thing. Okay, that was probably much more linear algebra than you needed to know. In case none of that made sense, don’t worry about it. All you need to know is that this system command you should implement in Octave. And if you’re implementing this in a different language than Octave or MATLAB, what you should do is find the numerical linear algebra library that can compute the SVD or singular value decomposition, and there are many such libraries for probably all of the major programming languages. People can use that to compute the matrices u, s, and d of the covariance matrix sigma. So just to fill in some more details, this covariance matrix sigma will be an n by n matrix. And one way to see that is if you look at the definition this is an n by 1 vector and this here I transpose is 1 by N so the product of these two things is going to be an N by N matrix. 1xN transfers, 1xN, so there’s an NxN matrix and when we add up all of these you still have an NxN matrix. And what the SVD outputs three matrices, u, s, and v. The thing you really need out of the SVD is the u matrix. The u matrix will also be a NxN matrix. And if we look at the columns of the U matrix it turns out that the columns of the U matrix will be exactly those vectors, u1, u2 and so on. So u, will be matrix. And if we want to reduce the data from n dimensions down to k dimensions, then what we need to do is take the first k vectors. that gives us u1 up to uK which gives us the K direction onto which we want to project the data. The rest of the procedure from this SVD numerical linear algebra routine we get this matrix u. We’ll call these columns u1-uN. So, just to wrap up the description of the rest of the procedure, from the SVD numerical linear algebra routine we get these matrices u, s, and d. we’re going to use the first K columns of this matrix to get u1-uK. Now the other thing we need to is take my original data set, X which is an RN And find a lower dimensional representation Z, which is a R K for this data. So the way we’re going to do that is take the first K Columns of the U matrix. Construct this matrix. Stack up U1, U2 and so on up to U K in columns. It’s really basically taking, you know, this part of the matrix, the first K columns of this matrix. And so this is going to be an N by K matrix. I’m going to give this matrix a name. I’m going to call this matrix U, subscript “reduce,” sort of a reduced version of the U matrix maybe. I’m going to use it to reduce the dimension of my data. And the way I’m going to compute Z is going to let Z be equal to this U reduce matrix transpose times X. Or alternatively, you know, to write down what this transpose means. When I take this transpose of this U matrix, what I’m going to end up with is these vectors now in rows. I have U1 transpose down to UK transpose. Then take that times X, and that’s how I get my vector Z. Just to make sure that these dimensions make sense, this matrix here is going to be k by n and x here is going to be n by 1 and so the product here will be k by 1. And so z is k dimensional, is a k dimensional vector, which is exactly what we wanted. And of course these x’s here right, can be Examples in our training set can be examples in our cross validation set, can be examples in our test set, and for example if you know, I wanted to take training example i, I can write this as xi XI and that’s what will give me ZI over there. So, to summarize, here’s the PCA algorithm on one slide. After mean normalization, to ensure that every feature is zero mean and optional feature scaling whichYou really should do feature scaling if your features take on very different ranges of values. After this pre-processing we compute the carrier matrix Sigma like so by the way if your data is given as a matrix like hits if you have your data Given in rows like this. If you have a matrix X which is your time trading sets written in rows where x1 transpose down to x1 transpose, this covariance matrix sigma actually has a nice vectorizing implementation. You can implement in octave, you can even run sigma equals 1 over m, times x, which is this matrix up here, transpose times x and this simple expression, that’s the vectorize implementation of how to compute the matrix sigma. I’m not going to prove that today. This is the correct vectorization whether you want, you can either numerically test this on yourself by trying out an octave and making sure that both this and this implementations give the same answers or you Can try to prove it yourself mathematically. Either way but this is the correct vectorizing implementation, without compusingnext we can apply the SVD routine to get u, s, and d. And then we grab the first k columns of the u matrix you reduce and finally this defines how we go from a feature vector x to this reduce dimension representation z. And similar to k Means if you’re apply PCA, they way you’d apply this is with vectors X and RN. So, this is not done with X-0 1. So that was the PCA algorithm. One thing I didn’t do is give a mathematical proof that this There it actually give the projection of the data onto the K dimensional subspace onto the K dimensional surface that actually minimizes the square projection error Proof of that is beyond the scope of this course. Fortunately the PCA algorithm can be implemented in not too many lines of code. and if you implement this in octave or algorithm, you actually get a very effective dimensionality reduction algorithm. So, that was the PCA algorithm. One thing I didn’t do was give a mathematical proof that the U1 and U2 and so on and the Z and so on you get out of this procedure is really the choices that would minimize these squared projection error. Right, remember we said What PCA tries to do is try to find a surface or line onto which to project the data so as to minimize to square projection error. So I didn’t prove that this that, and the mathematical proof of that is beyond the scope of this course. But fortunately the PCA algorithm can be implemented in not too many lines of octave code. And if you implement this, this is actually what will work, or this will work well, and if you implement this algorithm, you get a very effective dimensionality reduction algorithm. That does do the right thing of minimizing this square projection error summaryBefore we can apply PCA, there is a data pre-processing step we must perform:Data preprocessingGiven training set: x(1),x(2),…,x(m)Preprocess (feature scaling/mean normalization):$\mu_j = \dfrac{1}{m}\sum^m_{i=1}x_j^{(i)}$Replace each $x_j^{(i)}$ with $x_j^{(i)} - \mu_j$If different features on different scales (e.g., $x_1$ = size of house, $x_2$ = number of bedrooms), scale features to have comparable range of values.Above, we first subtract the mean of each feature from the original feature. Then we scale all the features $x_j^{(i)} = \dfrac{x_j^{(i)} - \mu_j}{s_j}$We can define specifically what it means to reduce from 2d to 1d data as follows:$$\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$$The z values are all real numbers and are the projections of our features onto $u^{(1)}$.So, PCA has two tasks: figure out $u^{(1)},\dots,u^{(k)}$ and also to find $z_1, z_2, \dots, z_m$.The mathematical proof for the following procedure is complicated and beyond the scope of this course.1. Compute “covariance matrix”$$\Sigma = \dfrac{1}{m}\sum^m_{i=1}(x^{(i)})(x^{(i)})^T$$This can be vectorized in Octave as:1Sigma = (1/m) * X&apos; * X; We denote the covariance matrix with a capital sigma (which happens to be the same symbol for summation, confusingly—they represent entirely different things).Note that $x^{(i)}$ is an n×1 vector, $(x^{(i)})^T$ is an 1×n vector and X is a m×n matrix (row-wise stored examples). The product of those will be an n×n matrix, which are the dimensions of Σ.2. Compute “eigenvectors” of covariance matrix Σ[U,S,V] = svd(Sigma);svd() is the ‘singular value decomposition’, a built-in Octave function.What we actually want out of svd() is the ‘U’ matrix of the Sigma covariance matrix: $U \in \mathbb{R}^{n \times n}$. U contains $u^{(1)},\dots,u^{(n)}$, which is exactly what we want. 3. Take the first k columns of the U matrix and compute zWe’ll assign the first k columns of U to a variable called ‘Ureduce’. This will be an n×k matrix. We compute z with:$$z^{(i)} = Ureduce^T \cdot x^{(i)}$$$UreduceZ^T$ will have dimensions k×n while x(i) will have dimensions n×1. The product $Ureduce^T \cdot x^{(i)}$ will have dimensions k×1.To summarize, the whole algorithm in octave is roughly:1234Sigma = (1/m) * X&apos; * X; % compute the covariance matrix[U,S,V] = svd(Sigma); % compute our projected directionsUreduce = U(:,1:k); % take the first k directionsZ = X * Ureduce; % compute the projected data 03_applying-pca01_reconstruction-from-compressed-representation In some of the earlier videos, I was talking about PCA as a compression algorithm where you may have say, 1,000-dimensional data and compres it to 100-dimensional feature vector. Or have three-dimensional data and compress it to a two-dimensiona representation. So, if this is a compression algorithm, there should be a way to go bac from this compressed representation back to an approximation of you original high-dimensional data. So given zi, which may 100-dimensional, how do you go back to your original representation, xi which was maybe a 1000-dimensional. In this video, I’d like to describe how to do that. In the PCA algorithm, we may have an example like this, so maybe that’s my example x1, and maybe that’s my example x2. And what we do is we take these examples, and we project them onto this one dimensional surface. And then now we need to use a real number, say z1, to specify the location of these points after they’ve been projected onto this one dimensional surface. So, given the point z1, how can we go back to this original two dimensional space? In particular, given the point z, which is R, can we map this back to some approximate representation x and R2 of whatever the original value of the data was? So whereas z equals U reduce transpose x, if you want to go in the opposite direction, the equation for that is, we’re going to write x approx equals U reduce, times z. And again, just to check the dimensions, here U reduce is going to be an n by k dimensional vector, z is going to be k by one dimensional vector. So you multiply these out that’s going to be n by one, so x approx is going to be an n dimensional vector. And so the intent of PCA, that is if the square projection error is not too big, is that this x approx will be close to whatever was the original value of x that you have used to derive z in the first place. To show a picture of what this looks like, this is what it looks like. What you get back of this procedure are points that lie on the projection of that, onto the green line. So to take our early example, if we started off with this value of x1, and we got this value of z1, if you plug z1 through this formula to get x1 approx, then this point here, that would be x1 approx, which is going to be in R2. And similarly, if you do the same procedure, this would be x2 approx. And that’s a pretty decent approximation to the original data. So that’s how you go back from your low dimensional representation z, back to an uncompressed representation of the data. We get back an approximation to your original data x. And we also call this process reconstruction of the original data where we think of trying to reconstruct the original value of x from the compressed representation. So, given an unlabeled data set, you now know how to apply PCA and take your high dimensional features x and map that to this lower-dimensional representation z. And from this video hopefully you now also know how to take these low-representation z and map it back up to an approximation of your original high-dimensional data Now that you know how to implement and apply PCA, what I’d like to do next is talk about some of the mechanics of how to actually use PCA well. And in particular in the next video, I’d like to talk about how to choose k, which is how to choose the dimension of the reduced representation vector z. summaryIf we use PCA to compress our data, how can we uncompress our data, or go back to our original number of features?To go from 1-dimension back to 2d we do: $z \in \mathbb{R} \rightarrow x \in \mathbb{R}^2$.We can do this with the equation:$$x_{approx}^{(1)} = U_{reduce} \cdot z^{(1)}$$.Note that we can only get approximations of our original data.Note: It turns out that the U matrix has the special property that it is a Unitary Matrix. One of the special properties of a Unitary Matrix is:$U^{-1} = U^$ where the ““ means “conjugate transpose”.Since we are dealing with real numbers here, this is equivalent to:$U^{-1} = U^T$ So we could compute the inverse and use that, but it would be a waste of energy and compute cycles. 02_choosing-the-number-of-principal-componentsIn the PCA algorithm we take N dimensional features and reduce them to some K dimensional feature representation. This number K is a parameter of the PCA algorithm. This number K is also called the number of principle components or the number of principle components that we’ve retained. And in this video I’d like to give you some guidelines, tell you about how people tend to think about how to choose this parameter K for PCA. In order to choose k, that is to choose the number of principal components, here are a couple of useful concepts. What PCA tries to do is it tries to minimize the average squared projection error. So it tries to minimize this quantity, which I’m writing down, which is the difference between the original data X and the projected version, X-approx-i, which was defined last video, so it tries to minimize the squared distance between x and it’s projection onto that lower dimensional surface. So that’s the average square projection error. Also let me define the total variation in the data to be the average length squared of these examples Xi so the total variation in the data is the average of my training sets of the length of each of my training examples. And this one says, “On average, how far are my training examples from the vector, from just being all zeros?” How far is, how far on average are my training examples from the origin? When we’re trying to choose k, a pretty common rule of thumb for choosing k is to choose the smaller values so that the ratio between these is less than 0.01. So in other words, a pretty common way to think about how we choose k is we want the average squared projection error. That is the average distance between x and it’s projections divided by the total variation of the data. That is how much the data varies. We want this ratio to be less than, let’s say, 0.01. Or to be less than 1%, which is another way of thinking about it. And the way most people think about choosing K is rather than choosing K directly the way most people talk about it is as what this number is, whether it is 0.01 or some other number. And if it is 0.01, another way to say this to use the language of PCA is that 99% of the variance is retained. I don’t really want to, don’t worry about what this phrase really means technically but this phrase “99% of variance is retained” just means that this quantity on the left is less than 0.01. And so, if you are using PCA and if you want to tell someone, you know, how many principle components you’ve retained it would be more common to say well, I chose k so that 99% of the variance was retained. And that’s kind of a useful thing to know, it means that you know, the average squared projection error divided by the total variation that was at most 1%. That’s kind of an insightful thing to think about, whereas if you tell someone that, “Well I had to 100 principle components” or “k was equal to 100 in a thousand dimensional data” it’s a little hard for people to interpret that. So this number 0.01 is what people often use. Other common values is 0.05, and so this would be 5%, and if you do that then you go and say well 95% of the variance is retained and, you know other numbers maybe 90% of the variance is retained, maybe as low as 85%. So 90% would correspond to say 0.10, kinda 10%. And so range of values from, you know, 90, 95, 99, maybe as low as 85% of the variables contained would be a fairly typical range in values. Maybe 95 to 99 is really the most common range of values that people use. For many data sets you’d be surprised, in order to retain 99% of the variance, you can often reduce the dimension of the data significantly and still retain most of the variance. Because for most real life data says many features are just highly correlated, and so it turns out to be possible to compress the data a lot and still retain you know 99% of the variance or 95% of the variance. So how do you implement this? Well, here’s one algorithm that you might use. You may start off, if you want to choose the value of k, we might start off with k equals 1. And then we run through PCA. You know, so we compute, you reduce, compute z1, z2, up to zm. Compute all of those x1 approx and so on up to xm approx and then we check if 99% of the variance is retained. Then we’re good and we use k equals 1. But if it isn’t then what we’ll do we’ll next try K equals 2. And then we’ll again run through this entire procedure and check, you know is this expression satisfied. Is this less than 0.01. And if not then we do this again. Let’s try k equals 3, then try k equals 4, and so on until maybe we get up to k equals 17 and we find 99% of the data have is retained and then we use k equals 17, right? That is one way to choose the smallest value of k, so that and 99% of the variance is retained. But as you can imagine, this procedure seems horribly inefficient we’re trying k equals one, k equals two, we’re doing all these calculations. Fortunately when you implement PCA it actually, in this step, it actually gives us a quantity that makes it much easier to compute these things as well. Specifically when you’re calling SVD to get these matrices u, s, and d, when you’re calling usvd on the covariance matrix sigma, it also gives us back this matrix S and what S is, is going to be a square matrix an N by N matrix in fact, that is diagonal. So is diagonal entries s one one, s two two, s three three down to s n n are going to be the only non-zero elements of this matrix, and everything off the diagonals is going to be zero. Okay? So those big O’s that I’m drawing, by that what I mean is that everything off the diagonal of this matrix all of those entries there are going to be zeros. And so, what is possible to show, and I won’t prove this here, and it turns out that for a given value of k, this quantity over here can be computed much more simply. And that quantity can be computed as one minus sum from i equals 1 through K of Sii divided by sum from I equals 1 through N of Sii. So just to say that it words, or just to take another view of how to explain that, if K equals 3 let’s say. What we’re going to do to compute the numerator is sum from one– I equals 1 through 3 of of Sii, so just compute the sum of these first three elements. So that’s the numerator. And then for the denominator, well that’s the sum of all of these diagonal entries. And one minus the ratio of that, that gives me this quantity over here, that I’ve circled in blue. And so, what we can do is just test if this is less than or equal to 0.01. Or equivalently, we can test if the sum from i equals 1 through k, s-i-i divided by sum from i equals 1 through n, s-i-i if this is greater than or equal to 4.99, if you want to be sure that 99% of the variance is retained. And so what you can do is just slowly increase k, set k equals one, set k equals two, set k equals three and so on, and just test this quantity to see what is the smallest value of k that ensures that 99% of the variance is retained. And if you do this, then you need to call the SVD function only once. Because that gives you the S matrix and once you have the S matrix, you can then just keep on doing this calculation by increasing the value of K in the numerator and so you don’t need keep to calling SVD over and over again to test out the different values of K. So this procedure is much more efficient, and this can allow you to select the value of K without needing to run PCA from scratch over and over. You just run SVD once, this gives you all of these diagonal numbers, all of these numbers S11, S22 down to SNN, and then you can just you know, vary K in this expression to find the smallest value of K, so that 99% of the variance is retained. So to summarize, the way that I often use, the way that I often choose K when I am using PCA for compression is I would call SVD once in the covariance matrix, and then I would use this formula and pick the smallest value of K for which this expression is satisfied. And by the way, even if you were to pick some different value of K, even if you were to pick the value of K manually, you know maybe you have a thousand dimensional data and I just want to choose K equals one hundred. Then, if you want to explain to others what you just did, a good way to explain the performance of your implementation of PCA to them, is actually to take this quantity and compute what this is, and that will tell you what was the percentage of variance retained. And if you report that number, then, you know, people that are familiar with PCA, and people can use this to get a good understanding of how well your hundred dimensional representation is approximating your original data set, because there’s 99% of variance retained. That’s really a measure of your square of construction error, that ratio being 0.01, just gives people a good intuitive sense of whether your implementation of PCA is finding a good approximation of your original data set. So hopefully, that gives you an efficient procedure for choosing the number K. For choosing what dimension to reduce your data to, and if you apply PCA to very high dimensional data sets, you know, to like a thousand dimensional data, very often, just because data sets tend to have highly correlated features, this is just a property of most of the data sets you see, you often find that PCA will be able to retain ninety nine percent of the variance or say, ninety five ninety nine, some high fraction of the variance, even while compressing the data by a very large factor. summaryHow do we choose k, also called _the number of principal components_ ? Recall that k is the dimension we are reducing to.One way to choose k is by using the following formula: Given the average squared projection error: $\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2$ Also given the total variation in the data: $\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2$ Choose k to be the smallest value such that: $\dfrac{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)} - x_{approx}^{(i)}||^2}{\dfrac{1}{m}\sum^m_{i=1}||x^{(i)}||^2} \leq 0.01$ In other words, the squared projection error divided by the total variation should be less than one percent, so that 99% of the variance is retained .Algorithm for choosing k Try PCA with k=1,2,… Compute $U_{reduce}, z, x$ Check the formula given above that 99% of the variance is retained. If not, go to step one and increase k. This procedure would actually be horribly inefficient. In Octave, we will call svd: 1[U,S,V] = svd(Sigma) Which gives us a matrix S. We can actually check for 99% of retained variance using the S matrix as follows: $$\dfrac{\sum_{i=1}^kS_{ii}}{\sum_{i=1}^nS_{ii}} \geq 0.99$$ 03_advice-for-applying-pcaIn an earlier video, I had said that PCA can be sometimes used to speed up the running time of a learning algorithm. In this video, I’d like to explain how to actually do that, and also say some, just try to give some advice about how to apply PCA. Here’s how you can use PCA to speed up a learning algorithm, and this supervised learning algorithm speed up is actually the most common use that I personally make of PCA. Let’s say you have a supervised learning problem, note this is a supervised learning problem with inputs X and labels Y, and let’s say that your examples xi are very high dimensional. So, lets say that your examples, xi are 10,000 dimensional feature vectors. One example of that, would be, if you were doing some computer vision problem, where you have a 100x100 images, and so if you have 100x100, that’s 10000 pixels, and so if xi are, you know, feature vectors that contain your 10000 pixel intensity values, then you have 10000 dimensional feature vectors. So with very high-dimensional feature vectors like this, running a learning algorithm can be slow, right? Just, if you feed 10,000 dimensional feature vectors into logistic regression, or a new network, or support vector machine or what have you, just because that’s a lot of data, that’s 10,000 numbers, it can make your learning algorithm run more slowly. Fortunately with PCA we’ll be able to reduce the dimension of this data and so make our algorithms run more efficiently. Here’s how you do that. We are going first check our labeled training set and extract just the inputs, we’re just going to extract the X’s and temporarily put aside the Y’s. So this will now give us an unlabelled training set x1 through xm which are maybe there’s a ten thousand dimensional data, ten thousand dimensional examples we have. So just extract the input vectors x1 through xm. Then we’re going to apply PCA and this will give me a reduced dimension representation of the data, so instead of 10,000 dimensional feature vectors I now have maybe one thousand dimensional feature vectors. So that’s like a 10x savings. So this gives me, if you will, a new training set. So whereas previously I might have had an example x1, y1, my first training input, is now represented by z1. And so we’ll have a new sort of training example, which is Z1 paired with y1. And similarly Z2, Y2, and so on, up to ZM, YM. Because my training examples are now represented with this much lower dimensional representation Z1, Z2, up to ZM. Finally, I can take this reduced dimension training set and feed it to a learning algorithm maybe a neural network, maybe logistic regression, and I can learn the hypothesis H, that takes this input, these low-dimensional representations Z and tries to make predictions. So if I were using logistic regression for example, I would train a hypothesis that outputs, you know, one over one plus E to the negative-theta transpose Z, that takes this input to one these z vectors, and tries to make a prediction. And finally, if you have a new example, maybe a new test example X. What you do is you would take your test example x, map it through the same mapping that was found by PCA to get you your corresponding z. And that z then gets fed to this hypothesis, and this hypothesis then makes a prediction on your input x. One final note, what PCA does is it defines a mapping from x to z and this mapping from x to z should be defined by running PCA only on the training sets. And in particular, this mapping that PCA is learning, right, this mapping, what that does is it computes the set of parameters. That’s the feature scaling and mean normalization. And there’s also computing this matrix U reduced. But all of these things that U reduce, that’s like a parameter that is learned by PCA and we should be fitting our parameters only to our training sets and not to our cross validation or test sets and so these things the U reduced so on, that should be obtained by running PCA only on your training set. And then having found U reduced, or having found the parameters for feature scaling where the mean normalization and scaling the scale that you divide the features by to get them on to comparable scales. Having found all those parameters on the training set, you can then apply the same mapping to other examples that may be In your cross-validation sets or in your test sets, OK? Just to summarize, when you’re running PCA, run your PCA only on the training set portion of the data not the cross-validation set or the test set portion of your data. And that defines the mapping from x to z and you can then apply that mapping to your cross-validation set and your test set and by the way in this example I talked about reducing the data from ten thousand dimensional to one thousand dimensional, this is actually not that unrealistic.For many problems we actually reduce the dimensional data. You know by 5x maybe by 10x and still retain most of the variance and we can do this barely effecting the performance, in terms of classification accuracy, let’s say, barely affecting the classification accuracy of the learning algorithm. And by working with lower dimensional data our learning algorithm can often run much much faster. To summarize, we’ve so far talked about the following applications of PCA. First is the compression application where we might do so to reduce the memory or the disk space needed to store data and we just talked about how to use this to speed up a learning algorithm. In these applications, in order to choose K, often we’ll do so according to, figuring out what is the percentage of variance retained, and so for this learning algorithm, speed up application often will retain 99% of the variance. That would be a very typical choice for how to choose k. So that’s how you choose k for these compression applications. Whereas for visualization applications while usually we know how to plot only two dimensional data or three dimensional data, and so for visualization applications, we’ll usually choose k equals 2 or k equals 3, because we can plot only 2D and 3D data sets. So that summarizes the main applications of PCA, as well as how to choose the value of k for these different applications. I should mention that there is often one frequent misuse of PCA and you sometimes hear about others doing this hopefully not too often. I just want to mention this so that you know not to do it. And there is one bad use of PCA, which iss to try to use it to prevent over-fitting. Here’s the reasoning. This is not a great way to use PCA, but here’s the reasoning behind this method, which is,you know if we have Xi, then maybe we’ll have n features, but if we compress the data, and use Zi instead and that reduces the number of features to k, which could be much lower dimensional. And so if we have a much smaller number of features, if k is 1,000 and n is 10,000, then if we have only 1,000 dimensional data, maybe we’re less likely to over-fit than if we were using 10,000-dimensional data with like a thousand features. So some people think of PCA as a way to prevent over-fitting. But just to emphasize this is a bad application of PCA and I do not recommend doing this. And it’s not that this method works badly. If you want to use this method to reduce the dimensional data, to try to prevent over-fitting, it might actually work OK. But this just is not a good way to address over-fitting and instead, if you’re worried about over-fitting, there is a much better way to address it, to use regularization instead of using PCA to reduce the dimension of the data. And the reason is, if you think about how PCA works, it does not use the labels y. You are just looking at your inputs xi, and you’re using that to find a lower-dimensional approximation to your data. So what PCA does, is it throws away some information. It throws away or reduces the dimension of your data without knowing what the values of y is, so this is probably okay using PCA this way is probably okay if, say 99 percent of the variance is retained, if you’re keeping most of the variance, but it might also throw away some valuable information. And it turns out that if you’re retaining 99% of the variance or 95% of the variance or whatever, it turns out that just using regularization will often give you at least as good a method for preventing over-fitting and regularization will often just work better, because when you are applying linear regression or logistic regression or some other method with regularization, well, this minimization problem actually knows what the values of y are, and so is less likely to throw away some valuable information, whereas PCA doesn’t make use of the labels and is more likely to throw away valuable information. So, to summarize, it is a good use of PCA, if your main motivation to speed up your learning algorithm, but using PCA to prevent over-fitting, that is not a good use of PCA, and using regularization instead is really what many people would recommend doing instead. Finally, one last misuse of PCA. And so I should say PCA is a very useful algorithm, I often use it for the compression on the visualization purposes. But, what I sometimes see, is also people sometimes use PCA where it shouldn’t be. So, here’s a pretty common thing that I see, which is if someone is designing a machine-learning system, they may write down the plan like this: let’s design a learning system. Get a training set and then, you know, what I’m going to do is run PCA, then train logistic regression and then test on my test data. So often at the very start of a project, someone will just write out a project plan than says lets do these four steps with PCA inside. Before writing down a project plan the incorporates PCA like this, one very good question to ask is, well, what if we were to just do the whole without using PCA. And often people do not consider this step before coming up with a complicated project plan and implementing PCA and so on. And sometime, and so specifically, what I often advise people is, before you implement PCA, I would first suggest that, you know, do whatever it is, take whatever it is you want to do and first consider doing it with your original raw data xi, and only if that doesn’t do what you want, then implement PCA before using Zi. So, before using PCA you know, instead of reducing the dimension of the data, I would consider well, let’s ditch this PCA step, and I would consider, let’s just train my learning algorithm on my original data. Let’s just use my original raw inputs xi, and I would recommend, instead of putting PCA into the algorithm, just try doing whatever it is you’re doing with the xi first. And only if you have a reason to believe that doesn’t work, so that only if your learning algorithm ends up running too slowly, or only if the memory requirement or the disk space requirement is too large, so you want to compress your representation, but if only using the xi doesn’t work, only if you have evidence or strong reason to believe that using the xi won’t work, then implement PCA and consider using the compressed representation. Because what I do see, is sometimes people start off with a project plan that incorporates PCA inside, and sometimes they, whatever they’re doing will work just fine, even without using PCA instead. So, just consider that as an alternative as well, before you go to spend a lot of time to get PCA in, figure out what k is and so on. So, that’s it for PCA. Despite these last sets of comments, PCA is an incredibly useful algorithm, when you use it for the appropriate applications and I’ve actually used PCA pretty often and for me, I use it mostly to speed up the running time of my learning algorithms. But I think, just as common an application of PCA, is to use it to compress data, to reduce the memory or disk space requirements, or to use it to visualize data. And PCA is one of the most commonly used and one of the most powerful unsupervised learning algorithms. And with what you’ve learned in these videos, I think hopefully you’ll be able to implement PCA and use them through all of these purposes as well. summaryThe most common use of PCA is to speed up supervised learning.Given a training set with a large number of features (e.g. $x^{(1)},\dots,x^{(m)} \in \mathbb{R}^{10000}$ ) we can use PCA to reduce the number of features in each example of the training set (e.g. $z^{(1)},\dots,z^{(m)} \in \mathbb{R}^{1000}$).Note that we should define the PCA reduction from $x^{(i)}$ to $z^{(i)}$ only on the training set and not on the cross-validation or test sets. You can apply the mapping z(i) to your cross-validation and test sets after it is defined on the training set.Applications Compressions Reduce space of dataSpeed up algorithm Visualization of data Choose k = 2 or k = 3Bad use of PC A: trying to prevent overfitting. We might think that reducing the features with PCA would be an effective way to address overfitting. It might work, but is not recommended because it does not consider the values of our results y. Using just regularization will be at least as effective.Don’t assume you need to do PCA. Try your full machine learning algorithm without PCA first. Then use PCA if you find that you need it.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[13_unsupervised-learning note13]]></title>
    <url>%2F2018%2F01%2F13%2F13_unsupervised-learning%2F</url>
    <content type="text"><![CDATA[NoteThis personal note is written after studying the opening course on the coursera website, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. 01_clustering01_unsupervised-learning-introductionIn this video, I’d like to start to talk about clustering. This will be exciting, because this is our first unsupervised learning algorithm, where we learn from unlabeled data instead from labelled data. So, what is unsupervised learning? I briefly talked about unsupervised learning at the beginning of the class but it’s useful to contrast it with supervised learning. So, here’s a typical supervised learning problem where we’re given a labeled training set and the goal is to find the decision boundary that separates the positive label examples and the negative label examples. So, the supervised learning problem in this case is given a set of labels to fit a hypothesis to it. In contrast, in the unsupervised learning problem we’re given data that does not have any labels associated with it. So, we’re given data that looks like this. Here’s a set of points add in no labels, and so, our training set is written just x1, x2, and so on up to xm and we don’t get any labels y. And that’s why the points plotted up on the figure don’t have any labels with them. So, in unsupervised learning what we do is we give this sort of unlabeled training set to an algorithm and we just ask the algorithm find some structure in the data for us. Given this data set one type of structure we might have an algorithm find is that it looks like this data set has points grouped into two separate clusters and so an algorithm that finds clusters like the ones I’ve just circled is called a clustering algorithm. And this would be our first type of unsupervised learning, although there will be other types of unsupervised learning algorithms that we’ll talk about later that finds other types of structure or other types of patterns in the data other than clusters. We’ll talk about this after we’ve talked about clustering. So, what is clustering good for? Early in this class I already mentioned a few applications. One is market segmentation where you may have a database of customers and want to group them into different marker segments so you can sell to them separately or serve your different market segments better. Social network analysis. There are actually groups have done this things like looking at a group of people’s social networks. So, things like Facebook, Google+, or maybe information about who other people that you email the most frequently and who are the people that they email the most frequently and to find coherence in groups of people. So, this would be another maybe clustering algorithm where you know want to find who are the coherent groups of friends in the social network? Here’s something that one of my friends actually worked on which is, use clustering to organize computer clusters or to organize data centers better. Because if you know which computers in the data center in the cluster tend to work together, you can use that to reorganize your resources and how you layout the network and how you design your data center communications. And lastly, something that actually another friend worked on using clustering algorithms to understand galaxy formation and using that to understand astronomical data. So, that’s clustering which is our first example of an unsupervised learning algorithm. In the next video we’ll start to talk about a specific clustering algorithm. summaryUnsupervised learning is contrasted from supervised learning because it uses an unlabeled training set rather than a labeled one.In other words, we don’t have the vector y of expected results, we only have a dataset of features where we can find structure.Clustering is good for: Market segmentation Social network analysis Organizing computer clusters Astronomical data analysis 02_k-means-algorithmIn the clustering problem we are given an unlabeled data set and we would like to have an algorithm automatically group the data into coherent subsets or into coherent clusters for us. The K Means algorithm is by far the most popular, by far the most widely used clustering algorithm, and in this video I would like to tell you what the K Means Algorithm is and how it works. The K means clustering algorithm is best illustrated in pictures. Let’s say I want to take an unlabeled data set like the one shown here, and I want to group the data into two clusters. If I run the K Means clustering algorithm, here is what I’m going to do. The first step is to randomly initialize two points, called the cluster centroids. So, these two crosses here, these are called the Cluster Centroids and I have two of them because I want to group my data into two clusters. K Means is an iterative algorithm and it does two things. First is a cluster assignment step, and second is a move centroid step. So, let me tell you what those things mean. The first of the two steps in the loop of K means, is this cluster assignment step. What that means is that, it’s going through each of the examples, each of these green dots shown here and depending on whether it’s closer to the red cluster centroid or the blue cluster centroid, it is going to assign each of the data points to one of the two cluster centroids. Specifically, what I mean by that, is to go through your data set and color each of the points either red or blue, depending on whether it is closer to the red cluster centroid or the blue cluster centroid, and I’ve done that in this diagram here. So, that was the cluster assignment step. The other part of K means, in the loop of K means, is the move centroid step, and what we are going to do is, we are going to take the two cluster centroids, that is, the red cross and the blue cross, and we are going to move them to the average of the points colored the same colour. So what we are going to do is look at all the red points and compute the average, really the mean of the location of all the red points, and we are going to move the red cluster centroid there. And the same things for the blue cluster centroid, look at all the blue dots and compute their mean, and then move the blue cluster centroid there. So, let me do that now. We’re going to move the cluster centroids as follows and I’ve now moved them to their new means. The red one moved like that and the blue one moved like that and the red one moved like that. And then we go back to another cluster assignment step, so we’re again going to look at all of my unlabeled examples and depending on whether it’s closer the red or the blue cluster centroid, I’m going to color them either red or blue. I’m going to assign each point to one of the two cluster centroids, so let me do that now. And so the colors of some of the points just changed. And then I’m going to do another move centroid step. So I’m going to compute the average of all the blue points, compute the average of all the red points and move my cluster centroids like this, and so, let’s do that again. Let me do one more cluster assignment step. So colour each point red or blue, based on what it’s closer to and then do another move centroid step and we’re done. And in fact if you keep running additional iterations of K means from here the cluster centroids will not change any further and the colours of the points will not change any further. And so, this is the, at this point, K means has converged and it’s done a pretty good job finding the two clusters in this data. Let’s write out the K means algorithm more formally. The K means algorithm takes two inputs. One is a parameter K, which is the number of clusters you want to find in the data. I’ll later say how we might go about trying to choose k, but for now let’s just say that we’ve decided we want a certain number of clusters and we’re going to tell the algorithm how many clusters we think there are in the data set. And then K means also takes as input this sort of unlabeled training set of just the Xs and because this is unsupervised learning, we don’t have the labels Y anymore. And for unsupervised learning of the K means I’m going to use the convention that $X^{(i)}$ is an $R^N$ dimensional vector. And that’s why my training examples are now N dimensional rather N plus one dimensional vectors. This is what the K means algorithm does. The first step is that it randomly initializes k cluster centroids which we will call mu 1, mu 2, up to mu k. And so in the earlier diagram, the cluster centroids corresponded to the location of the red cross and the location of the blue cross. So there we had two cluster centroids, so maybe the red cross was mu 1 and the blue cross was mu 2, and more generally we would have k cluster centroids rather than just 2. Then the inner loop of k means does the following, we’re going to repeatedly do the following. First for each of my training examples, I’m going to set this variable $c^{(i)}$ to be the index 1 through K of the cluster centroid closest to $x^{(i)}$. So this was my cluster assignment step, where we took each of my examples and coloured it either red or blue, depending on which cluster centroid it was closest to. So $c^{(i)}$ is going to be a number from 1 to K that tells us, you know, is it closer to the red cross or is it closer to the blue cross, and another way of writing this is I’m going to, to compute $c^{(i)}$, I’m going to take my $i_{th}$ example $x^{(i)}$ and and I’m going to measure it’s distance to each of my cluster centroids, this is mu and then lower-case k, right, so capital K is the total number centroids and I’m going to use lower case k here to index into the different centroids. But so, $c^{(i)}$ is going to, I’m going to minimize over my values of k and find the value of K that minimizes this distance between Xi and the cluster centroid, and then, you know, the value of k that minimizes this, that’s what gets set in $c^{(i)}$. So, here’s another way of writing out what Ci is. If I write the norm between Xi minus Mu-k, then this is the distance between my ith training example Xi and the cluster centroid Mu subscript K, this is–this here, that’s a lowercase K. So uppercase K is going to be used to denote the total number of cluster centroids, and this lowercase K’s a number between one and capital K. I’m just using lower case K to index into my different cluster centroids. Next is lower case k. So that’s the distance between the example and the cluster centroid and so what I’m going to do is find the value of K, of lower case k that minimizes this, and so the value of k that minimizes you know, that’s what I’m going to set as Ci, and by convention here I’ve written the distance between Xi and the cluster centroid, by convention people actually tend to write this as the squared distance. So we think of Ci as picking the cluster centroid with the smallest squared distance to my training example Xi. But of course minimizing squared distance, and minimizing distance that should give you the same value of Ci, but we usually put in the square there, just as the convention that people use for K means. So that was the cluster assignment step. The other in the loop of K means does the move centroid step. And what that does is for each of my cluster centroids, so for lower case k equals 1 through K, it sets Mu-k equals to the average of the points assigned to the cluster. So as a concrete example, let’s say that one of my cluster centroids, let’s say cluster centroid two, has training examples, you know, 1, 5, 6, and 10 assigned to it. And what this means is, really this means that C1 equals to C5 equals to C6 equals to and similarly well c10 equals, too, right? If we got that from the cluster assignment step, then that means examples 1,5,6 and 10 were assigned to the cluster centroid two. Then in this move centroid step, what I’m going to do is just compute the average of these four things. So X1 plus X5 plus X6 plus X10. And now I’m going to average them so here I have four points assigned to this cluster centroid, just take one quarter of that. And now Mu2 is going to be an n-dimensional vector. Because each of these example x1, x5, x6, x10 each of them were an n-dimensional vector, and I’m going to add up these things and, you know, divide by four because I have four points assigned to this cluster centroid, I end up with my move centroid step, for my cluster centroid mu-2. This has the effect of moving mu-2 to the average of the four points listed here. One thing that I’ve asked is, well here we said, let’s let mu-k be the average of the points assigned to the cluster. But what if there is a cluster centroid no points with zero points assigned to it. In that case the more common thing to do is to just eliminate that cluster centroid. And if you do that, you end up with K minus one clusters instead of k clusters. Sometimes if you really need k clusters, then the other thing you can do if you have a cluster centroid with no points assigned to it is you can just randomly reinitialize that cluster centroid, but it’s more common to just eliminate a cluster if somewhere during K means it with no points assigned to that cluster centroid, and that can happen, altthough in practice it happens not that often. So that’s the K means Algorithm. Before wrapping up this video I just want to tell you about one other common application of K Means and that’s to the problems with non well separated clusters. Here’s what I mean. So far we’ve been picturing K Means and applying it to data sets like that shown here where we have three pretty well separated clusters, and we’d like an algorithm to find maybe the 3 clusters for us. But it turns out that very often K Means is also applied to data sets that look like this where there may not be several very well separated clusters. Here is an example application, to t-shirt sizing. Let’s say you are a t-shirt manufacturer you’ve done is you’ve gone to the population that you want to sell t-shirts to, and you’ve collected a number of examples of the height and weight of these people in your population and so, well I guess height and weight tend to be positively highlighted so maybe you end up with a data set like this, you know, with a sample or set of examples of different peoples heights and weight. Let’s say you want to size your t shirts. Let’s say I want to design and sell t shirts of three sizes, small, medium and large. So how big should I make my small one? How big should I my medium? And how big should I make my large t-shirts. One way to do that would to be to run my k means clustering logarithm on this data set that I have shown on the right and maybe what K Means will do is group all of these points into one cluster and group all of these points into a second cluster and group all of those points into a third cluster. So, even though the data, you know, before hand it didn’t seem like we had 3 well separated clusters, K Means will kind of separate out the data into multiple pluses for you. And what you can do is then look at this first population of people and look at them and, you know, look at the height and weight, and try to design a small t-shirt so that it kind of fits this first population of people well and then design a medium t-shirt and design a large t-shirt. And this is in fact kind of an example of market segmentation where you’re using K Means to separate your market into 3 different segments. So you can design a product separately that is a small, medium, and large t-shirts, that tries to suit the needs of each of your 3 separate sub-populations well. So that’s the K Means algorithm. And by now you should know how to implement the K Means Algorithm and kind of get it to work for some problems. But in the next few videos what I want to do is really get more deeply into the nuts and bolts of K means and to talk a bit about how to actually get this to work really well. summaryThe K-Means Algorithm is the most popular and widely used algorithm for automatically grouping data into coherent subsets. Randomly initialize two points in the dataset called the cluster centroids . Cluster assignment: assign all examples into one of two groups based on which cluster centroid the example is closest to. Move centroid: compute the averages for all the points inside each of the two cluster centroid groups, then move the cluster centroid points to those averages. Re-run (2) and (3) until we have found our clusters. Our main variables are: K (number of clusters) Training set ${x^{(1)}, x^{(2)}, \dots,x^{(m)}}$ Where $x^{(i)} \in \mathbb{R}^n$ Note that we will not use the $x_0=1$ convention. The algorithm:123456Randomly initialize K cluster centroids mu(1), mu(2), ..., mu(K)Repeat: for i = 1 to m: c(i):= index (from 1 to K) of cluster centroid closest to x(i) for k = 1 to K: mu(k):= average (mean) of points assigned to cluster k The first for-loop is the ‘Cluster Assignment’ step. We make a vector c where $c^{(i)}$ represents the centroid assigned to example $x^{(i)}$ .We can write the operation of the Cluster Assignment step more mathematically as follows:$c^{(i)} = argmin_k\ ||x^{(i)} - \mu_k||^2$That is, each $c^{(i)}$ contains the index of the centroid that has minimal distance to $x^{(i)}$.By convention, we square the right-hand-side, which makes the function we are trying to minimize more sharply increasing. It is mostly just a convention. But a convention that helps reduce the computation load because the Euclidean distance requires a square root but it is canceled.Without the square:$$||x^{(i)} - \mu_k|| = ||\quad\sqrt{(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + …}\quad||$$With the square:$$||x^{(i)} - \mu_k||^2 = ||\quad(x_1^i - \mu_{1(k)})^2 + (x_2^i - \mu_{2(k)})^2 + (x_3^i - \mu_{3(k)})^2 + …\quad||$$so the square convention serves two purposes, minimize more sharply and less computation.The second for-loop is the ‘Move Centroid’ step where we move each centroid to the average of its group.More formally, the equation for this loop is as follows:$$\mu_k = \dfrac{1}{n}[x^{(k_1)} + x^{(k_2)} + \dots + x^{(k_n)}] \in \mathbb{R}^n$$Where each of $x^{(k_1)}, x^{(k_2)}, \dots, x^{(k_n)}$ are the training examples assigned to group $mμ_k$.If you have a cluster centroid with 0 points assigned to it, you can randomly re-initialize that centroid to a new point. You can also simply eliminate that cluster group.After a number of iterations the algorithm will converge , where new iterations do not affect the clusters.Note on non-separated clusters: some datasets have no real inner separation or natural structure. K-means can still evenly segment your data into K subsets, so can still be useful in this case. 03_optimization-objectiveMost of the supervised learning algorithms we’ve seen, things like linear regression, logistic regression, and so on, all of those algorithms have an optimization objective or some cost function that the algorithm was trying to minimize. It turns out that k-means also has an optimization objective or a cost function that it’s trying to minimize. And in this video I’d like to tell you what that optimization objective is. And the reason I want to do so is because this will be useful to us for two purposes. First, knowing what is the optimization objective of k-means will help us to debug the learning algorithm and just make sure that k-means is running correctly. And second, and perhaps more importantly, in a later video we’ll talk about how we can use this to help k-means find better costs for this and avoid the local optima. But we do that in a later video that follows this one. Just as a quick reminder while k-means is running we’re going to be keeping track of two sets of variables. First is the $c^{(i)}$’s and that keeps track of the index or the number of the cluster, to which an example $x^{(i)}$ is currently assigned. And then the other set of variables we use is $/mu_k$, which is the location of cluster centroid k. Again, for k-means we use capital K to denote the total number of clusters. And here lower case k is going to be an index into the cluster centroids and so, lower case k is going to be a number between one and capital K. Now here’s one more bit of notation, which is gonna use mu subscript ci ($/mu_{c^{(i)}}$) to denote the cluster centroid of the cluster to which example $x^{(i)}$ has been assigned, right? And to explain that notation a little bit more, lets say that xi has been assigned to cluster number five. What that means is that ci, that is the index of xi, that that is equal to five. Right? Because having ci equals five, if that’s what it means for the example xi to be assigned to cluster number five. And so mu subscript ci is going to be equal to mu subscript 5. Because ci is equal to five. And so this mu subscript ci is the cluster centroid of cluster number five, which is the cluster to which my example xi has been assigned. Out with this notation, we’re now ready to write out what is the optimization objective of the k-means clustering algorithm and here it is. The cost function that k-means is minimizing is a function J of all of these parameters, $c^{(1)}$ through $c^{(m)}$ and $/mu_1$ through $/gmu_K$. That k-means is varying as the algorithm runs. And the optimization objective is shown to the right, is the average of 1 over m of sum from i equals 1 through m of this term here. That I’ve just drawn the red box around, right? The square distance between each example xi and the location of the cluster centroid to which xi has been assigned. So let’s draw this and just let me explain this. Right, so here’s the location of training example xi and here’s the location of the cluster centroid to which example xi has been assigned. So to explain this in pictures, if here’s x1, x2, and if a point here is my example xi, so if that is equal to my example xi, and if xi has been assigned to some cluster centroid, I’m gonna denote my cluster centroid with a cross, so if that’s the location of mu 5, let’s say. If x i has been assigned cluster centroid five as in my example up there, then this square distance, that’s the square of the distance between the point xi and this cluster centroid to which xi has been assigned. And what k-means can be shown to be doing is that it is trying to define parameters ci and mu i. Trying to find c and mu to try to minimize this cost function J. This cost function is sometimes also called the distortion cost function, or the distortion of the k-means algorithm. And just to provide a little bit more detail, here’s the k-means algorithm. Here’s exactly the algorithm as we have written it out on the earlier slide. And what this first step of this algorithm is, this was the cluster assignment step where we assigned each point to the closest centroid. And it’s possible to show mathematically that what the cluster assignment step is doing is exactly Minimizing J, with respect to the variables c1, c2 and so on, up to cm, while holding the cluster centroids mu 1 up to mu K, fixed. So what the cluster assignment step does is it doesn’t change the cluster centroids, but what it’s doing is this is exactly picking the values of c1, c2, up to cm. That minimizes the cost function, or the distortion function J. And it’s possible to prove that mathematically, but I won’t do so here. But it has a pretty intuitive meaning of just well, let’s assign each point to a cluster centroid that is closest to it, because that’s what minimizes the square of distance between the points in the cluster centroid. And then the second step of k-means, this second step over here. The second step was the move centroid step. And once again I won’t prove it, but it can be shown mathematically that what the move centroid step does is it chooses the values of mu that minimizes J, so it minimizes the cost function J with respect to, wrt is my abbreviation for, with respect to, when it minimizes J with respect to the locations of the cluster centroids mu 1 through mu K. So if is really is doing is this taking the two sets of variables and partitioning them into two halves right here. First the c sets of variables and then you have the mu sets of variables. And what it does is it first minimizes J with respect to the variable c and then it minimizes J with respect to the variables mu and then it keeps on. And, so all that’s all that k-means does. And now that we understand k-means as trying to minimize this cost function J, we can also use this to try to debug other any algorithm and just kind of make sure that our implementation of k-means is running correctly. So, we now understand the k-means algorithm as trying to optimize this cost function J, which is also called the distortion function. We can use that to debug k means and help make sure that k-means is converging and is running properly. And in the next video we’ll also see how we can use this to help k-means find better clusters and to help k-means to avoid local optima. summaryRecall some of the parameters we used in our algorithm:$c^{(i)}$ = index of cluster (1,2,…,K) to which example $x^{(i)}$ is currently assigned$\mu_k $= cluster centroid k (μk∈ℝn)$\mu_{c^{(i)}}$ = cluster centroid of cluster to which example $x^{(i)}$ has been assignedUsing these variables we can define our cost function :$$J(c^{(i)},\dots,c^{(m)},\mu_1,\dots,\mu_K) = \dfrac{1}{m}\sum_{i=1}^m ||x^{(i)} - \mu_{c^{(i)}}||^2$$Our optimization objective is to minimize all our parameters using the above cost function:$$min_{c,\mu}\ J(c,\mu)$$That is, we are finding all the values in sets c, representing all our clusters, and μ, representing all our centroids, that will minimize the average of the distances of every training example to its corresponding cluster centroid.The above cost function is often called the distortion of the training examples.In the cluster assignment step , our goal is to:Minimize J(…) with $c^{(1)},\dots,c^{(m)}$ (holding $\mu_1,\dots,\mu_K$ fixed)In the move centroid step, our goal is to:Minimize J(…) with $\mu_1,\dots,\mu_K$With k-means, it is not possible for the cost function to sometimes increase . It should always descend. 04_random-initializationIn this video, I’d like to talk about how to initialize K-means and more importantly, this will lead into a discussion of how to make K-means avoid local optima as well. Here’s the K-means clustering algorithm that we talked about earlier. One step that we never really talked much about was this step of how you randomly initialize the cluster centroids. There are few different ways that one can imagine using to randomly initialize the cluster centroids. But, it turns out that there is one method that is much more recommended than most of the other options one might think about. So, let me tell you about that option since it’s what often seems to work best. Here’s how I usually initialize my cluster centroids. When running K-means, you should have the number of cluster centroids, K, set to be less than the number of training examples M. It would be really weird to run K-means with a number of cluster centroids that’s, you know, equal or greater than the number of examples you have, right? So the way I usually initialize K-means is, I would randomly pick k training examples. So, and, what I do is then set $\mu_1$ of $\mu_K$ equal to these k examples. Let me show you a concrete example. Lets say that k is equal to 2 and so on this example on the right let’s say I want to find two clusters. So, what I’m going to do in order to initialize my cluster centroids is, I’m going to randomly pick a couple examples. And let’s say, I pick this one and I pick that one. And the way I’m going to initialize my cluster centroids is, I’m just going to initialize my cluster centroids to be right on top of those examples. So that’s my first cluster centroid and that’s my second cluster centroid, and that’s one random initialization of K-means. The one I drew looks like a particularly good one. And sometimes I might get less lucky and maybe I’ll end up picking that as my first random initial example, and that as my second one. And here I’m picking two examples because k equals 2. Some we have randomly picked two training examples and if I chose those two then I’ll end up with, may be this as my first cluster centroid and that as my second initial location of the cluster centroid. So, that’s how you can randomly initialize the cluster centroids. And so at initialization, your first cluster centroid Mu1 will be equal to x(i) for some randomly value of i and Mu2 will be equal to x(j) for some different randomly chosen value of j and so on, if you have more clusters and more cluster centroid. And sort of the side common. I should say that in the earlier video where I first illustrated K-means with the animation. In that set of slides. Only for the purpose of illustration. I actually used a different method of initialization for my cluster centroids.But the method described on this slide, this is really the recommended way. And the way that you should probably use, when you implement K-means. So, as they suggested perhaps by these two illustrations on the right. You might really guess that K-means can end up converging to different solutions depending on exactly how the clusters were initialized, and so, depending on the random initialization. K-means can end up at different solutions. And, in particular, K-means can actually end up at local optima. If you’re given the data sale like this. Well, it looks like, you know, there are three clusters, and so, if you run K-means and if it ends up at a good local optima this might be really the global optima, you might end up with that cluster ring. But if you had a particularly unlucky, random initialization, K-means can also get stuck at different local optima. So, in this example on the left it looks like this blue cluster has captured a lot of points of the left and then the they were on the green clusters each is captioned on the relatively small number of points. And so, this corresponds to a bad local optima because it has basically taken these two clusters and used them into 1 and furthermore, has split the second cluster into two separate sub-clusters like so, and it has also taken the second cluster and split it into two separate sub-clusters like so, and so, both of these examples on the lower right correspond to different local optima of K-means and in fact, in this example here, the cluster, the red cluster has captured only a single optima example. And the term local optima, by the way, refers to local optima of this distortion function J, and what these solutions on the lower left, what these local optima correspond to is really solutions where K-means has gotten stuck to the local optima and it’s not doing a very good job minimizing this distortion function J. So, if you’re worried about K-means getting stuck in local optima, if you want to increase the odds of K-means finding the best possible clustering, like that shown on top here, what we can do, is try multiple, random initializations. So, instead of just initializing K-means once and hopping that that works, what we can do is, initialize K-means lots of times and run K-means lots of times, and use that to try to make sure we get as good a solution, as good a local or global optima as possible. Concretely, here’s how you could go about doing that. Let’s say, I decide to run K-meanss a hundred times so I’ll execute this loop a hundred times and it’s fairly typical a number of times when came to will be something from 50 up to may be 1000. So, let’s say you decide to say K-means one hundred times. So what that means is that we would randomnly initialize K-means. And for each of these one hundred random intializations we would run K-means and that would give us a set of clusteringings, and a set of cluster centroids, and then we would then compute the distortion J, that is compute this cause function on the set of cluster assignments and cluster centroids that we got. Finally, having done this whole procedure a hundred times. You will have a hundred different ways of clustering the data and then finally what you do is all of these hundred ways you have found of clustering the data, just pick one, that gives us the lowest cost. That gives us the lowest distortion. And it turns out that if you are running K-means with a fairly small number of clusters , so you know if the number of clusters is anywhere from two up to maybe 10 - then doing multiple random initializations can often, can sometimes make sure that you find a better local optima. Make sure you find the better clustering data. But if K is very large, so, if K is much greater than 10, certainly if K were, you know, if you were trying to find hundreds of clusters, then, having multiple random initializations is less likely to make a huge difference and there is a much higher chance that your first random initialization will give you a pretty decent solution already and doing, doing multiple random initializations will probably give you a slightly better solution but, but maybe not that much. But it’s really in the regime of where you have a relatively small number of clusters, especially if you have, maybe 2 or 3 or 4 clusters that random initialization could make a huge difference in terms of making sure you do a good job minimizing the distortion function and giving you a good clustering. So, that’s K-means with random initialization. If you’re trying to learn a clustering with a relatively small number of clusters, 2, 3, 4, 5, maybe, 6, 7, using multiple random initializations can sometimes, help you find much better clustering of the data. But, even if you are learning a large number of clusters, the initialization, the random initialization method that I describe here. That should give K-means a reasonable starting point to start from for finding a good set of clusters. summaryThere’s one particular recommended method for randomly initializing your cluster centroids. Have K&lt;m. That is, make sure the number of your clusters is less than the number of your training examples. Randomly pick K training examples. (Not mentioned in the lecture, but also be sure the selected examples are unique). Set $\mu_1,\dots,\mu_K$ equal to these K examples. K-means can get stuck in local optima. To decrease the chance of this happening, you can run the algorithm on many different random initializations. In cases where K&lt;10 it is strongly recommended to run a loop of random initializations. 12345for i = 1 to 100: randomly initialize k-means run k-means to get 'c' and 'm' compute the cost function (distortion) J(c,m)pick the clustering that gave us the lowest cost 05_choosing-the-number-of-clustersIn this video I’d like to talk about one last detail of K-means clustering which is how to choose the number of clusters, or how to choose the value of the parameter capsule K. To be honest, there actually isn’t a great way of answering this or doing this automatically and by far the most common way of choosing the number of clusters, is still choosing it manually by looking at visualizations or by looking at the output of the clustering algorithm or something else. But I do get asked this question quite a lot of how do you choose the number of clusters, and so I just want to tell you know what are peoples’ current thinking on it although, the most common thing is actually to choose the number of clusters by hand. A large part of why it might not always be easy to choose the number of clusters is that it is often generally ambiguous how many clusters there are in the data. Looking at this data set some of you may see four clusters and that would suggest using K equals 4. Or some of you may see two clusters and that will suggest K equals 2 and now this may see three clusters. And so, looking at the data set like this, the true number of clusters, it actually seems genuinely ambiguous to me, and I don’t think there is one right answer. And this is part of our supervised learning. We are aren’t given labels, and so there isn’t always a clear cut answer. And this is one of the things that makes it more difficult to say, have an automatic algorithm for choosing how many clusters to have. When people talk about ways of choosing the number of clusters, one method that people sometimes talk about is something called the Elbow Method. Let me just tell you a little bit about that, and then mention some of its advantages but also shortcomings. So the Elbow Method, what we’re going to do is vary K, which is the total number of clusters. So, we’re going to run K-means with one cluster, that means really, everything gets grouped into a single cluster and compute the cost function or compute the distortion J and plot that here. And then we’re going to run K means with two clusters, maybe with multiple random initial agents, maybe not. But then, you know, with two clusters we should get, hopefully, a smaller distortion, and so plot that there. And then run K-means with three clusters, hopefully, you get even smaller distortion and plot that there. I’m gonna run K-means with four, five and so on. And so we end up with a curve showing how the distortion, you know, goes down as we increase the number of clusters. And so we get a curve that maybe looks like this. And if you look at this curve, what the Elbow Method does it says “Well, let’s look at this plot. Looks like there’s a clear elbow there“. Right, this is, would be by analogy to the human arm where, you know, if you imagine that you reach out your arm, then, this is your shoulder joint, this is your elbow joint and I guess, your hand is at the end over here. And so this is the Elbow Method. Then you find this sort of pattern where the distortion goes down rapidly from 1 to 2, and 2 to 3, and then you reach an elbow at 3, and then the distortion goes down very slowly after that. And then it looks like, you know what, maybe using three clusters is the right number of clusters, because that’s the elbow of this curve, right? That it goes down, distortion goes down rapidly until K equals 3, really goes down very slowly after that. So let’s pick K equals 3. If you apply the Elbow Method, and if you get a plot that actually looks like this, then, that’s pretty good, and this would be a reasonable way of choosing the number of clusters. It turns out the Elbow Method isn’t used that often, and one reason is that, if you actually use this on a clustering problem, it turns out that fairly often, you know, you end up with a curve that looks much more ambiguous, maybe something like this. And if you look at this, I don’t know, maybe there’s no clear elbow, but it looks like distortion continuously goes down, maybe 3 is a good number, maybe 4 is a good number, maybe 5 is also not bad. And so, if you actually do this in a practice, you know, if your plot looks like the one on the left and that’s great. It gives you a clear answer, but just as often, you end up with a plot that looks like the one on the right and is not clear where the ready location of the elbow is. It makes it harder to choose a number of clusters using this method. So maybe the quick summary of the Elbow Method is that is worth the shot but I wouldn’t necessarily, you know, have a very high expectation of it working for any particular problem. Finally, here’s one other way of how, thinking about how you choose the value of K, very often people are running K-means in order you get clusters for some later purpose, or for some sort of downstream purpose. Maybe you want to use K-means in order to do market segmentation, like in the T-shirt sizing example that we talked about. Maybe you want K-means to organize a computer cluster better, or maybe a learning cluster for some different purpose, and so, if that later, downstream purpose, such as market segmentation. If that gives you an evaluation metric, then often, a better way to determine the number of clusters, is to see how well different numbers of clusters serve that later downstream purpose. Let me step through a specific example. Let me go through the T-shirt size example again, and I’m trying to decide, do I want three T-shirt sizes? So, I choose K equals 3, then I might have small, medium and large T-shirts. Or maybe, I want to choose K equals 5, and then I might have, you know, extra small, small, medium, large and extra large T-shirt sizes. So, you can have like 3 T-shirt sizes or four or five T-shirt sizes. We could also have four T-shirt sizes, but I’m just showing three and five here, just to simplify this slide for now. So, if I run K-means with K equals 3, maybe I end up with, that’s my small and that’s my medium and that’s my large. Whereas, if I run K-means with 5 clusters, maybe I end up with, those are my extra small T-shirts, these are my small, these are my medium, these are my large and these are my extra large. And the nice thing about this example is that, this then maybe gives us another way to choose whether we want 3 or 4 or 5 clusters, and in particular, what you can do is, you know, think about this from the perspective of the T-shirt business and ask: “Well if I have five segments, then how well will my T-shirts fit my customers and so, how many T-shirts can I sell? How happy will my customers be?” What really makes sense, from the perspective of the T-shirt business, in terms of whether, I want to have Goer T-shirt sizes so that my T-shirts fit my customers better. Or do I want to have fewer T-shirt sizes so that I make fewer sizes of T-shirts. And I can sell them to the customers more cheaply. And so, the t-shirt selling business, that might give you a way to decide, between three clusters versus five clusters. So, that gives you an example of how a later downstream purpose like the problem of deciding what T-shirts to manufacture, how that can give you an evaluation metric for choosing the number of clusters. For those of you that are doing the program exercises, if you look at this week’s program exercise associative K-means, that’s an example there of using K-means for image compression. And so if you were trying to choose how many clusters to use for that problem, you could also, again use the evaluation metric of image compression to choose the number of clusters, K? So, how good do you want the image to look versus, how much do you want to compress the file size of the image, and, you know, if you do the programming exercise, what I’ve just said will make more sense at that time. So, just summarize, for the most part, the number of customers K is still chosen by hand by human input or human insight. One way to try to do so is to use the Elbow Method, but I wouldn’t always expect that to work well, but I think the better way to think about how to choose the number of clusters is to ask, for what purpose are you running K-means? And then to think, what is the number of clusters K that serves that, you know, whatever later purpose that you actually run the K-means for. summaryChoosing K can be quite arbitrary and ambiguous.The elbow method : plot the cost J and the number of clusters K. The cost function should reduce as we increase the number of clusters, and then flatten out. Choose K at the point where the cost function starts to flatten out.However, fairly often, the curve is very gradual , so there’s no clear elbow.Note: J will always decrease as K is increased. The one exception is if k-means gets stuck at a bad local optimum.Another way to choose K is to observe how well k-means performs on a downstream purpose . In other words, you choose K that proves to be most useful for some goal you’re trying to achieve from using these clusters. Bonus: Discussion of the drawbacks of K-MeansThis links to a discussion that shows various situations in which K-means gives totally correct but unexpected results: http://stats.stackexchange.com/questions/133656/how-to-understand-the-drawbacks-of-k-means]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[12_support-vector-machines note12]]></title>
    <url>%2F2018%2F01%2F12%2F12_support-vector-machines%2F</url>
    <content type="text"><![CDATA[NoteThis personal note is written after studying the opening course on the coursera website, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. 01_large-margin-classification01_optimization-objectiveBy now, you’ve seen a range of difference learning algorithms. With supervised learning, the performanceof many supervised learning algorithms will be pretty similar, and what matters less often will be whether you use learning algorithm a or learning algorithm b, but what matters more will often be things like the amount of data you create these algorithms on, as well as your skill in applying these algorithms. Things like your choice of the features you design to give to the learning algorithms, and how you choose the colorization parameter, and things like that. But, there’s one more algorithm that is very powerful and is very widely used both within industry and academia, and that’s called the Support Vector Machine. And compared to both logistic regression and neural networks, the Support Vector Machine, or SVM sometimes gives a cleaner, and sometimes more powerful way of learning complex non-linear functions. And so let’s take the next videos to talk about that. Later in this course, I will do a quick survey of a range of different supervisory algorithms just as a very briefly describe them. But the support vector machine, given its popularity and how powerful it is, this will be the last of the supervisory algorithms that I’ll spend a significant amount of time on in this course as with our development other learning algorithms, we’re gonna start by talking about the optimization objective. So, let’s get started on this algorithm. In order to describe the support vector machine, I’m actually going to start with logistic regression, and show how we can modify it a bit, and get what is essentially the support vector machine. So in logistic regression, we have our familiar form of the hypothesis there and the sigmoid activation function shown on the right. And in order to explain some of the math, I’m going to use z to denote theta transpose axiom. Now let’s think about what we would like logistic regression to do. If we have an example with y equals one and by this I mean an example in either the training set or the test set or the cross-validation set, but when y is equal to one then we’re sort of hoping that h of x will be close to one. Right, we’re hoping to correctly classify that example. And what having x subscript 1, what that means is that theta transpose x must be must larger than 0. So there’s greater than, greater than sign that means much, much greater than 0. And that’s because it is z, the theta of transpose x is when z is much bigger than 0 is far to the right of the sphere. That the outputs of logistic progression becomes close to one. Conversely, if we have an example where y is equal to zero, then what we’re hoping for is that the hypothesis will output a value close to zero. And that corresponds to theta transpose x of z being much less than zero because that corresponds to a hypothesis of putting a value close to zero. If you look at the cost function of logistic regression, what you’ll find is that each example (x,y) contributes a term like this to the overall cost function, right? So for the overall cost function, we will also have a sum over all the chain examples and the 1 over m term, that this expression here, that’s the term that a singletraining example contributes to the overall objective function so we can just rush them. Now if I take the definition for the fall of my hypothesis and plug it in over here, then what I get is that each training example contributes this term, ignoring the one over M but it contributes that term to my overall cost function for logistic regression. Now let’s consider two cases of when y is equal to one and when y is equal to zero. In the first case, let’s suppose that y is equal to 1. In that case, only this first term in the objective matters, because this one minus y term would be equal to zero if y is equal to one. So when y is equal to one, when in our example x comma y, when y is equal to 1 what we get is this term.. Minus log one over one, plus E to the negative Z where as similar to the last line I’m using Z to denote data transposed X and of course in a cost I should have this minus line that we just had if Y is equal to one so that’s equal to one I just simplify in a way in the expression that I have written down here. And if we plot this function as a function of z, what you find is that you get this curve shown on the lower left of the slide. And thus, we also see that when z is equal to large, that is, when theta transpose x is large, that corresponds to a value of z that gives us a fairly small value, a very, very small contribution to the consumption. And this kinda explains why, when logistic regression sees a positive example, with y=1, it tries to set theta transport x to be very large because that corresponds to this term, in the cross function, being small. Now, to fill the support vector machine, here’s what we’re going to do. We’re gonna take this cross function, this minus log 1 over 1 plus e to negative z, and modify it a little bit. Let me take this point 1 over here, and let me draw the cross functions you’re going to use. The new pass functions can be flat from here on out, and then we draw something that grows as a straight line, similar to logistic regression. But this is going to be a straight line at this portion. So the curve that I just drew in magenta, and the curve I just drew purple and magenta, so if it’s pretty close approximation to the cross function used by logistic regression. Except it is now made up of two line segments, there’s this flat portion on the right, and then there’s this straight line portion on the left. And don’t worry too much about the slope of the straight line portion. It doesn’t matter that much. But that’s the new cost function we’re going to use for when y is equal to one, and you can imagine it should do something pretty similar to logistic regression. But turns out, that this will give the support vector machine computational advantages and give us, later on, an easier optimization problem that would be easier for software to solve. We just talked about the case of y equals one. The other case is if y is equal to zero. In that case, if you look at the cost, then only the second term will apply because the first term goes away, right? If y is equal to zero, then you have a zero here, so you’re left only with the second term of the expression above. And so the cost of an example, or the contribution of the cost function, is going to be given by this term over here. And if you plot that as a function of z, to have pure z on the horizontal axis, you end up with this one. And for the support vector machine, once again, we’re going to replace this blue line with something similar and at the same time we replace it with a new cost, this flat out here, this 0 out here. And that then grows as a straight line, like so. So let me give these two functions names. This function on the left below I’m going to call cost subscript 1 of z $cost_1(z)$, and this function of the right I’m gonna call cost subscript 0 of z $cost_0(z)$. And the subscript just refers to the cost corresponding to when y is equal to 1, versus when y Is equal to zero. Armed with these definitions, we’re now ready to build a support vector machine. Here’s the cost function, j of theta, that we have for logistic regression. In case this equation looks a bit unfamiliar, it’s because previously we had a minus sign outside, but here what I did was I instead moved the minus signs inside these expressions, so it just makes it look a little different. For the support vector machine what we’re going to do is essentially take this and replace this with cost1 of z, that is cost1 of theta transpose x. And we’re going to take this and replace it with cost0 of z, that is cost0 of theta transpose x. Where the cost one function is what we had on the previous slide that looks like this. And the cost zero function, again what we had on the previous slide, and it looks like this. So what we have for the support vector machine is a minimization problem of one over M, the sum of Y I times cost one, theta transpose X I, plus one minus Y I times cause zero of theta transpose X I, and then plus my usual regularization parameter. Like so. Now, by convention, for the support of vector machine, we’re actually write things slightly different. We re-parameterize this just very slightly differently. First, we’re going to get rid of the 1 over m terms, and this just this happens to be a slightly different convention that people use for support vector machines compared to or just a progression. But here’s what I mean. You’re one way to do this, we’re just gonna get rid of these one over m terms and this should give you me the same optimal value of beta right? Because one over m is just as constant so whether I solved this minimization problem with one over n in front or not. I should end up with the same optimal value for theta. Here’s what I mean, to give you an example, suppose I had a minimization problem $min_u(u-5)^2+1$. Well, the minimum of this happens to be $u=5$. Now if I were to take this objective function and multiply it by 10. So here my minimization problem is min over U, 10 U minus five squared plus 10 ( $min_u10[(u-5)^2+1]$ ). Well the value of U that minimizes this is still U equals five right? So multiply something that you’re minimizing over, by some constant, 10 in this case, it does not change the value of U that gives us, that minimizes this function. So the same way, what I’ve done is by crossing out the M is all I’m doing is multiplying my objective function by some constant M and it doesn’t change the value of theta. That achieves the minimum. The second bit of notational change, which is just, again, the more standard convention when using SVMs instead of logistic regression, is the following. So for logistic regression, we add two terms to the objective function. The first is this term, which is the cost that comes from the training set and the second is this row, which is the regularization term. And what we had was we had a, we control the trade-off between these by saying, what we want is A plus, and then my regularization parameter lambda. And then times some other term B, where I guess I’m using your A to denote this first term, and I’m using B to denote the second term, maybe without the lambda. And instead of prioritizing this as A plus lambda B, and so what we did was by setting different values for this regularization parameter lambda, we could trade off the relative weight between how much we wanted the training set well, that is, minimizing A, versus how much we care about keeping the values of the parameter small, so that will be, the parameter is B for the support vector machine, just by convention, we’re going to use a different parameter. So instead of using lambda here to control the relative waiting between the first and second terms. We’re instead going to use a different parameter which by convention is called C and is set to minimize $C \times a + B$. So for logistic regression, if we set a very large value of lambda, that means you will give B a very high weight. Here is that if we set C to be a very small value, then that responds to giving B a much larger rate than C, than A. So this is just a different way of controlling the trade off, it’s just a different way of prioritizing how much we care about optimizing the first term, versus how much we care about optimizing the second term. And if you want you can think of this as the parameter C playing a role similar to 1 over lambda. And it’s not that it’s two equations or these two expressions will be equal. This equals 1 over lambda, that’s not the case. It’s rather that if C is equal to 1 over lambda, then these two optimization objectives should give you the same value, the same optimal value for theta. so we just filling that in I’m gonna cross out lambda here and write in the constant C there. So that gives us our overall optimization objective function for the support vector machine. And if you minimize that function, then what you have is the parameters learned by the SVM. Finally unlike logistic regression, the support vector machine doesn’t output the probability is that what we have is we have this cost function, that we minimize to get the parameter’s data, and what a support vector machine does is it just makes a prediction of y being equal to one or zero, directly. So the hypothesis will predict one if theta transpose x is greater or equal to zero, and it will predict zero otherwise and so having learned the parameters theta, this is the form of the hypothesis for the support vector machine. So that was a mathematical definition of what a support vector machine does. In the next few videos, let’s try to get back to intuition about what this optimization objective leads to and whether the source of the hypotheses SVM will learn and we’ll also talk about how to modify this just a little bit to the complex nonlinear functions. summaryOptimization ObjectiveThe Support Vector Machine (SVM) is yet another type of supervised machine learning algorithm. It is sometimes cleaner and more powerful.Recall that in logistic regression, we use the following rules:if y=1, then $h_\theta(x) \approx 1$ and $\Theta^Tx \gg 0$if y=0, then $h_\theta(x) \approx 0$ and $\Theta^Tx \ll 0$Recall the cost function for (unregularized) logistic regression: $$\begin{align}J(\theta) &amp; = \frac{1}{m}\sum_{i=1}^m -y^{(i)} \log(h_\theta(x^{(i)})) - (1 - y^{(i)})\log(1 - h_\theta(x^{(i)}))\\ &amp; = \frac{1}{m}\sum_{i=1}^m -y^{(i)} \log\Big(\dfrac{1}{1 + e^{-\theta^Tx^{(i)}}}\Big) - (1 - y^{(i)})\log\Big(1 - \dfrac{1}{1 + e^{-\theta^Tx^{(i)}}}\Big)\end{align}$$ To make a support vector machine, we will modify the first term of the cost function $-\log(h_{\theta}(x)) = -\log\Big(\dfrac{1}{1 + e^{-\theta^Tx}}\Big)$ so that when $θ^Tx$ (from now on, we shall refer to this as z) is greater than 1, it outputs 0. Furthermore, for values of z less than 1, we shall use a straight decreasing line instead of the sigmoid curve.(In the literature, this is called a hinge loss ( https://en.wikipedia.org/wiki/Hinge_loss) function.) Similarly, we modify the second term of the cost function $$-\log(1 - h_{\theta(x)}) = -\log\Big(1 - \dfrac{1}{1 + e^{-\theta^Tx}}\Big)$$ so that when z is less than -1, it outputs 0. We also modify it so that for values of z greater than -1, we use a straight increasing line instead of the sigmoid curve. We shall denote these as $\text{cost}_1(z)$ and $\text{cost}_0(z)$ (respectively, note that $\text{cost}_1(z)$ is the cost for classifying when y=1, and $\text{cost}_0(z)$ is the cost for classifying when y=0), and we may define them as follows (where k is an arbitrary constant defining the magnitude of the slope of the line):$$z = \theta^Tx$$$$\text{cost}_0(z) = \max(0, k(1+z))$$$$\text{cost}_1(z) = \max(0, k(1-z))$$Recall the full cost function from (regularized) logistic regression:$$J(\theta) = \frac{1}{m} \sum_{i=1}^m y^{(i)}(-\log(h_\theta(x^{(i)}))) + (1 - y^{(i)})(-\log(1 - h_\theta(x^{(i)}))) + \dfrac{\lambda}{2m}\sum_{j=1}^n \Theta^2_j$$Note that the negative sign has been distributed into the sum in the above equation.We may transform this into the cost function for support vector machines by substituting $\text{cost}_0(z)$ and $\text{cost}_1(z)$:$$J(\theta) = \frac{1}{m} \sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{\lambda}{2m}\sum_{j=1}^n \Theta^2_j$$We can optimize this a bit by multiplying this by m (thus removing the m factor in the denominators). Note that this does not affect our optimization, since we’re simply multiplying our cost function by a positive constant (for example, minimizing $$(u-5)^2 + 1$$ gives us 5; multiplying it by 10 to make it $$10(u-5)^2 + 10$$ still gives us 5 when minimized).$$J(\theta) = \sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{\lambda}{2}\sum_{j=1}^n \Theta^2_j$$Furthermore, convention dictates that we regularize using a factor C, instead of λ, like so:$$J(\theta) = C\sum_{i=1}^m y^{(i)} \ \text{cost}_1(\theta^Tx^{(i)}) + (1 - y^{(i)}) \ \text{cost}_0(\theta^Tx^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j$$This is equivalent to multiplying the equation by $C = \dfrac{1}{\lambda}$, and thus results in the same values when optimized. Now, when we wish to regularize more (that is, reduce overfitting), we decrease C, and when we wish to regularize less (that is, reduce underfitting), we increase C.Finally, note that the hypothesis of the Support Vector Machine is not interpreted as the probability of y being 1 or 0 (as it is for the hypothesis of logistic regression). Instead, it outputs either 1 or 0. (In technical terms, it is a discriminant function.)$$h_\theta(x) =\begin{cases} 1 &amp; \text{if} \ \Theta^Tx \geq 0 \\ 0 &amp; \text{otherwise}\end{cases}$$ 02_large-margin-intuitionSometimes people talk about support vector machines, as large margin classifiers, in this video I’d like to tell you what that means, and this will also give us a useful picture of what an SVM hypothesis may look like. Here’s my cost function for the support vector machine where here on the left I’ve plotted my cost 1 of z function that I used for positive examples and on the right I’ve plotted my zero of ‘Z’ function, where I have ‘Z’ here on the horizontal axis. Now, let’s think about what it takes to make these cost functions small. If you have a positive example, so if y is equal to 1, then cost 1 of Z is zero only when Z is greater than or equal to 1. So in other words, if you have a positive example, we really want theta transpose x to be greater than or equal to 1 and conversely if y is equal to zero, look this cost zero of z function, then it’s only in this region where z is less than equal to 1 we have the cost is zero as z is equals to zero, and this is an interesting property of the support vector machine right, which is that, if you have a positive example so if y is equal to one, then all we really need is that theta transpose x is greater than equal to zero.And that would mean that we classify correctly because if theta transpose x is greater than zero our hypothesis will predict zero. And similarly, if you have a negative example, then really all you want is that theta transpose x is less than zero and that will make sure we got the example right. But the support vector machine wants a bit more than that. It says, you know, don’t just barely get the example right. So then don’t just have it just a little bit bigger than zero. What i really want is for this to be quite a lot bigger than zero say maybe bit greater or equal to one and I want this to be much less than zero. Maybe I want it less than or equal to -1. And so this builds in an extra safety factor or safety margin factor into the support vector machine. Logistic regression does something similar too of course, but let’s see what happens or let’s see what the consequences of this are, in the context of the support vector machine. Concretely, what I’d like to do next is consider a case case where we set this constant C to be a very large value, so let’s imagine we set C to a very large value, may be a hundred thousand, some huge number. Let’s see what the support vector machine will do. If C is very, very large, then when minimizing this optimization objective, we’re going to be highly motivated to choose a value, so that this first term is equal to zero. So let’s try to understand the optimization problem in the context of, what would it take to make this first term in the objective equal to zero, because you know, maybe we’ll set C to some huge constant, and this will hope, this should give us additional intuition about what sort of hypotheses a support vector machine learns. So we saw already that whenever you have a training example with a label of y=1 if you want to make that first term zero, what you need is is to find a value of theta so that theta transpose $x^{(i)}$ is greater than or equal to 1. And similarly, whenever we have an example, with label zero, in order to make sure that the cost, cost zero of Z, in order to make sure that cost is zero we need that theta transpose $x^{(i)}$ is less than or equal to -1. So, if we think of our optimization problem as now, really choosing parameters and show that this first term is equal to zero, what we’re left with is the following optimization problem. We’re going to minimize that first term zero, so C times zero, because we’re going to choose parameters so that’s equal to zero, plus one half and then you know that second term and this first term is ‘C’ times zero, so let’s just cross that out because I know that’s going to be zero. And this will be subject to the constraint that theta transpose $x^{(i)}$ is greater than or equal to one, if $y^{(i)}$ Is equal to one and theta transpose $x^{(i)}$ is less than or equal to minus one whenever you have a negative example and it turns out that when you solve this optimization problem, when you minimize this as a function of the parameters theta you get a very interesting decision boundary.$$\min_\limits{\theta}C\sum_\limits{i=1}^{m}\left[y^{(i)}{\cos}t_{1}\left(\theta^{T}x^{(i)}\right)+\left(1-y^{(i)}\right){\cos}t\left(\theta^{T}x^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{i=1}^{n}\theta^{2}_{j}$$ Concretely, if you look at a data set like this with positive and negative examples, this data is linearly separable and by that, I mean that there exists, you know, a straight line, although there is many a different straight lines, they can separate the positive and negative examples perfectly. For example, here is one green decision boundary that separates the positive and negative examples, but somehow that doesn’t look like a very natural one, right? Or by drawing an even worse one, you know here’s another magenta decision boundary that separates the positive and negative examples but just barely. But neither of those seem like particularly good choices. The Support Vector Machines will instead choose this decision boundary, which I’m drawing in black. And that seems like a much better decision boundary than either of the ones that I drew in magenta or in green. The black line seems like a more robust separator, it does a better job of separating the positive and negative examples. And mathematically, what that does is, this black decision boundary has a larger distance. That distance is called the margin, when I draw up this two extra blue lines, we see that the black decision boundary has some larger minimum distance from any of my training examples, whereas the magenta and the green lines they come awfully close to the training examples. and then that seems to do a less a good job separating the positive and negative classes than my black line. And so this distance is called the margin of the support vector machine and this gives the SVM a certain robustness, because it tries to separate the data with as a large margin as possible. So the support vector machine is sometimes also called a large margin classifier and this is actually a consequence of the optimization problem we wrote down on the previous slide. I know that you might be wondering how is it that the optimization problem I wrote down in the previous slide, how does that lead to this large margin classifier. I know I haven’t explained that yet. And in the next video I’m going to sketch a little bit of the intuition about why that optimization problem gives us this large margin classifier. But this is a useful feature to keep in mind if you are trying to understand what are the sorts of hypothesis that an SVM will choose. That is, trying to separate the positive and negative examples with as big a margin as possible.$$\min_\limits{\theta}C\sum_\limits{i=1}^{m}\left[y^{(i)}{\cos}t_{1}\left(\theta^{T}x^{(i)}\right)+\left(1-y^{(i)}\right){\cos}t\left(\theta^{T}x^{(i)}\right)\right]+\frac{1}{2}\sum_\limits{i=1}^{n}\theta^{2}_{j} \\\min_\limits{\theta} \frac{1}{2}\sum\limits_{j=1}^{n} s.t. \cases{\theta^Tx \ge 1 &amp; if y=1, then left term is 0 \\\theta^Tx \le 0 &amp; if y=-1, then left term is 0}$$ outliersI want to say one last thing about large margin classifiers in this intuition, so we wrote out this large margin classification setting in the case of when C, that regularization concept, was very large, I think I set that to a hundred thousand or something. So given a dataset like this, maybe we’ll choose that decision boundary that separate the positive and negative examples on large margin. (personal note : In order to minimize the cost function as possible as the classifier, SVM , can, it need to choose a proper $\theta$ to make the left term equal to 0 when C is a pretty large constant) Now, the SVM is actually slightly more sophisticated than this large margin view might suggest. And in particular, if all you’re doing is use a large margin classifier then your learning algorithms can be sensitive to outliers, so lets just add an extra positive example like that shown on the screen. If he had one example then it seems as if to separate data with a large margin, maybe I’ll end up learning a decision boundary like that, right? that is the magenta line and it’s really not clear that based on the single outlier based on a single example and it’s really not clear that it’s actually a good idea to change my decision boundary from the black one over to the magenta one. So, if C, if the regularization parameter C were very large, then this is actually what SVM will do, it will change the decision boundary from the black to the magenta one but if C were reasonably small if you were to use the C, not too large then you still end up with this black decision boundary. And of course if the data were not linearly separable so if you had some positive examples in here, or if you had some negative examples in here then the SVM will also do the right thing. And so this picture of a large margin classifier that’s really, that’s really the picture that gives better intuition only for the case of when the regulations parameter C is very large, and just to remind you this corresponds C plays a role similar to one over Lambda, where Lambda is the regularization parameter we had previously. And so it’s only of one over Lambda is very large or equivalently if Lambda is very small that you end up with things like this Magenta decision boundary, but in practice when applying support vector machines, when C is not very very large like that, it can do a better job ignoring the few outliers like here. And also do fine and do reasonable things even if your data is not linearly separable. But when we talk about bias and variance in the context of support vector machines which will do a little bit later, hopefully all of of this trade-offs involving the regularization parameter will become clearer at that time. So I hope that gives some intuition about how this support vector machine functions as a large margin classifier that tries to separate the data with a large margin, technically this picture of this view is true only when the parameter C is very large, which is a useful way to think about support vector machines. There was one missing step in this video which is, why is it that the optimization problem we wrote down on these slides, how does that actually lead to the large margin classifier, I didn’t do that in this video, in the next video I will sketch a little bit more of the math behind that to explain that separate reasoning of how the optimization problem we wrote out results in a large margin classifier. summaryA useful way to think about Support Vector Machines is to think of them as Large Margin Classifiers .If y=1, we want $\Theta^Tx \geq 1$ (not just ≥0)If y=0, we want $\Theta^Tx \leq -1$ (not just &lt;0)Now when we set our constant C to a very large value (e.g. 100,000), our optimizing function will constrain Θ such that the equation A (the summation of the cost of each example) equals 0. We impose the following constraints on Θ:$$\Theta^Tx \geq 1 \text{ if y=1 and } \Theta^Tx \leq -1 \text{ if y=0 .}$$If C is very large, we must choose Θ parameters such that:$$\sum_{i=1}^m y^{(i)}\text{cost}_1(\Theta^Tx) + (1 - y^{(i)})\text{cost}_0(\Theta^Tx) = 0$$This reduces our cost function to:$$ \begin{align} J(\theta) = C \cdot 0 + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j \newline = \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j \end{align}$$Recall the decision boundary from logistic regression (the line separating the positive and negative examples). In SVMs, the decision boundary has the special property that it is as far away as possible from both the positive and the negative examples.The distance of the decision boundary to the nearest example is called the margin . Since SVMs maximize this margin, it is often called a Large Margin Classifier.The SVM will separate the negative and positive examples by a large margin .This large margin is only achieved when C is very large .Data is linearly separable when a straight line can separate the positive and negative examples.If we have outlier examples that we don’t want to affect the decision boundary, then we can reduce C.Increasing and decreasing C is similar to respectively decreasing and increasing $λ$, and can simplify our decision boundary. 03_mathematics-behind-large-margin-classificationIn this video, I’d like to tell you a bit about the math behind large margin classification. This video is optional, so please feel free to skip it. It may also give you better intuition about how the optimization problem of the support vex machine, how that leads to large margin classifiers. In order to get started, let me first remind you of a couple of properties of what vector inner products look like. Let’s say I have two vectors U and V, that look like this. So both two dimensional vectors. Then let’s see what U transpose V looks like. And U transpose V is also called the inner products between the vectors U and V. Use a two dimensional vector, so I can on plot it on this figure. So let’s say that’s the vector U. And what I mean by that is if on the horizontal axis that value takes whatever value U1 is and on the vertical axis the height of that is whatever U2 is the second component of the vector U. Now, one quantity that will be nice to have is the norm of the vector U. So, these are, you know, double bars on the left and right that denotes the norm or length of U. So this just means; really the euclidean length of the vector U. And this is Pythagoras theorem is just equal to U1 squared plus U2 squared square root, right? And this is the length of the vector U. That’s a real number. Just say you know, what is the length of this, what is the length of this vector down here. What is the length of this arrow that I just drew, is the normal view? Now let’s go back and look at the vector V because we want to compute the inner product. So V will be some other vector with, you know, some value V1, V2. And so, the vector V will look like that, towards V like so. Now let’s go back and look at how to compute the inner product between U and V. Here’s how you can do it. Let me take the vector V and project it down onto the vector U. So I’m going to take a orthogonal projection or a 90 degree projection, and project it down onto U like so. And what I’m going to do measure length of this red line that I just drew here. So, I’m going to call the length of that red line P. So, P is the length or is the magnitude of the projection of the vector V onto the vector U. Let me just write that down. So, P is the length of the projection of the vector V onto the vector U. And it is possible to show that unit product U transpose V, that this is going to be equal to P times the norm or the length of the vector U. So, this is one way to compute the inner product. And if you actually do the geometry figure out what P is and figure out what the norm of U is. This should give you the same way, the same answer as the other way of computing unit product. Right. Which is if you take U transpose V then U transposes this U1 U2, its a one by two matrix, 1 times V. And so this should actually give you U1, V1 plus U2, V2. And so the theorem of linear algebra that these two formulas give you the same answer. And by the way, U transpose V is also equal to V transpose U. So if you were to do the same process in reverse, instead of projecting V onto U, you could project U onto V. Then, you know, do the same process, but with the rows of U and V reversed. And you would actually, you should actually get the same number whatever that number is. And just to clarify what’s going on in this equation the norm of U is a real number and P is also a real number. And so U transpose V is the regular multiplication as two real numbers of the length of P times the normal view.Just one last detail, which is if you look at the norm of P, P is actually signed so to the right. And it can either be positive or negative. So let me say what I mean by that, if U is a vector that looks like this and V is a vector that looks like this. So if the angle between U and V is greater than ninety degrees. Then if I project V onto U, what I get is a projection it looks like this and so that length P. And in this case, I will still have that U transpose V is equal to P times the norm of U. Except in this example P will be negative. So, you know, in inner products if the angle between U and V is less than ninety degrees, then P is the positive length for that red line whereas if the angle of this angle of here is greater than 90 degrees then P here will be negative of the length of the super line of that little line segment right over there. So the inner product between two vectors can also be negative if the angle between them is greater than 90 degrees. So that’s how vector inner products work. We’re going to use these properties of vector inner product to try to understand the support vector machine optimization objective over there.Here is the optimization objective for the support vector machine that we worked out earlier. Just for the purpose of this slide I am going to make one simplification or once just to make the objective easy to analyze and what I’m going to do is ignore the indeceptrums.So, we’ll just ignore theta 0 and set that to be equal to 0. To make things easier to plot, I’m also going to set N the number of features to be equal to 2. So, we have only 2 features, X1 and X2. Now, let’s look at the objective function. The optimization objective of the SVM. What we have only two features. When N is equal to 2. This can be written, one half of theta one squared plus theta two squared. Because we only have two parameters, theta one and theta two. What I’m going to do is rewrite this a bit. I’m going to write this as one half of theta one squared plus theta two squared and the square root squared. And the reason I can do that, is because for any number, you know, W, right, the square roots of W and then squared, that’s just equal to W. So square roots and squared should give you the same thing. What you may notice is that this term inside is that’s equal to the norm or the length of the vector theta and what I mean by that is that if we write out the vector theta like this, as you know theta one, theta two. Then this term that I’ve just underlined in red, that’s exactly the length, or the norm, of the vector theta. We are calling the definition of the norm of the vector that we have on the previous line. And in fact this is actually equal to the length of the vector theta, whether you write it as theta zero, theta 1, theta 2. That is, if theta zero is equal to zero, as I assume here. Or just the length of theta 1, theta 2; but for this line I am going to ignore theta 0. So let me just, you know, treat theta as this, let me just write theta, the normal theta as this theta 1, theta 2 only, but the math works out either way, whether we include theta zero here or not. So it’s not going to matter for the rest of our derivation. And so finally this means that my optimization objective is equal to one half of the norm of theta squared. Support vector machine is doing in the optimization objective is it’s minimizing the squared norm of the square length of the parameter vector theta. Now what I’d like to do is look at these terms, theta transpose X and understand better what they’re doing. So given the parameter vector theta and given and example x, what is this is equal to? And on the previous slide, we figured out what U transpose V looks like, with different vectors U and V. And so we’re going to take those definitions, you know, with theta and X(i) playing the roles of U and V. And let’s see what that picture looks like. So, let’s say I plot. Let’s say I look at just a single training example. Let’s say I have a positive example the drawing was across there and let’s say that is my example X(i), what that really means is plotted on the horizontal axis some value X(i) 1 and on the vertical axis X(i) 2. That’s how I plot my training examples. And although we haven’t been really thinking of this as a vector, what this really is, this is a vector from the origin from 0, 0 out to the location of this training example. And now let’s say we have a parameter vector and I’m going to plot that as vector, as well. What I mean by that is if I plot theta 1 here and theta 2 there so what is the inner product theta transpose X(i). While using our earlier method, the way we compute that is we take my example and project it onto my parameter vector theta. And then I’m going to look at the length of this segment that I’m coloring in, in red. And I’m going to call that P superscript I to denote that this is a projection of the i-th training example onto the parameter vector theta. And so what we have is that theta transpose X(i) is equal to following what we have on the previous slide, this is going to be equal to P times the length of the norm of the vector theta. And this is of course also equal to theta 1 x1 plus theta 2 x2. So each of these is, you know, an equally valid way of computing the inner product between theta and X(i). Okay. So where does this leave us? What this means is that, this constrains that theta transpose X(i) be greater than or equal to one or less than minus one. What this means is that it can replace the use of constraints that P(i) times X be greater than or equal to one. Because theta transpose X(i) is equal to P(i) times the norm of theta. So writing that into our optimization objective. This is what we get where I have, instead of theta transpose X(i), I now have this P(i) times the norm of theta. And just to remind you we worked out earlier too that this optimization objective can be written as one half times the norm of theta squared. So, now let’s consider the training example that we have at the bottom and for now, continuing to use the simplification that theta 0 is equal to 0. Let’s see what decision boundary the support vector machine will choose. Here’s one option, let’s say the support vector machine were to choose this decision boundary. This is not a very good choice because it has very small margins. This decision boundary comes very close to the training examples. Let’s see why the support vector machine will not do this. For this choice of parameters it’s possible to show that the parameter vector theta is actually at 90 degrees to the decision boundary. And so, that green decision boundary corresponds to a parameter vector theta that points in that direction. And by the way, the simplification that theta 0 equals 0 that just means that the decision boundary must pass through the origin, (0,0) over there. So now, let’s look at what this implies for the optimization objective. Let’s say that this example here. Let’s say that’s my first example, you know, X1. If we look at the projection of this example onto my parameters theta. That’s the projection. And so that little red line segment. That is equal to P1. And that is going to be pretty small, right. And similarly, if this example here, if this happens to be X2, that’s my second example. Then, if I look at the projection of this this example onto theta. You know. Then, let me draw this one in magenta. This little magenta line segment, that’s going to be P2. That’s the projection of the second example onto my, onto the direction of my parameter vector theta which goes like this. And so, this little projection line segment is getting pretty small. P2 will actually be a negative number, right so P2 is in the opposite direction. This vector has greater than 90 degree angle with my parameter vector theta, it’s going to be less than 0. And so what we’re finding is that these terms P(i) are going to be pretty small numbers. So if we look at the optimization objective and see, well, for positive examples we need P(i) times the norm of theta to be bigger than either one. But if P(i) over here, if P1 over here is pretty small, that means that we need the norm of theta to be pretty large, right? If P1 of theta is small and we want P1 you know times in all of theta to be bigger than either one, well the only way for that to be true for the profit that these two numbers to be large if P1 is small, as we said we want the norm of theta to be large. And similarly for our negative example, we need P2 times the norm of theta to be less than or equal to minus one. And we saw in this example already that P2 is going pretty small negative number, and so the only way for that to happen as well is for the norm of theta to be large, but what we are doing in the optimization objective is we are trying to find a setting of parameters where the norm of theta is small, and so you know, so this doesn’t seem like such a good direction for the parameter vector and theta. In contrast, just look at a different decision boundary. Here, let’s say, this SVM chooses that decision boundary. Now the is going to be very different. If that is the decision boundary, here is the corresponding direction for theta. So, with the direction boundary you know, that vertical line that corresponds to it is possible to show using linear algebra that the way to get that green decision boundary is have the vector of theta be at 90 degrees to it, and now if you look at the projection of your data onto the vector x, lets say its before this example is my example of x1. So when I project this on to x, or onto theta, what I find is that this is P1. That length there is P1. The other example, that example is and I do the same projection and what I find is that this length here is a P2 really that is going to be less than 0. And you notice that now P1 and P2, these lengths of the projections are going to be much bigger, and so if we still need to enforce these constraints that P1 of the norm of theta is phase number one because P1 is so much bigger now. The normal can be smaller. And so, what this means is that by choosing the decision boundary shown on the right instead of on the left, the SVM can make the norm of the parameters theta much smaller. So, if we can make the norm of theta smaller and therefore make the squared norm of theta smaller, which is why the SVM would choose this hypothesis on the right instead. And this is how the SVM gives rise to this large margin certification effect. Mainly, if you look at this green line, if you look at this green hypothesis we want the projections of my positive and negative examples onto theta to be large, and the only way for that to hold true this is if surrounding the green line. There’s this large margin, there’s this large gap that separates positive and negative examples is really the magnitude of this gap. The magnitude of this margin is exactly the values of P1, P2, P3 and so on. And so by making the margin large, by these tyros P1, P2, P3 and so on. that’s the SVM can end up with a smaller value for the norm of theta which is what it is trying to do in the objective. And this is why this machine ends up with enlarge margin classifiers because it’s trying to maximize the norm of these P1 which is the distance from the training examples to the decision boundary. Finally, we did this whole derivation using this simplification that the parameter theta 0 must be equal to 0. The effect of that as I mentioned briefly, is that if theta 0 is equal to 0 what that means is that we are entertaining decision boundaries that pass through the origins of decision boundaries pass through the origin like that, if you allow theta zero to be non 0 then what that means is that you entertain the decision boundaries that did not cross through the origin, like that one I just drew. And I’m not going to do the full derivation that. It turns out that this same large margin proof works in pretty much in exactly the same way. And there’s a generalization of this argument that we just went through them long ago through that shows that even when theta 0 is non 0, what the SVM is trying to do when you have this optimization objective. Which again corresponds to the case of when C is very large. But it is possible to show that, you know, when theta is not equal to 0 this support vector machine is still finding is really trying to find the large margin separator that between the positive and negative examples. So that explains how this support vector machine is a large margin classifier. In the next video we will start to talk about how to take some of these SVM ideas and start to apply them to build a complex nonlinear classifiers. summaryVector Inner ProductSay we have two vectors, u and v:$$\begin{align} u = \begin{bmatrix} u_1 \newline u_2 \end{bmatrix} &amp; v = \begin{bmatrix} v_1 \newline v_2 \end{bmatrix} \end{align}$$The length of vector v is denoted $||v||$, and it describes the line on a graph from origin (0,0) to $(v_1,v_2)$.The length of vector v can be calculated with $\sqrt{v_1^2 + v_2^2}$ by the Pythagorean theorem.The projection of vector v onto vector u is found by taking a right angle from u to the end of v, creating a right triangle.p= length of projection of v onto the vector u.$$u^Tv= p \cdot ||u||$$Note that $u^Tv = ||u|| \cdot ||v|| \cos \theta$ where θ is the angle between u and v. Also, $p = ||v|| \cos \theta$. If you substitute p for $||v|| \cos \theta$, you get $u^Tv= p \cdot ||u||$.So the product $u^Tv$ is equal to the length of the projection times the length of vector u.In our example, since u and v are vectors of the same length, $u^Tv = v^Tu$.$$u^Tv = v^Tu = p \cdot ||u|| = u_1v_1 + u_2v_2$$If the angle between the lines for v and u is greater than 90 degrees , then the projection p will be negative .$$\begin{align}&amp;\min_\Theta \dfrac{1}{2}\sum_{j=1}^n \Theta_j^2 \newline&amp;= \dfrac{1}{2}(\Theta_1^2 + \Theta_2^2 + \dots + \Theta_n^2) \newline&amp;= \dfrac{1}{2}(\sqrt{\Theta_1^2 + \Theta_2^2 + \dots + \Theta_n^2})^2 \newline&amp;= \dfrac{1}{2}||\Theta ||^2 \newline\end{align}$$We can use the same rules to rewrite $\Theta^Tx^{(i)}$:$$\Theta^Tx^{(i)} = p^{(i)} \cdot ||\Theta || = \Theta_1x_1^{(i)} + \Theta_2x_2^{(i)} + \dots + \Theta_n x_n^{(i)}$$So we now have a new optimization objective by substituting $p^{(i)} \cdot ||\Theta ||$ in for $\Theta^Tx^{(i)}$:If y=1, we want $p^{(i)} \cdot ||\Theta || \geq 1$If y=0, we want $p^{(i)} \cdot ||\Theta || \leq -1$The reason this causes a “large margin” is because: the vector for Θ is perpendicular to the decision boundary. In order for our optimization objective (above) to hold true, we need the absolute value of our projections $p^{(i)}$ to be as large as possible.If $\Theta_0 =0$, then all our decision boundaries will intersect (0,0). If $\Theta_0 \neq 0$, the support vector machine will still find a large margin for the decision boundary. 02_kernels01_kernels-iIn this video, I’d like to start adapting support vector machines in order to develop complex nonlinear classifiers. The main technique for doing that is something called kernels. Let’s see what this kernels are and how to use them. If you have a training set that looks like this, and you want to find a nonlinear decision boundary to distinguish the positive and negative examples, maybe a decision boundary that looks like that. One way to do so is to come up with a set of complex polynomial features, right? So, set of features that looks like this, so that you end up with a hypothesis X that predicts 1 if you know that theta 0 and plus theta 1 X1 plus dot dot dot all those polynomial features is greater than 0, and predict 0, otherwise.$$ h(\theta)=\theta_0+\theta_1x_1+\theta_2x_2+\theta_3x_1x_2+\theta_4x_1^2+\theta_5x_2^2+\cdots$$ And another way of writing this, to introduce a level of new notation that I’ll use later, is that we can think of a hypothesis as computing a decision boundary using this. So, theta 0 plus theta 1 f1 plus theta 2, f2 plus theta 3, f3 plus and so on.$$ h(\theta)=\theta_0+\theta_1f_1+\theta_2f_2+\theta_3f_3+\theta_4f_4+\cdots$$ Where I’m going to use this new denotation f1, f2, f3 and so on to denote these new sort of features that I’m computing, so f1 is just X1, f2 is equal to X2, f3 is equal to this one here.$$ f_1=x_1,f_2=x_2,f_3=x_1x_2,f_4=x_1^2,f_5=x_2^2,\cdots$$we seen previously that coming up with these high order polynomials is one way to come up with lots more features, the question is, is there a different choice of features or is there better sort of features than this high order polynomials because you know it’s not clear that this high order polynomial is what we want, and what we talked about computer vision talk about when the input is an image with lots of pixels. We also saw how using high order polynomials becomes very computationally expensive because there are a lot of these higher order polynomial terms. So, is there a different or a better choice of the features that we can use to plug into this sort of hypothesis form. So, here is one idea for how to define new features f1, f2, f3. On this line I am going to define only three new features, but for real problems we can get to define a much larger number. But here’s what I’m going to do in this phase of features X1, X2, and I’m going to leave X0 out of this, the interceptor X0, but in this phase X1 X2, I’m going to just, you know, manually pick a few points, and then call these points l1, we are going to pick a different point, let’s call that l2 and let’s pick the third one and call this one l3, and for now let’s just say that I’m going to choose these three points manually. I’m going to call these three points landmarks, so line landmark one, two, three. What I’m going to do is define my new features as follows, given an example X, let me define my first feature f1 to be some measure of the similarity between my training example X and my first landmark and this specific formula that I’m going to use to measure similarity is going to be this $f_i = similarity(x^{(1)}, l^{(1)}) = e^{(-\frac{||x^{(1)}-l^{(1)}||^2}{2δ^2})}$. So, depending on whether or not you watched the previous optional video, this notation, you know, this is the length of the vector W (=$x^{(1)}-l^{(1)}$). And so, this thing here, this $||x-l^{(1)}||^2$, this is actually just the euclidean distance squared, is the euclidean distance between the point x and the landmark l1. We will see more about this later. But that’s my first feature, and my second feature f2 is going to be, you know, similarity function that measures how similar X is to l2 and the game is going to be defined as the following function. This is E to the minus of the square of the euclidean distance between X and the second landmark, that is what the enumerator is and then divided by 2 sigma squared and similarly f3 is, you know, similarity between X and l3, which is equal to, again, similar formula. And what this similarity function is, the mathematical term for this, is that this is going to be a kernel function. And the specific kernel I’m using here, this is actually called a Gaussian kernel. And so this formula, this particular choice of similarity function is called a Gaussian kernel. But the way the terminology goes is that, you know, in the abstract these different similarity functions are called kernels and we can have different similarity functions and the specific example I’m giving here is called the Gaussian kernel. We’ll see other examples of other kernels. But for now just think of these as similarity functions. And so, instead of writing similarity between X and l, sometimes we also write this a kernel denoted you know, lower case k between x and one of my landmarks all right. So let’s see what this kernel actually do and why these sorts of similarity functions, why these expressions might make sense. So let’s take my first landmark. My landmark l1, which is one of those points I chose on my figure just now. So the similarity of the kernel between x and l1 is given by this expression. Just to make sure, you know, we are on the same page about what the numerator term is, the numerator can also be written as a sum from j equals 1 through N on sort of the distance. So this is the component wise distance between the vector X and the vector l. And again for the purpose of these slides I’m ignoring X0. So just ignoring the intercept term X0, which is always equal to 1. So, you know, this is how you compute the kernel with similarity between X and a landmark. So let’s see what this function does. Suppose X is close to one of the landmarks. Then this euclidean distance formula and the numerator will be close to 0, right. So, that is this term here, the distance was great, the distance using X and 0 will be close to zero, and so f1, this is a simple feature, will be approximately E to the minus 0 and then the numerator squared over 2 is equal to squared so that E to the 0, E to minus 0, E to 0 is going to be close to one. And I’ll put the approximation symbol here because the distance may not be exactly 0, but if X is closer to landmark this term will be close to 0 and so f1 would be close 1. Conversely, if X is far from 01 then this first feature f1 will be E to the minus of some large number squared, divided divided by two sigma squared and E to the minus of a large number is going to be close to 0. So what these features do is they measure how similar X is from one of your landmarks and the feature f is going to be close to one when X is close to your landmark and is going to be 0 or close to zero when X is far from your landmark. Each of these landmarks. On the previous line, I drew three landmarks, l1, l2, l3. Each of these landmarks, defines a new feature f1, f2 and f3. That is, given the the training example X, we can now compute three new features: f1, f2, and f3, given, you know, the three landmarks that I wrote just now. But first, let’s look at this exponentiation function, let’s look at this similarity function and plot in some figures and just, you know, understand better what this really looks like. For this example, let’s say I have two features X1 and X2. And let’s say my first landmark, l1 is at a location, $\begin{bmatrix}3\\5\end{bmatrix}$. So and let’s say I set sigma squared equals one for now. If I plot what this feature looks like, what I get is this figure. So the vertical axis, the height of the surface is the value of f1 and down here on the horizontal axis are, if I have some training example, and there is x1 and there is x2. Given a certain training example, the training example here which shows the value of x1 and x2 at a height above the surface, shows the corresponding value of f1 and down below this is the same figure I had showed, using a quantifiable plot, with x1 on horizontal axis, x2 on horizontal axis and so, this figure on the bottom is just a contour plot of the 3D surface. You notice that when X is equal to 3 5 exactly, then we the f1 takes on the value 1, because that’s at the maximum and X moves away as X goes further away then this feature takes on values that are close to 0. And so, this is really a feature, f1 measures, you know, how close X is to the first landmark and if varies between 0 and one depending on how close X is to the first landmark l1. Now the other was due on this slide is show the effects of varying this parameter sigma squared. So, sigma squared is the parameter of the Gaussian kernel and as you vary it, you get slightly different effects. Let’s set sigma squared to be equal to 0.5 and see what we get. We set sigma square to 0.5, what you find is that the kernel looks similar, except for the width of the bump becomes narrower. The contours shrink a bit too. So if sigma squared equals to 0.5 then as you start from X equals $\begin{bmatrix}3\\5\end{bmatrix}$ and as you move away, then the feature f1 falls to zero much more rapidly and conversely, if you has increase since where three in that case and as I move away from, you know l. So this point here is really l, right, that’s l1 is at location $\begin{bmatrix}3\\5\end{bmatrix}$, right. So it’s shown up here. And if sigma squared is large, then as you move away from l1, the value of the feature falls away much more slowly. So, given this definition of the features, let’s see what source of hypothesis we can learn. Given the training example X, we are going to compute these features f1, f2, f3 and a hypothesis is going to predict one when theta 0 plus theta 1 f1 plus theta 2 f2, and so on is greater than or equal to 0. For this particular example, let’s say that I’ve already found a learning algorithm and let’s say that, you know, somehow I ended up with these values of the parameter. So if theta 0 equals minus 0.5, theta 1 equals 1, theta 2 equals 1, and theta 3 equals 0 And what I want to do is consider what happens if we have a training example that takes has location at this magenta dot, right where I just drew this dot over here. So let’s say I have a training example X, what would my hypothesis predict? Well, If I look at this formula. Because my training example X is close to l1, we have that f1 is going to be close to 1 the because my training example X is far from l2 and l3 I have that, you know, f2 would be close to 0 and f3 will be close to 0. So, if I look at that formula, I have theta 0 plus theta 1 times 1 plus theta 2 times some value. Not exactly 0, but let’s say close to 0. Then plus theta 3 times something close to 0. And this is going to be equal to plugging in these values now. So, that gives minus 0.5 plus 1 times 1 which is 1, and so on. Which is equal to 0.5 which is greater than or equal to 0. So, at this point, we’re going to predict Y equals 1, because that’s greater than or equal to zero.$$h_θ(x) = θ_0+θ_1f_1+θ_2f_2+θ_3f_3=-0.5+11+10+00=0.5≥0$$Now let’s take a different point. Now lets’ say I take a different point, I’m going to draw this one in a different color, **in cyan say, for a point out there, if that were my training example X, then if you make a similar computation, you find that f1, f2, f3 are all going to be close to 0. And so, we have theta 0 plus theta1, f1, plus so on and this will be about equal to minus 0.5, because theta 0 is minus 0.5 and f1, f2, f3 are all zero. So this will be minus 0.5, this is less than zero. And so, at this point out there, we’re going to predict Y equals zero.$$h_θ(x) = θ_0+θ_1f_1+θ_2f_2+θ_3f_3=-0.5+10+10+00=-0.5&lt;0$$And if you do this yourself for a range of different points, be sure to convince yourself that if you have a training example that’s close to L2, say, then at this point we’ll also predict Y equals one. And in fact, what you end up doing is, you know, if you look around this boundary, this space, what we’ll find is that for points near l1 and l2 we end up predicting positive. And for points far away from l1 and l2, that’s for points far away from these two landmarks, we end up predicting that the class is equal to 0. As so, what we end up doing,is that the decision boundary of this hypothesis would end up looking something like this where inside this red decision boundary would predict Y equals 1 and outside we predict Y equals 0. And so this is how with this definition of the landmarks and of the kernel function. We can learn pretty complex non-linear decision boundary, like what I just drew where we predict positive when we’re close to either one of the two landmarks. And we predict negative when we’re very far away from any of the landmarks. And so this is part of the idea of kernels of and how we use them with the support vector machine, which is that we define these extra features using landmarks and similarity functions to learn more complex nonlinear classifiers. So hopefully that gives you a sense of the idea of kernels and how we could use it to define new features for the Support Vector Machine. ** But there are a couple of questions that we haven’t answered yet. One is, how do we get these landmarks? How do we choose these landmarks? And another is, what other similarity functions, if any, can we use other than the one we talked about, which is called the Gaussian kernel. In the next video we give answers to these questions and put everything together to show how support vector machines with kernels can be a powerful way to learn complex nonlinear functions. summaryKernels allow us to make complex, non-linear classifiers using Support Vector Machines.Given x, compute new feature depending on proximity to landmarks $l^{(1)},\ l^{(2)},\ l^{(3)}$.To do this, we find the “similarity” of x and some landmark $l^{(i)}$:$$f_i = similarity(x, l^{(i)}) = \exp(-\dfrac{||x - l^{(i)}||^2}{2\sigma^2})$$This “similarity” function is called a Gaussian Kernel . It is a specific example of a kernel.The similarity function can also be written as follows:$$f_i = similarity(x, l^{(i)}) = \exp(-\dfrac{\sum^n_{j=1}(x_j-l_j^{(i)})^2}{2\sigma^2})$$There are a couple properties of the similarity function:If $x \approx l^{(i)}$, then $f_i = \exp(-\dfrac{\approx 0^2}{2\sigma^2}) \approx 1$If x is far from $l^{(i)}$, then $f_i = \exp(-\dfrac{(large\ number)^2}{2\sigma^2}) \approx 0$In other words, if x and the landmark are close, then the similarity will be close to 1, and if x and the landmark are far away from each other, the similarity will be close to 0.Each landmark gives us the features in our hypothesis:$$\begin{align}l^{(1)} \rightarrow f_1 \newline l^{(2)} \rightarrow f_2 \newline l^{(3)} \rightarrow f_3 \newline\dots \newline h_\Theta(x) = \Theta_1f_1 + \Theta_2f_2 + \Theta_3f_3 + \dots\end{align}$$$\sigma^2$ is a parameter of the Gaussian Kernel, and it can be modified to increase or decrease the drop-off of our feature $f_i$. Combined with looking at the values inside Θ, we can choose these landmarks to get the general shape of the decision boundary. 02_kernels-iiIn the last video, we started to talk about the kernels idea and how it can be used to define new features for the support vector machine. In this video, I’d like to throw in some of the missing details and, also, say a few words about how to use these ideas in practice. Such as, how they pertain to, for example, the bias variance trade-off in support vector machines. In the last video, I talked about the process of picking a few landmarks. You know, l1, l2, l3 and that allowed us to define the similarity function also called the kernel or in this example if you have this similarity function this is a Gaussian kernel. And that allowed us to build this form of a hypothesis function. where do we get these landmarks fromWhere do we get l1, l2, l3 from? And it seems, also, that for complex learning problems, maybe we want a lot more landmarks than just three of them that we might choose by hand. So in practice this is how the landmarks are chosen which is that given the machine learning problem. We have some data set of some some positive and negative examples. So, this is the idea here which is that we’re gonna take the examples and for every training example that we have, we are just going to call it. We’re just going to put landmarks as exactly the same locations as the training examples. So if I have one training example if that is x1, well then I’m going to choose this is my first landmark to be at exactly the same location as my first training example. And if I have a different training example x2. Well we’re going to set the second landmark to be the location of my second training example. On the figure on the right, I used red and blue dots just as illustration, the color of this figure, the color of the dots on the figure on the right is not significant. But what I’m going to end up with using this method is I’m going to end up with m landmarks of l1, l2 down to l(m) if I have m training examples with one landmark per location of my per location of each of my training examples. And this is nice because it is saying that my features are basically going to measure how close an example is to one of the things I saw in my training set. So, just to write this outline a little more concretely, given m training examples, I’m going to choose the the location of my landmarks to be exactly near the locations of my m training examples. When you are given example x, and in this example x can be something in the training set, it can be something in the cross validation set, or it can be something in the test set. Given an example x we are going to compute, you know, these features as so f1, f2, and so on. Where l1 is actually equal to x1 and so on. And these then give me a feature vector. So let me write f as the feature vector. I’m going to take these f1, f2 and so on, and just group them into feature vector. Take those down to fm. And, you know, just by convention. If we want, we can add an extra feature f0, which is always equal to 1. So this plays a role similar to what we had previously. For x0, which was our interceptor. So, for example, if we have a training example x(i), y(i), the features we would compute for this training example will be as follows: given x(i), we will then map it to, you know, f1(i). Which is the similarity. I’m going to abbreviate as SIM instead of writing out the whole word similarity, right? And f2(i) equals the similarity between x(i) and l2, and so on, down to fm(i) equals the similarity between x(i) and l(m). And somewhere in the middle. Somewhere in this list, you know, at the i-th component, I will actually have one feature component which is f subscript i(i), which is going to be the similarity between x and l(i). Where l(i) is equal to x(i), and so you know f(i) is just going to be the similarity between x and itself. And if you’re using the Gaussian kernel this is actually e to the minus 0 over 2 sigma squared and so, this will be equal to 1 and that’s okay. So one of my features for this training example is going to be equal to 1. And then similar to what I have above. I can take all of these m features and group them into a feature vector. So instead of representing my example, using, you know, x(i) which is this what R(n) plus one dimensional vector. Depending on whether you can set terms, is either R(n) or R(n) plus 1. We can now instead represent my training example using this feature vector f. I am going to write this f superscript i. Which is going to be taking all of these things and stacking them into a vector. So, $f_1^{(i)}$ down to $f_m^{(i)}$ and if you want and well, usually we’ll also add this $f_0^{(i)}$ , where $f_0^{(i)}$ is equal to 1. And so this vector here gives me my new feature vector with which to represent my training example. So given these kernels and similarity functions, here’s how we use a simple vector machine. If you already have a learning set of parameters theta, then if you given a value of x and you want to make a prediction. What we do is we compute the features f, which is now an $R^{m}$ plus 1 dimensional feature vector. And we have m here because we have m training examples and thus m landmarks and what we do is we predict 1 if theta transpose f is greater than or equal to 0. Right. So, if theta transpose f, of course, that’s just equal to theta 0, f0 plus theta 1, f1 plus dot dot dot, plus theta m f(m). And so my parameter vector theta is also now going to be an m plus 1 dimensional vector. And we have m here because where the number of landmarks is equal to the training set size. So m was the training set size and now, the parameter vector theta is going to be m plus one dimensional. So that’s how you make a prediction if you already have a setting for the parameter’s theta. How do you get the parameter’s theta? Well you do that using the SVM learning algorithm, and specifically what you do is you would solve this minimization problem. You’ve minimized the parameter’s theta of C times this cost function which we had before. Only now, instead of looking there instead of making predictions using theta transpose x(i) using our original features, x(i). Instead we’ve taken the features x(i) and replace them with a new features so we are using theta transpose f(i) to make a prediction on the i’th training examples and we see that, you know, in both places here and it’s by solving this minimization problem that you get the parameters for your Support Vector Machine. $$minJ(θ)=min C[\sum_{i=1}^{m} y^{(i)}Cost_1(θ^Tf^{(i)}) + (1-y^{(i)})Cost_0(θ^Tf^{(i)}) ] + \frac{1}{2}\sum_{j=1}^{n}θ_j^2$$And one last detail is because this optimization problem we really have n equals m features. That is here. The number of features we have. Really, the effective number of features we have is dimension of f. So that n is actually going to be equal to m. So, if you want to, you can think of this as a sum, this really is a sum from j equals 1 through m. And then one way to think about this, is you can think of it as n being equal to m, because if f isn’t a new feature, then we have m plus 1 features, with the plus 1 coming from the interceptor. And here, we still do sum from j equal 1 through n, because similar to our earlier videos on regularization, we still do not regularize the parameter theta zero, which is why this is a sum for j equals 1 through m instead of j equals zero though m. So that’s the support vector machine learning algorithm. That’s one sort of, mathematical detail aside that I should mention, which is that in the way the support vector machine is implemented, this last term is actually done a little bit differently. So you don’t really need to know about this last detail in order to use support vector machines, and in fact the equations that are written down here should give you all the intuitions that should need. But in the way the support vector machine is implemented, you know, that term, the sum of j of theta j squared right? Another way to write this is this can be written as theta transpose theta if we ignore the parameter theta 0. So theta 1 down to theta m. Ignoring theta 0. Then this sum of j of theta j squared that this can also be written theta transpose theta. SVM thetas And what most support vector machine implementations do is actually replace this theta transpose theta, will instead, theta transpose times some matrix inside, that depends on the kernel you use, times theta. And so this gives us a slightly different distance metric. We’ll use a slightly different measure instead of minimizing exactly the norm of theta squared means that minimize something slightly similar to it. That’s like a rescale version of the parameter vector theta that depends on the kernel. But this is kind of a mathematical detail. That allows the support vector machine software to run much more efficiently. And the reason the support vector machine does this is with this modification. It allows it to scale to much bigger training sets. Because for example, if you have a training set with 10,000 training examples. Then, you know, the way we define landmarks, we end up with 10,000 landmarks. And so theta becomes 10,000 dimensional. And maybe that works, but when m becomes really, really big then solving for all of these parameters, you know, if m were 50,000 or a 100,000 then solving for all of these parameters can become expensive for the support vector machine optimization software, thus solving the minimization problem that I drew here. So kind of as mathematical detail, which again you really don’t need to know about. It actually modifies that last term a little bit to optimize something slightly different than just minimizing the norm squared of theta squared, of theta. But if you want, you can feel free to think of this as an kind of an implementational detail that does change the objective a bit, but is done primarily for reasons of computational efficiency, so usually you don’t really have to worry about this. And by the way, in case your wondering why we don’t apply the kernel’s idea to other algorithms as well like logistic regression, it turns out that if you want, you can actually apply the kernel’s idea and define the source of features using landmarks and so on for logistic regression. But the computational tricks that apply for support vector machines don’t generalize well to other algorithms like logistic regression. And so, using kernels with logistic regression is going too very slow, whereas, because of computational tricks, like that embodied and how it modifies this and the details of how the support vector machine software is implemented, support vector machines and kernels tend go particularly well together. Whereas, logistic regression and kernels, you know, you can do it, but this would run very slowly. And it won’t be able to take advantage of advanced optimization techniques that people have figured out for the particular case of running a support vector machine with a kernel.But all this pertains only to how you actually implement software to minimize the cost function. I will say more about that in the next video, but you really don’t need to know about how to write software to minimize this cost function because you can find very good off the shelf software for doing so. And just as, you know, I wouldn’t recommend writing code to invert a matrix or to compute a square root, I actually do not recommend writing software to minimize this cost function yourself, but instead to use off the shelf software packages that people have developed and so those software packages already embody these numerical optimization tricks, so you don’t really have to worry about them. SVM regularization parametersBut one other thing that is worth knowing about is when you’re applying a support vector machine, how do you choose the parameters of the support vector machine? And the last thing I want to do in this video is say a little word about the bias and variance trade offs when using a support vector machine. When using an SVM, one of the things you need to choose is the parameter C which was in the optimization objective, and you recall that C played a role similar to 1 over lambda, where lambda was the regularization parameter we had for logistic regression. So, if you have a large value of C, this corresponds to what we have back in logistic regression, of a small value of lambda meaning of not using much regularization. And if you do that, you tend to have a hypothesis with lower bias and higher variance. Whereas if you use a smaller value of C then this corresponds to when we are using logistic regression with a large value of lambda and that corresponds to a hypothesis with higher bias and lower variance. And so, hypothesis with large C has a higher variance, and is more prone to overfitting, whereas hypothesis with small C has higher bias and is thus more prone to underfitting. So this parameter C is one of the parameters we need to choose. The other one is the parameter sigma squared, which appeared in the Gaussian kernel. So if the Gaussian kernel sigma squared is large, then in the similarity function, which was this you know E to the minus x minus landmark varies squared over 2 sigma squared. In this one of the example; If I have only one feature, x1, if I have a landmark there at that location, if sigma squared is large, then, you know, the Gaussian kernel would tend to fall off relatively slowly and so this would be my feature f(i), and so this would be smoother function that varies more smoothly, and so this will give you a hypothesis with higher bias and lower variance, because the Gaussian kernel that falls off smoothly, you tend to get a hypothesis that varies slowly, or varies smoothly as you change the input x. Whereas in contrast, if sigma squared was small and if that’s my landmark given my 1 feature x1, you know, my Gaussian kernel, my similarity function, will vary more abruptly. And in both cases I’d pick out 1, and so if sigma squared is small, then my features vary less smoothly. So if it’s just higher slopes or higher derivatives here. And using this, you end up fitting hypotheses of lower bias and you can have higher variance. And if you look at this week’s points exercise, you actually get to play around with some of these ideas yourself and see these effects yourself. So, that was the support vector machine with kernels algorithm. And hopefully this discussion of bias and variance will give you some sense of how you can expect this algorithm to behave as well. summaryOne way to get the landmarks is to put them in the exact same locations as all the training examples. This gives us m landmarks, with one landmark per training example.Given example x:$f_1 = similarity(x,l^{(1)}), f_2 = similarity(x,l^{(2)}), f_3 = similarity(x,l^{(3)})$, and so on.This gives us a “feature vector,” $f_{(i)}$ of all our features for example $x_{(i)}$. We may also set $f_0 = 1$ to correspond with $Θ_0$. Thus given training example $x_{(i)}$:$$x^{(i)} \rightarrow \begin{bmatrix}f_1^{(i)} = similarity(x^{(i)}, l^{(1)}) \\ f_2^{(i)} = similarity(x^{(i)}, l^{(2)}) \\ \vdots \\ f_m^{(i)} = similarity(x^{(i)}, l^{(m)}) \\ \end{bmatrix}$$Now to get the parameters Θ we can use the SVM minimization algorithm but with $f^{(i)}$ substituted in for $x^{(i)}$:$$\min_{\Theta} C \sum_{i=1}^m y^{(i)}\text{cost}_1(\Theta^Tf^{(i)}) + (1 - y^{(i)})\text{cost}_0(\theta^Tf^{(i)}) + \dfrac{1}{2}\sum_{j=1}^n \Theta^2_j$$Using kernels to generate f(i) is not exclusive to SVMs and may also be applied to logistic regression. However, because of computational optimizations on SVMs, kernels combined with SVMs is much faster than with other algorithms, so kernels are almost always found combined only with SVMs.Choosing SVM ParametersChoosing C (recall that $C = \dfrac{1}{\lambda}$If C is large, then we get higher variance/lower biasIf C is small, then we get lower variance/higher biasThe other parameter we must choose is $σ^2$ from the Gaussian Kernel function:With a large $σ^2$, the features fi vary more smoothly, causing higher bias and lower variance.With a small $σ^2$, the features fi vary less smoothly, causing lower bias and higher variance.Using An SVMThere are lots of good SVM libraries already written. A. Ng often uses ‘liblinear’ and ‘libsvm’. In practical application, you should use one of these libraries rather than rewrite the functions.In practical application, the choices you do need to make are:Choice of parameter CChoice of kernel (similarity function)No kernel (“linear” kernel) – gives standard linear classifierChoose when n is large and when m is smallGaussian Kernel (above) – need to choose $σ^2$Choose when n is small and m is largeThe library may ask you to provide the kernel function.Note: do perform feature scaling before using the Gaussian Kernel.Note: not all similarity functions are valid kernels. They must satisfy “Mercer’s Theorem” which guarantees that the SVM package’s optimizations run correctly and do not diverge.You want to train C and the parameters for the kernel function using the training and cross-validation datasets.Multi-class ClassificationMany SVM libraries have multi-class classification built-in.You can use the one-vs-all method just like we did for logistic regression, where $$y \in {1,2,3,\dots,K}$$ with $$\Theta^{(1)}, \Theta^{(2)}, \dots,\Theta{(K)}$$. We pick class i with the largest $$(\Theta^{(i)})^Tx$$.Logistic Regression vs. SVMsIf n is large (relative to m), then use logistic regression, or SVM without a kernel (the “linear kernel”)If n is small and m is intermediate, then use SVM with a Gaussian KernelIf n is small and m is large, then manually create/add more features, then use logistic regression or SVM without a kernel.In the first case, we don’t have enough examples to need a complicated polynomial hypothesis. In the second example, we have enough examples that we may need a complex non-linear hypothesis. In the last case, we want to increase our features so that logistic regression becomes applicable.Note : a neural network is likely to work well for any of these situations, but may be slower to train.Additional references“An Idiot’s Guide to Support Vector Machines”: http://web.mit.edu/6.034/wwwbob/svm-notes-long-08.pdf 03_svms-in-practiceSo far we’ve been talking about SVMs in a fairly abstract level. In this video I’d like to talk about what you actually need to do in order to run or to use an SVM. The support vector machine algorithm poses a particular optimization problem. But as I briefly mentioned in an earlier video, I really do not recommend writing your own software to solve for the parameter’s theta yourself. So just as today, very few of us, or maybe almost essentially none of us would think of writing code ourselves to invert a matrix or take a square root of a number, and so on. We just, you know, call some library function to do that. In the same way, the software for solving the SVM optimization problem is very complex, and there have been researchers that have been doing essentially numerical optimization research for many years. So you come up with good software libraries and good software packages to do this. And then strongly recommend just using one of the highly optimized software libraries rather than trying to implement something yourself. And there are lots of good software libraries out there. The two that I happen to use the most often are the linear SVM but there are really lots of good software libraries for doing this that you know, you can link to many of the major programming languages that you may be using to code up learning algorithm. Even though you shouldn’t be writing your own SVM optimization software, there are a few things you need to do, though. linear kernelFirst is to come up with with some choice of the parameter’s C. We talked a little bit of the bias/variance properties of this in the earlier video. Second, you also need to choose the kernel or the similarity function that you want to use. So one choice might be if we decide not to use any kernel. And the idea of no kernel is also called a linear kernel. So if someone says, I use an SVM with a linear kernel, what that means is you know, they use an SVM without using without using a kernel and it was a version of the SVM that just uses theta transpose X, right, that predicts 1 theta 0 plus theta 1 X1 plus so on plus theta N, X N is greater than equals 0. This term linear kernel, you can think of this as you know this is the version of the SVM that just gives you a standard linear classifier. So that would be one reasonable choice for some problems, and you know, there would be many software libraries, like linear, was one example, out of many, one example of a software library that can train an SVM without using a kernel, also called a linear kernel. So, why would you want to do this? If you have a large number of features, if N is large, and M the number of training examples is small, then you know you have a huge number of features that if X, this is an X is an Rn, Rn +1. So if you have a huge number of features already, with a small training set, you know, maybe you want to just fit a linear decision boundary and not try to fit a very complicated nonlinear function, because might not have enough data. And you might risk overfitting, if you’re trying to fit a very complicated function in a very high dimensional feature space, but if your training set sample is small. So this would be one reasonable setting where you might decide to just not use a kernel, or equivalents to use what’s called a linear kernel. Gaussian kernelA second choice for the kernel that you might make, is this Gaussian kernel, and this is what we had previously. And if you do this, then the other choice you need to make is to choose this parameter sigma squared when we also talk a little bit about the bias variance tradeoffs of how, if sigma squared is large, then you tend to have a higher bias, lower variance classifier, but if sigma squared is small, then you have a higher variance, lower bias classifier. So when would you choose a Gaussian kernel? Well, if your omission of features X, I mean Rn, and if N is small, and, ideally, you know, if n is large, right, so that’s if, you know, we have say, a two-dimensional training set, like the example I drew earlier. So n is equal to 2, but we have a pretty large training set. So, you know, I’ve drawn in a fairly large number of training examples, then maybe you want to use a kernel to fit a more complex nonlinear decision boundary, and the Gaussian kernel would be a fine way to do this. I’ll say more towards the end of the video, a little bit more about when you might choose a linear kernel, a Gaussian kernel and so on. But if concretely, if you decide to use a Gaussian kernel, then here’s what you need to do. Depending on what support vector machine software package you use, it may ask you to implement a kernel function, or to implement the similarity function. So if you’re using an octave or MATLAB implementation of an SVM, it may ask you to provide a function to compute a particular feature of the kernel. So this is really computing f subscript i for one particular value of i, where f here is just a single real number, so maybe I should move this better written f(i), but what you need to do is to write a kernel function that takes this input, you know, a training example or a test example whatever it takes in some vector X and takes as input one of the landmarks and but only I’ve come down X1 and X2 here, because the landmarks are really training examples as well. But what you need to do is write software that takes this input, you know, X1, X2 and computes this sort of similarity function between them and return a real number. And so what some support vector machine packages do is expect you to provide this kernel function that take this input you know, X1, X2 and returns a real number. And then it will take it from there and it will automatically generate all the features, and so automatically take X and map it to f1, f2, down to f(m) using this function that you write, and generate all the features and train the support vector machine from there. But sometimes you do need to provide this function yourself. Other if you are using the Gaussian kernel, some SVM implementations will also include the Gaussian kernel and a few other kernels as well, since the Gaussian kernel is probably the most common kernel. Gaussian and linear kernels are really the two most popular kernels by far. Just one implementational note. If you have features of very different scales, it is important to perform feature scaling before using the Gaussian kernel. And here’s why. If you imagine the computing the norm between X and l, right, so this term here, and the numerator term over there. What this is doing, the norm between X and l, that’s really saying, you know, let’s compute the vector V, which is equal to X minus l. And then let’s compute the norm does vector V, which is the difference between X. So the norm of V is really equal to V1 squared plus V2 squared plus dot dot dot, plus Vn squared. Because here X is in Rn, or Rn plus 1, but I’m going to ignore, you know, X0. So, let’s pretend X is an Rn, square on the left side is what makes this correct. So this is equal to that, right? And so written differently, this is going to be X1 minus l1 squared, plus x2 minus l2 squared, plus dot dot dot plus Xn minus ln squared. And now if your features take on very different ranges of value. So take a housing prediction, for example, if your data is some data about houses. And if X is in the range of thousands of square feet, for the first feature, X1. But if your second feature, X2 is the number of bedrooms. So if this is in the range of one to five bedrooms, then X1 minus l1 is going to be huge. This could be like a thousand squared, whereas X2 minus l2 is going to be much smaller and if that’s the case, then in this term, those distances will be almost essentially dominated by the sizes of the houses and the number of bathrooms would be largely ignored. As so as, to avoid this in order to make a machine work well, do perform future scaling. And that will sure that the SVM gives, you know, comparable amount of attention to all of your different features, and not just to in this example to size of houses were big movement here the features. When you try a support vector machines chances are by far the two most common kernels you use will be the linear kernel, meaning no kernel, or the Gaussian kernel that we talked about. And just one note of warning which is that not all similarity functions you might come up with are valid kernels. And the Gaussian kernel and the linear kernel and other kernels that you sometimes others will use, all of them need to satisfy a technical condition. It’s called Mercer’s Theorem and the reason you need to this is because support vector machine algorithms or implementations of the SVM have lots of clever numerical optimization tricks. In order to solve for the parameter’s theta efficiently and in the original design envisaged, those are decision made to restrict our attention only to kernels that satisfy this technical condition called Mercer’s Theorem. And what that does is, that makes sure that all of these SVM packages, all of these SVM software packages can use the large class of optimizations and get the parameter theta very quickly. So, what most people end up doing is using either the linear or Gaussian kernel, but there are a few other kernels that also satisfy Mercer’s theorem and that you may run across other people using, although I personally end up using other kernels you know, very, very rarely, if at all. Other choices of kernel Just to mention some of the other kernels that you may run across. One is the polynomial kernel. And for that the similarity between X and l is defined as, there are a lot of options, you can take X transpose l squared. So, here’s one measure of how similar X and l are. If X and l are very close with each other, then the inner product will tend to be large. And so, you know, this is a slightly unusual kernel. That is not used that often, but you may run across some people using it. This is one version of a polynomial kernel. Another is X transpose l cubed. These are all examples of the polynomial kernel. X transpose l plus 1 cubed. X transpose l plus maybe a number different then one 5 and, you know, to the power of 4 and so the polynomial kernel actually has two parameters. One is, what number do you add over here? It could be 0. This is really plus 0 over there, as well as what’s the degree of the polynomial over there. So the degree power and these numbers. And the more general form of the polynomial kernel is X transpose l, plus some constant and then to some degree in the X1 and so both of these are parameters for the polynomial kernel. So the polynomial kernel almost always or usually performs worse. And the Gaussian kernel does not use that much, but this is just something that you may run across. Usually it is used only for data where X and l are all strictly non negative, and so that ensures that these inner products are never negative. And this captures the intuition that X and l are very similar to each other, then maybe the inter product between them will be large. They have some other properties as well but people tend not to use it much. And then, depending on what you’re doing, there are other, sort of more esoteric kernels as well, that you may come across. You know, there’s a string kernel, this is sometimes used if your input data is text strings or other types of strings. There are things like the chi-square kernel, the histogram intersection kernel, and so on. There are sort of more esoteric kernels that you can use to measure similarity between different objects. So for example, if you’re trying to do some sort of text classification problem, where the input x is a string then maybe we want to find the similarity between two strings using the string kernel, but I personally you know end up very rarely, if at all, using these more esoteric kernels. I think I might have use the chi-square kernel, may be once in my life and the histogram kernel, may be once or twice in my life. I’ve actually never used the string kernel myself. But in case you’ve run across this in other applications. You know, if you do a quick web search we do a quick Google search or quick Bing search you should have found definitions that these are the kernels as well. Two last detailsMulti-class classification So just two last details I want to talk about in this video. One in multiclass classification. So, you have four classes or more generally 3 classes output some appropriate decision boundary between your multiple classes. Most SVM, many SVM packages already have built-in multiclass classification functionality. So if your using a pattern like that, you just use the both that functionality and that should work fine. Otherwise, one way to do this is to use the one versus all method that we talked about when we are developing logistic regression. So what you do is you trade kSVM’s if you have k classes, one to distinguish each of the classes from the rest. And this would give you k parameter vectors, so this will give you, upi lmpw. theta 1, which is trying to distinguish class y equals one from all of the other classes, then you get the second parameter, vector theta 2, which is what you get when you, you know, have y equals 2 as the positive class and all the others as negative class and so on up to a parameter vector theta k, which is the parameter vector for distinguishing the final class key from anything else, and then lastly, this is exactly the same as the one versus all method we have for logistic regression. Where we you just predict the class i with the largest theta transpose X. So let’s multiclass classification designate. For the more common cases that there is a good chance that whatever software package you use, you know, there will be a reasonable chance that are already have built in multiclass classification functionality, and so you don’t need to worry about this result. Finally, we developed support vector machines starting off with logistic regression and then modifying the cost function a little bit. ** logistic regression vs. SVMs The last thing we want to do in this video is, just say a little bit about. when you will use one of these two algorithms, so let’s say n is the number of features and m is the number of training examples. So, when should we use one algorithm versus the other? Well, if n is larger relative to your training set size, so for example, if you take a business with a number of features this is much larger than m and this might be, for example, if you have a text classification problem, where you know, the dimension of the feature vector is I don’t know, maybe, 10 thousand. And if your training set size is maybe 10 you know, maybe, up to 1000. So, imagine a spam classification problem, where email spam, where you have 10,000 features corresponding to 10,000 words but you have, you know, maybe 10 training examples or maybe up to 1,000 examples. So if n is large relative to m, then what I would usually do is use logistic regression or use it as the m without a kernel or use it with a linear kernel. Because, if you have so many features with smaller training sets, you know, a linear function will probably do fine, and you don’t have really enough data to fit a very complicated nonlinear function. Now if is n is small and m is intermediate what I mean by this is n is maybe anywhere from 1 - 1000, 1 would be very small. But maybe up to 1000 features and if the number of training examples is maybe anywhere from 10, you know, 10 to maybe up to 10,000 examples. Maybe up to 50,000 examples. If m is pretty big like maybe 10,000 but not a million. Right? So if m is an intermediate size then often an SVM with a linear kernel will work well. We talked about this early as well, with the one concrete example, this would be if you have a two dimensional training set. So, if n is equal to 2 where you have, you know, drawing in a pretty large number of training examples. So Gaussian kernel will do a pretty good job separating positive and negative classes. One third setting that’s of interest is if n is small but m is large. So if n is you know, again maybe 1 to 1000, could be larger. But if m was, maybe 50,000 and greater to millions. So, 50,000, a 100,000, million, trillion. You have very very large training set sizes, right. So if this is the case, then a SVM of the Gaussian Kernel will be somewhat slow to run. Today’s SVM packages, if you’re using a Gaussian Kernel, tend to struggle a bit. If you have, you know, maybe 50 thousands okay, but if you have a million training examples, maybe or even a 100,000 with a massive value of m. Today’s SVM packages are very good, but they can still struggle a little bit when you have a massive, massive trainings that size when using a Gaussian Kernel. So in that case, what I would usually do is try to just manually create have more features and then use logistic regression or an SVM without the Kernel. And in case you look at this slide and you see logistic regression or SVM without a kernel. In both of these places, I kind of paired them together. There’s a reason for that, is that logistic regression and SVM without the kernel, those are really pretty similar algorithms and, you know, either logistic regression or SVM without a kernel will usually do pretty similar things and give pretty similar performance, but depending on your implementational details, one may be more efficient than the other. But, where one of these algorithms applies, logistic regression where SVM without a kernel, the other one is to likely to work pretty well as well. But along with the power of the SVM is when you use different kernels to learn complex nonlinear functions. And this regime, you know, when you have maybe up to 10,000 examples, maybe up to 50,000. And your number of features, this is reasonably large. That’s a very common regime and maybe that’s a regime where a support vector machine with a kernel kernel will shine. You can do things that are much harder to do that will need logistic regression. And finally, where do neural networks fit in? Well for all of these problems, for all of these different regimes, a well designed neural network is likely to work well as well. The one disadvantage, or the one reason that might not sometimes use the neural network is that, for some of these problems, the neural network might be slow to train. But if you have a very good SVM implementation package, that could run faster, quite a bit faster than your neural network. And, although we didn’t show this earlier, it turns out that the optimization problem that the SVM has is a convex optimization problem and so the good SVM optimization software packages will always find the global minimum or something close to it. And so for the SVM you don’t need to worry about local optima. In practice local optima aren’t a huge problem for neural networks but they all solve, so this is one less thing to worry about if you’re using an SVM. And depending on your problem, the neural network may be slower, especially in this sort of regime than the SVM. In case the guidelines they gave here, seem a little bit vague and if you’re looking at some problems, you know, the guidelines are a bit vague, I’m still not entirely sure, should I use this algorithm or that algorithm, that’s actually okay. When I face a machine learning problem, you know, sometimes its actually just not clear whether that’s the best algorithm to use, but as you saw in the earlier videos, really, you know, the algorithm does matter, but what often matters even more is things like, how much data do you have. And how skilled are you, how good are you at doing error analysis and debugging learning algorithms, figuring out how to design new features and figuring out what other features to give you learning algorithms and so on. And often those things will matter more than what you are using logistic regression or an SVM. But having said that, the SVM is still widely perceived as one of the most powerful learning algorithms, and there is this regime of when there’s a very effective way to learn complex non linear functions. And so I actually, together with logistic regressions, neural networks, SVM’s, using those to speed learning algorithms you’re I think very well positioned to build state of the art you know, machine learning systems for a wide region for applications and this is another very powerful tool to have in your arsenal. One that is used all over the place in Silicon Valley, or in industry and in the Academia, to build many high performance machine learning system.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[11_machine-learning-system-design note11]]></title>
    <url>%2F2018%2F01%2F11%2F11_machine-learning-system-design%2F</url>
    <content type="text"><![CDATA[In the next few videos I’d like to talk about machine learning system design. These videos will touch on the main issues that you may face when designing a complex machine learning system, and will actually try to give advice on how to strategize putting together a complex machine learning system. In case this next set of videos seems a little disjointed that’s because these videos will touch on a range of the different issues that you may come across when designing complex learning systems. And even though the next set of videos may seem somewhat less mathematical, I think that this material may turn out to be very useful, and potentially huge time savers when you’re building big machine learning systems. NoteThis personal note is written after studying the opening course on the coursera website, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. 01_building-a-spam-classifierPrioritizing What to Work OnConcretely, I’d like to begin with the issue of prioritizing how to spend your time on what to work on, and I’ll begin with an example on spam classification. Let’s say you want to build a spam classifier. Here are a couple of examples of obvious spam and non-spam emails. if the one on the left tried to sell things. And notice how spammers will deliberately misspell words, like Vincent with a 1 there, and mortgages. And on the right as maybe an obvious example of non-stamp email, actually email from my younger brother. Let’s say we have a labeled training set of some number of spam emails and some non-spam emails denoted with labels y equals 1 or 0, how do we build a classifier using supervised learning to distinguish between spam and non-spam? In order to apply supervised learning, the first decision we must make is how do we want to represent x, that is the features of the email. Given the features x and the labels y in our training set, we can then train a classifier, for example using logistic regression. Here’s one way to choose a set of features for our emails. We could come up with, say, a list of maybe a hundred words that we think are indicative of whether e-mail is spam or non-spam, for example, if a piece of e-mail contains the word ‘deal’ maybe it’s more likely to be spam if it contains the word ‘buy’ maybe more likely to be spam, a word like ‘discount’ is more likely to be spam, whereas if a piece of email contains my name, Andrew, maybe that means the person actually knows who I am and that might mean it’s less likely to be spam. And maybe for some reason I think the word “now” may be indicative of non-spam because I get a lot of urgent emails, and so on, and maybe we choose a hundred words or so. Given a piece of email, we can then take this piece of email and encode it into a feature vector as follows. I’m going to take my list of a hundred words and sort them in alphabetical order say. It doesn’t have to be sorted. But, you know, here’s a, here’s my list of words, just count and so on, until eventually I’ll get down to now, and so on and given a piece of e-mail like that shown on the right, I’m going to check and see whether or not each of these words appears in the e-mail and then I’m going to define a feature vector x where in this piece of an email on the right, my name doesn’t appear so I’m gonna put a zero there. The word “by” does appear, so I’m gonna put a one there and I’m just gonna put one’s or zeroes. I’m gonna put a one even though the word “by” occurs twice. I’m not gonna recount how many times the word occurs. The word “due” appears, I put a one there. The word “discount” doesn’t appear, at least not in this this little short email, and so on. The word “now” does appear and so on. So I put ones and zeroes in this feature vector depending on whether or not a particular word appears. And in this example my feature vector would have to mention one hundred, if I have a hundred, if if I chose a hundred words to use for this representation and each of my features $X_j$ will basically be $1$ if you have a particular word that, we’ll call this word j, appears in the email and $X_j$ would be $0$ otherwise. Okay. So that gives me a feature representation of a piece of email. By the way, even though I’ve described this process as manually picking a hundred words, in practice what’s most commonly done is to look through a training set, and in the training set depict the most frequently occurring n words where n is usually between ten thousand and fifty thousand, and use those as your features. So rather than manually picking a hundred words, here you look through the training examples and pick the most frequently occurring words like ten thousand to fifty thousand words, and those form the features that you are going to use to represent your email for spam classification. Now, if you’re building a spam classifier, one question that you may face is, what’s the best use of your time in order to make your spam classifier have higher accuracy, you have lower error. One natural inclination is going to collect lots of data. Right? And in fact there’s this tendency to think that, well the more data we have the better the algorithm will do. And in fact, in the email spam domain, there are actually pretty serious projects called Honey Pot Projects, which create fake email addresses and try to get these fake email addresses into the hands of spammers and use that to try to collect tons of spam email, and therefore you know, get a lot of spam data to train learning algorithms. But we’ve already seen in the previous sets of videos that getting lots of data will often help, but not all the time. But for most machine learning problems, there are a lot of other things you could usually imagine doing to improve performance. For spam, one thing you might think of is to develop more sophisticated features on the email, maybe based on the email routing information. And this would be information contained in the email header. So, when spammers send email, very often they will try to obscure the origins of the email, and maybe use fake email headers. Or send email through very unusual sets of computer service. Through very unusual routes, in order to get the spam to you. And some of this information will be reflected in the email header. And so one can imagine, looking at the email headers and trying to develop more sophisticated features to capture this sort of email routing information to identify if something is spam. Something else you might consider doing is to look at the email message body, that is the email text, and try to develop more sophisticated features. For example, should the word ‘discount’ and the word ‘discounts’ be treated as the same words or should we have treat the words ‘deal’ and ‘dealer’ as the same word? Maybe even though one is lower case and one in capitalized in this example. Or do we want more complex features about punctuation because maybe spam is using exclamation marks a lot more. I don’t know. And along the same lines, maybe we also want to develop more sophisticated algorithms to detect and maybe to correct to deliberate misspellings, like mortgage, medicine, watches. Because spammers actually do this, because if you have watches with a 4 in there then well, with the simple technique that we talked about just now, the spam classifier might not equate this as the same thing as the word “watches,” and so it may have a harder time realizing that something is spam with these deliberate misspellings. And this is why spammers do it. While working on a machine learning problem, very often you can brainstorm lists of different things to try, like these. By the way, I’ve actually worked on the spam problem myself for a while. And I actually spent quite some time on it. And even though I kind of understand the spam problem, I actually know a bit about it, I would actually have a very hard time telling you of these four options which is the best use of your time so what happens, frankly what happens far too often is that a research group or product group will randomly fixate on one of these options. And sometimes that turns out not to be the most fruitful way to spend your time depending, you know, on which of these options someone ends up randomly fixating on. By the way, in fact, if you even get to the stage where you brainstorm a list of different options to try, you’re probably already ahead of the curve. Sadly, what most people do is instead of trying to list out the options of things you might try, what far too many people do is wake up one morning and, for some reason, just, you know, have a weird gut feeling that, “Oh let’s have a huge honeypot project to go and collect tons more data” and for whatever strange reason just sort of wake up one morning and randomly fixate on one thing and just work on that for six months. But I think we can do better. And in particular what I’d like to do in the next video is tell you about the concept of error analysis and talk about the way where you can try to have a more systematic way to choose amongst the options of the many different things you might work, and therefore be more likely to select what is actually a good way to spend your time, you know for the next few weeks, or next few days or the next few months. System Design Example: Given a data set of emails, we could construct a vector for each email. Each entry in this vector represents a word. The vector normally contains 10,000 to 50,000 entries gathered by finding the most frequently used words in our data set. If a word is to be found in the email, we would assign its respective entry a 1, else if it is not found, that entry would be a 0. Once we have all our x vectors ready, we train our algorithm and finally, we could use it to classify if an email is a spam or not. So how could you spend your time to improve the accuracy of this classifier? Collect lots of data (for example “honeypot” project but doesn’t always work) Develop sophisticated features (for example: using email header data in spam emails) Develop sophisticated features for message body (for example: should“discount” and “discounts” be treated as the same word? How about “deal” and “Dealer”? Features about punctuation)? Develop algorithms to process your input in different ways (recognizing misspellings in spam, for example, med1cine, m0rtgage, w4tches). It is difficult to tell which of the options will be most helpful. Error AnalysisThe recommended approach to solving machine learning problems is to: Start with a simple algorithm, implement it quickly, and test it early on your cross validation data. Plot learning curves to decide if more data, more features, etc. are likely to help. Manually examine the errors on examples in the cross validation set and try to spot a trend where most of the errors were made. For example, assume that we have 500 emails and our algorithm misclassifies a 100 of them. We could manually analyze the 100 emails and categorize them based on what type of emails they are. We could then try to come up with new cues and features that would help us classify these 100 emails correctly. Hence, if most of our misclassified emails are those which try to steal passwords, then we could find some features that are particular to those emails and add them to our model. We could also see how classifying each word according to its root changes our error rate: It is very important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm’s performance. For example if we use stemming, which is the process of treating the same word with different forms (fail/failing/failed) as one word (fail), and get a 3% error rate instead of 5%, then we should definitely add it to our model. However, if we try to distinguish between upper case and lower case letters and end up getting a 3.2% error rate instead of 3%, then we should avoid using this new feature. Hence, we should try new things, get a numerical value for our error rate, and based on our result decide whether we want to keep the new feature or not. 02_handling-skewed-data01_error-metrics-for-skewed-classesIn the previous video, I talked about error analysis and the importance of having error metrics, that is of having a single real number evaluation metric for your learning algorithm to tell how well it’s doing. In the context of evaluation and of error metrics, there is one important case, where it’s particularly tricky to come up with an appropriate error metric, or evaluation metric, for your learning algorithm. That case is the case of what’s called skewed classes. Let me tell you what that means. Consider the problem of cancer classification, where we have features of medical patients and we want to decide whether or not they have cancer. So this is like the malignant versus benign tumor classification example that we had earlier. So let’s say y equals 1 if the patient has cancer and y equals 0 if they do not. We have trained the progression classifier and let’s say we test our classifier on a test set and find that we get 1 percent error. So, we’re making 99% correct diagnosis. Seems like a really impressive result, right. We’re correct 99% percent of the time. But now, let’s say we find out that only 0.5 percent of patients in our training test sets actually have cancer. So only half a percent of the patients that come through our screening process have cancer. In this case, the 1% error no longer looks so impressive. And in particular, here’s a piece of code, here’s actually a piece of non learning code that takes this input of features x and it ignores it. It just sets y equals 0 and always predicts, you know, nobody has cancer and this algorithm would actually get 0.5 percent error. So this is even better than the 1% error that we were getting just now and this is a non learning algorithm that you know, it is just predicting y equals 0 all the time. So this setting of when the ratio of positive to negative examples is very close to one of two extremes, where, in this case, the number of positive examples is much, much smaller than the number of negative examples because y equals one so rarely, this is what we call the case of skewed classes. We just have a lot more of examples from one class than from the other class. And by just predicting y equals 0 all the time, or maybe our predicting y equals 1 all the time, an algorithm can do pretty well. So the problem with using classification error or classification accuracy as our evaluation metric is the following. Let’s say you have one joining algorithm that’s getting 99.2% accuracy. So, that’s a 0.8% error. Let’s say you make a change to your algorithm and you now are getting 99.5% accuracy. That is 0.5% error. So, is this an improvement to the algorithm or not? One of the nice things about having a single real number evaluation metric is this helps us to quickly decide if we just need a good change to the algorithm or not. By going from 99.2% accuracy to 99.5% accuracy. You know, did we just do something useful or did we just replace our code with something that just predicts y equals zero more often? So, if you have very skewed classes it becomes much harder to use just classification accuracy, because you can get very high classification accuracies or very low errors, and it’s not always clear if doing so is really improving the quality of your classifier because predicting y equals 0 all the time doesn’t seem like a particularly good classifier. But just predicting y equals 0 more often can bring your error down to, you know, maybe as low as 0.5%. When we’re faced with such a skewed classes therefore we would want to come up with a different error metric or a different evaluation metric. One such evaluation metric are what’s called precision and recall. Let me explain what that is. Let’s say we are evaluating a classifier on the test set. For the examples in the test set the actual class of that example in the test set is going to be either one or zero, right, if there is a binary classification problem. And what our learning algorithm will do is it will, you know, predict some value for the class and our learning algorithm will predict the value for each example in my test set and the predicted value will also be either one or zero. So let me draw a two by two table as follows, depending on a full of these entries depending on what was the actual class and what was the predicted class. If we have an example where the actual class is one and the predicted class is one then that’s called an example that’s a true positive, meaning our algorithm predicted that it’s positive and in reality the example is positive. If our learning algorithm predicted that something is negative, class zero, and the actual class is also class zero then that’s what’s called a true negative. We predicted zero and it actually is zero. To find the other two boxes, if our learning algorithm predicts that the class is one but the actual class is zero, then that’s called a false positive. So that means our algorithm for the patient is cancelled out in reality if the patient does not. And finally, the last box is a zero, one. That’s called a false negative because our algorithm predicted zero, but the actual class was one. And so, we have this little sort of two by two table based on what was the actual class and what was the predicted class. So here’s a different way of evaluating the performance of our algorithm. We’re going to compute two numbers. The first is called precision - and what that says is, of all the patients where we’ve predicted that they have cancer, what fraction of them actually have cancer? So let me write this down, the precision of a classifier is the number of true positives divided by the number that we predicted as positive, right? So of all the patients that we went to those patients and we told them, “We think you have cancer.” Of all those patients, what fraction of them actually have cancer? So that’s called precision. And another way to write this would be true positives and then in the denominator is the number of predicted positives, and so that would be the sum of the, you know, entries in this first row of the table. So it would be true positives divided by positives. I’m going to abbreviate positive as POS and then plus false positives, again abbreviating positive using POS. So that’s called precision, and as you can tell high precision would be good. That means that all the patients that we went to and we said, “You know, we’re very sorry. We think you have cancer,” high precision means that of that group of patients most of them we had actually made accurate predictions on them and they do have cancer. The second number we’re going to compute is called recall, and what recall say is, if all the patients in, let’s say, in the test set or the cross-validation set, but if all the patients in the data set that actually have cancer, what fraction of them that we correctly detect as having cancer. So if all the patients have cancer, how many of them did we actually go to them and you know, correctly told them that we think they need treatment. So, writing this down, recall is defined as the number of positives, the number of true positives, meaning the number of people that have cancer and that we correctly predicted have cancer and we take that and divide that by, divide that by the number of actual positives, so this is the right number of actual positives of all the people that do have cancer. What fraction do we directly flag and you know, send the treatment. So, to rewrite this in a different form, the denominator would be the number of actual positives as you know, is the sum of the entries in this first column over here. And so writing things out differently, this is therefore, the number of true positives, divided by the number of true positives plus the number of false negatives. And so once again, having a high recall would be a good thing. So by computing precision and recall this will usually give us a better sense of how well our classifier is doing. And in particular if we have a learning algorithm that predicts y equals zero all the time, if it predicts no one has cancer, then this classifier will have a recall equal to zero, because there won’t be any true positives and so that’s a quick way for us to recognize that, you know, a classifier that predicts y equals 0 all the time, just isn’t a very good classifier. And more generally, even for settings where we have very skewed classes, it’s not possible for an algorithm to sort of “cheat” and somehow get a very high precision and a very high recall by doing some simple thing like predicting y equals 0 all the time or predicting y equals 1 all the time. And so we’re much more sure that a classifier of a high precision or high recall actually is a good classifier, and this gives us a more useful evaluation metric that is a more direct way to actually understand whether, you know, our algorithm may be doing well. So one final note in the definition of precision and recall, that we would define precision and recall, usually we use the convention that y is equal to 1, in the presence of the more rare class. So if we are trying to detect. rare conditions such as cancer, hopefully that’s a rare condition, precision and recall are defined setting y equals 1, rather than y equals 0, to be sort of that the presence of that rare class that we’re trying to detect. And by using precision and recall, we find, what happens is that even if we have very skewed classes, it’s not possible for an algorithm to you know, “cheat” and predict y equals 1 all the time, or predict y equals 0 all the time, and get high precision and recall. And in particular, if a classifier is getting high precision and high recall, then we are actually confident that the algorithm has to be doing well, even if we have very skewed classes. So for the problem of skewed classes precision recall gives us more direct insight into how the learning algorithm is doing and this is often a much better way to evaluate our learning algorithms, than looking at classification error or classification accuracy, when the classes are very skewed. 02_trading-off-precision-and-recallIn the last video,we talked about precision and recall as an evaluation metric for classification problems with skewed constants. For many applications, we’ll want to somehow control the trade-off between precision and recall. Let me tell you how to do that and also show you some even more effective ways to use precision and recall as an evaluation metric for learning algorithms. As a reminder,here are the definitions of precision and recall from the previous video. Let’s continue our cancer classification example, where y equals 1 if the patient has cancer and y equals 0 otherwise. And let’s say we’re trained in logistic regression classifier which outputs probability between 0 and 1. So, as usual, we’re going to predict 1, y equals 1, if h(x) is greater or equal to 0.5. And predict 0 if the hypothesis outputs a value less than 0.5. And this classifier may give us some value for precision and some value for recall. But now, suppose we want to predict that the patient has cancer only if we’re very confident that they really do. Because if you go to a patient and you tell them that they have cancer, it’s going to give them a huge shock. What we give is a seriously bad news, and they may end up going through a pretty painful treatment process and so on. And so maybe we want to tell someone that we think they have cancer only if they are very confident. One way to do this would be to modify the algorithm, so that instead of setting this threshold at 0.5, we might instead say that we will predict that y is equal to 1 only if h(x) is greater or equal to 0.7. So this is like saying, we’ll tell someone they have cancer only if we think there’s a greater than or equal to, 70% chance that they have cancer. And, if you do this, then you’re predicting someone has canceronly when you’re more confident and so you end up with a classifier that has higher precision. Because all of the patients that you’re going to and saying, we think you have cancer, although those patients are now ones that you’re pretty confident actually have cancer. And so a higher fraction of the patients that you predict have cancer will actually turn out to have cancer because making those predictions only if we’re pretty confident. But in contrast this classifier will have lower recall because now we’re going to make predictions, we’re going to predict y = 1 on a smaller number of patients. Now, can even take this further. Instead of setting the threshold at 0.7, we can set this at 0.9. Now we’ll predict y=1 only if we are more than 90% certain that the patient has cancer. And so, a large fraction of those patients will turn out to have cancer. And so this would be a higher precision classifier will have lower recall because we want to correctly detect that those patients have cancer. Now consider a different example. Suppose we want to avoid missing too many actual cases of cancer, so we want to avoid false negatives. In particular, if a patient actually has cancer, but we fail to tell them that they have cancer then that can be really bad. Because if we tell a patient that they don’t have cancer, then they’re not going to go for treatment. And if it turns out that they have cancer, but we fail to tell them they have cancer, well, they may not get treated at all. And so that would be a really bad outcome because they die because we told them that they don’t have cancer. They fail to get treated, but it turns out they actually have cancer. So, suppose that, when in doubt, we want to predict that y=1. So, when in doubt, we want to predict that they have cancer so that at least they look further into it, and these can get treated in case they do turn out to have cancer. In this case, rather than setting higher probability threshold, we might instead take this value and instead set it to a lower value. So maybe 0.3 like so, right? And by doing so, we’re saying that,you know what, if we think there’s more than a 30% chance that they have cancer we better be more conservative and tell them that they may have cancer so that they can seek treatment if necessary. And in this case what we would have is going to be a higher recall classifier, because we’re going to be correctly flagging a higher fraction of all of the patients that actually do have cancer. But we’re going to end up with lower precision because a higher fraction of the patients that we said have cancer, a high fraction of them will turnout not to have cancer after all. And by the way, just as a sider, when I talk about this to other students, I’ve been told before, it’s pretty amazing, some of my students say, is how I can tell the story both ways. Why we might want to have higher precision or higher recall and the story actually seems to work both ways. But I hope the details of the algorithm is true and the more general principle is depending on where you want, whether you want higher precision- lower recall, or higher recall- lower precision. You can end up predicting y=1 when h(x) is greater than some threshold. And so in general, for most classifiers there is going to be a trade off between precision and recall, and as you vary the value of this threshold that we join here, you can actually plot out some curve that trades off precision and recall. Where a value up here, this would correspond to a very high value of the threshold, maybe threshold equals 0.99. So that’s saying, predict y=1 only if we’re more than 99% confident, at least 99% probability this one. So that would be a high precision, relatively low recall. Where as the point down here, will correspond to a value of the threshold that’s much lower, maybe equal 0.01, meaning, when in doubt at all, predict y=1, and if you do that, you end up with a much lower precision, higher recall classifier. And as you vary the threshold, if you want you can actually trace of a curve for your classifier to see the range of different values you can get for precision recall. And by the way, the precision-recall curve can look like many different shapes. Sometimes it will look like this, sometimes it will look like that. Now there are many different possible shapes for the precision-recall curve, depending on the details of the classifier. So, this raises another interesting question which is, is there a way to choose this threshold automatically? Or more generally, if we have a few different algorithms or a few different ideas for algorithms, how do we compare different precision recall numbers? Concretely, suppose we have three different learning algorithms. So actually, maybe these are three different learning algorithms, maybe these are the same algorithm but just with different values for the threshold. How do we decide which of these algorithms is best? 03_using-large-data-setsIn the previous video, we talked about evaluation metrics. In this video, I’d like to switch tracks a bit and touch on another important aspect of machine learning system design, which will often come up, which is the issue of how much data to train on. Now, in some earlier videos, I had cautioned against blindly going out and just spending lots of time collecting lots of data, because it’s only sometimes that that would actually help. But it turns out that under certain conditions, and I will say in this video what those conditions are, getting a lot of data and training on a certain type of learning algorithm, can be a very effective way to get a learning algorithm to do very good performance. And this arises often enough that if those conditions hold true for your problem and if you’re able to get a lot of data, this could be a very good way to get a very high performance learning algorithm. So in this video, let’s talk more about that. Let me start with a story. Michelle Banko and Eric BrouleMany, many years ago, two researchers that I know, Michelle Banko and Eric Broule ran the following fascinating study. They were interested in studying the effect of using different learning algorithms versus trying them out on different training set sciences, they were considering the problem of classifying between confusable words, so for example, in the sentence: for breakfast I ate, should it be to, two or too? Well, for this example, for breakfast I ate two, 2 eggs. So, this is one example of a set of confusable words and that’s a different set. So they took machine learning problems like these, sort of supervised learning problems to try to categorize what is the appropriate word to go into a certain position in an English sentence. They took a few different learning algorithms which were, you know, sort of considered state of the art back in the day, when they ran the study in 2001, so they took a variance, roughly a variance on logistic regression called the Perceptron. They also took some of their algorithms that were fairly out back then but somewhat less used now so when the algorithm also very similar to which is a regression but different in some ways, much used somewhat less, used not too much right now took what’s called a memory based learning algorithm again used somewhat less now. But I’ll talk a little bit about that later. And they used a naive based algorithm, which is something they’ll actually talk about in this course. The exact algorithms of these details aren’t important. Think of this as, you know, just picking four different classification algorithms and really the exact algorithms aren’t important. But what they did was they varied the training set size and tried out these learning algorithms on the range of training set sizes and that’s the result they got. And the trends are very clear, right? first, most of these algorithms give remarkably similar performance. And second, as the training set size increases, on the horizontal axis is the training set size in millions go from, you know, a hundred thousand up to a thousand million that is a billion training examples. The performance of the algorithms all pretty much monotonically increase and the fact that if you pick any algorithm, may be pick a “inferior algorithm”, but if you give that “inferior algorithm” more data, then from these examples, it looks like it will most likely beat even a “superior algorithm”. So since this original study which is very influential, there’s been a range of many different studies showing similar results that show that many different learning algorithms you know tend to, can sometimes, depending on details, can give pretty similar ranges of performance, but what can really drive performance is you can give the algorithm a ton of training data. And this is, results like these has led to a saying in machine learning that often in machine learning it’s not who has the best algorithm that wins, it’s who has the most data So when is this true and when is this not true? Because we have a learning algorithm for which this is true then getting a lot of data is often maybe the best way to ensure that we have an algorithm with very high performance rather than you know, debating worrying about exactly which of these items to use. the features x have sufficient informationLet’s try to lay out a set of assumptions under which having a massive training set we think will be able to help. Let’s assume that in our machine learning problem, the features x have sufficient information, with which we can use to predict y accurately. For example, if we take the confusable words all of them that we had on the previous slide. Let’s say that it features x capture what are the surrounding words around the blank that we’re trying to fill in. So the features capture then we want to have, sometimes for breakfast I have black eggs. Then yeah that is pretty much information to tell me that the word I want in the middle is TWO and that is not word TO and its not the word TOO. So the features capture, you know, one of these surrounding words then that gives me enough information to pretty unambiguously decide what is the label y or in other words what is the word that I should be using to fill in that blank out of this set of three confusable words. So that’s an example what the futures x has sufficient information for specific y. For a counter example. Consider a problem of predicting the price of a house from only the size of the house and from no other features. So if you imagine I tell you that a house is, you know, 500 square feet but I don’t give you any other features. I don’t tell you that the house is in an expensive part of the city. Or if I don’t tell you that the house, the number of rooms in the house, or how nicely furnished the house is, or whether the house is new or old. If I don’t tell you anything other than that this is a 500 square foot house, well there’s so many other factors that would affect the price of a house other than just the size of a house that if all you know is the size, it’s actually very difficult to predict the price accurately. So that would be a counter example to this assumption that the features have sufficient information to predict the price to the desired level of accuracy. domain knowledgeThe way I think about testing this assumption, one way I often think about it is, how often I ask myself. Given the input features x, given the features, given the same information available as well as learning algorithm. If we were to go to human expert in this domain. Can a human experts actually or can human expert confidently predict the value of y. For this first example if we go to, you know an expert human English speaker. You go to someone that speaks English well, right, then a human expert in English just read most people like you and me will probably we would probably be able to predict what word should go in here, to a good English speaker can predict this well, and so this gives me confidence that x allows us to predict y accurately, but in contrast if we go to an expert in house prices. Like maybe an expert realtor, right, someone who sells houses for a living. If I just tell them the size of a house and I tell them what the price is well even an expert in pricing or selling houses wouldn’t be able to tell me and so this is fine that for the housing price example knowing only the size doesn’t give me enough information to predict the price of the house. So, let’s say, this assumption holds. Let’s see then, when having a lot of data could help. Suppose the features have enough information to predict the value of y. And let’s suppose we use a learning algorithm with a large number of parameters so maybe logistic regression or linear regression with a large number of features. Or one thing that I sometimes do, one thing that I often do actually is using neural network with many hidden units. That would be another learning algorithm with a lot of parameters. So these are all powerful learning algorithms with a lot of parameters that can fit very complex functions. So, I’m going to call these, I’m going to think of these as low-bias algorithms because you know we can fit very complex functions and because we have a very powerful learning algorithm, they can fit very complex functions. Chances are, if we run these algorithms on the data sets, it will be able to fit the training set well, and so hopefully the training error will be slow. Large data rationale Now let’s say, we use a massive, massive training set, in that case, if we have a huge training set, then hopefully even though we have a lot of parameters but if the training set is sort of even much larger than the number of parameters then hopefully these albums will be unlikely to overfit. Right, because we have such a massive training set and by unlikely to overfit what that means is that the training error will hopefully be close to the test error. Finally putting these two together that the train set error is small and the test set error is close to the training error what this two together imply is that hopefully the test set error will also be small. Another way to think about this is that in order to have a high performance learning algorithm we want it not to have high bias and not to have high variance. So the bias problem we’re going to address by making sure we have a learning algorithm with many parameters and so that gives us a low bias algorithm and by using a very large training set, this ensures that we don’t have a variance problem here. So hopefully our algorithm will have no variance and so is by pulling these two together, that we end up with a low bias and a low variance learning algorithm and this allows us to do well on the test set. And fundamentally it’s a key ingredients of assuming that the features have enough information and we have a rich class of functions that’s why it guarantees low bias, and then it having a massive training set that that’s what guarantees more variance. So this gives us a set of conditions rather hopefully some understanding of what’s the sort of problem where if you have a lot of data and you train a learning algorithm with lot of parameters, that might be a good way to give a high performance learning algorithm The Key Testand really, I think the key test that I often ask myself are first, can a human experts look at the features x and confidently predict the value of y. Because that’s sort of a certification that y can be predicted accurately from the features x and second, can we actually get a large training set, and train the learning algorithm with a lot of parameters in the training set and if you can’t do both then that’s more often give you a very kind performance learning algorithm. summaryPrioritizing What to Work OnDifferent ways we can approach a machine learning problem: Collect lots of data (for example “honeypot” project but doesn’t always work) Develop sophisticated features (for example: using email header data in spam emails) Develop algorithms to process your input in different ways (recognizing misspellings in spam). It is difficult to tell which of the options will be helpful. Error AnalysisThe recommended approach to solving machine learning problems is: Start with a simple algorithm, implement it quickly, and test it early. Plot learning curves to decide if more data, more features, etc. will help Error analysis: manually examine the errors on examples in the cross validation set and try to spot a trend. It’s important to get error results as a single, numerical value. Otherwise it is difficult to assess your algorithm’s performance.You may need to process your input before it is useful. For example, if your input is a set of words, you may want to treat the same word with different forms (fail/failing/failed) as one word, so must use “stemming software” to recognize them all as one. Error Metrics for Skewed ClassesIt is sometimes difficult to tell whether a reduction in error is actually an improvement of the algorithm. For example: In predicting a cancer diagnoses where 0.5% of the examples have cancer, we find our learning algorithm has a 1% error. However, if we were to simply classify every single example as a 0, then our error would reduce to 0.5% even though we did not improve the algorithm. This usually happens with skewed classes ; that is, when our class is very rare in the entire data set.Or to say it another way, when we have lot more examples from one class than from the other class.For this we can use Precision/Recall . Predicted: 1, Actual: 1 — True positive Predicted: 0, Actual: 0 — True negative Predicted: 0, Actual, 1 — False negative Predicted: 1, Actual: 0 — False positive Precision : of all patients we predicted where y=1, what fraction actually has cancer?$$\dfrac{\text{True Positives}}{\text{Total number of predicted positives}} = \dfrac{\text{True Positives}}{\text{True Positives}+\text{False positives}}$$Recall : Of all the patients that actually have cancer, what fraction did we correctly detect as having cancer?$$\dfrac{\text{True Positives}}{\text{Total number of actual positives}}= \dfrac{\text{True Positives}}{\text{True Positives}+\text{False negatives}}$$These two metrics give us a better sense of how our classifier is doing. We want both precision and recall to be high.In the example at the beginning of the section, if we classify all patients as 0, then our recall will be $\dfrac{0}{0 + f} = 0$, so despite having a lower error percentage, we can quickly see it has worse recall.Accuracy = $\frac {true\ positive + true\ negative} {total\ population}$Note 1: if an algorithm predicts only negatives like it does in one of exercises, the precision is not defined, it is impossible to divide by 0. F1 score will not be defined too. Trading Off Precision and RecallWe might want a confident prediction of two classes using logistic regression. One way is to increase our threshold:Predict 1 if: $h_\theta(x) \geq 0.7$Predict 0 if: $h_\theta(x) &lt; 0.7$This way, we only predict cancer if the patient has a 70% chance.Doing this, we will have higher precision but but lower recall(refer to the definitions in the previous section).In the opposite example, we can lower our threshold:Predict 1 if: $h_\theta(x) \geq 0.3$Predict 0 if: $h_\theta(x) &lt; 0.3$That way, we get a very safe prediction. This will cause higher recall but lower precision .The greater the threshold, the greater the precision and the lower the recall.The lower the threshold, the greater the recall and the lower the precision.In order to turn these two metrics into one single number, we can take the F value .One way is to take the average :$$\dfrac{P+R}{2}$$This does not work well. If we predict all y=0 then that will bring the average up despite having 0 recall. If we predict all examples as y=1, then the very high recall will bring up the average despite having 0 precision.A better way is to compute the F Score (or F1 score):$$\text{F Score} = 2\dfrac{PR}{P + R}$$In order for the F Score to be large, both precision and recall must be large.We want to train precision and recall on the cross validation set so as not to bias our test set. Data for Machine LearningHow much data should we train on?In certain cases, an “inferior algorithm,” if given enough data, can outperform a superior algorithm with less data.We must choose our features to have enough information. A useful test is: Given input x, would a human expert be able to confidently predict y?Rationale for large data : if we have a low bias algorithm (many features or hidden units making a very complex function), then the larger the training set we use, the less we will have overfitting (and the more accurate the algorithm will be on the test set). Quiz instructionsWhen the quiz instructions tell you to enter a value to “two decimal digits”, what it really means is “two significant digits”. So, just for example, the value 0.0123 should be entered as “0.012”, not “0.01”.References:https://class.coursera.org/ml/lecture/indexhttp://www.cedar.buffalo.edu/~srihari/CSE555/Chap9.Part2.pdfhttp://blog.stephenpurpura.com/post/13052575854/managing-bias-variance-tradeoff-in-machine-learninghttp://www.cedar.buffalo.edu/~srihari/CSE574/Chap3/Bias-Variance.pdf]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[10_advice-for-applying-machine-learning note10]]></title>
    <url>%2F2018%2F01%2F10%2F10_advice-for-applying-machine-learning%2F</url>
    <content type="text"><![CDATA[By now you have seen a lot of different learning algorithms. And if you’ve been following along these videos you should consider yourself an expert on many state-of-the-art machine learning techniques. But even among people that know a certain learning algorithm. There’s often a huge difference between someone that really knows how to powerfully and effectively apply that algorithm, versus someone that’s less familiar with some of the material that I’m about to teach and who doesn’t really understand how to apply these algorithms and can end up wasting a lot of their time trying things out that don’t really make sense. What I would like to do is make sure that if you are developing machine learning systems, that you know how to choose one of the most promising avenues to spend your time pursuing. And on this and the next few videos I’m going to give a number of practical suggestions, advice, guidelines on how to do that. And concretely what we’d focus on is the problem of, suppose you are developing a machine learning system or trying to improve the performance of a machine learning system, how do you go about deciding what are the proxy avenues to try next? To explain this, let’s continue using our example of learning to predict housing prices. And let’s say you’ve implement and regularize linear regression. Thus minimizing that cost function j. Now suppose that after you take your learn parameters, if you test your hypothesis on the new set of houses, suppose you find that this is making huge errors in this prediction of the housing prices. The question is what should you then try mixing in order to improve the learning algorithm? There are many things that one can think of that could improve the performance of the learning algorithm. One thing they could try, is to get more training examples. And concretely, you can imagine, maybe, you know, setting up phone surveys, going door to door, to try to get more data on how much different houses sell for. And the sad thing is I’ve seen a lot of people spend a lot of time collecting more training examples, thinking oh, if we have twice as much or ten times as much training data, that is certainly going to help, right? But sometimes getting more training data doesn’t actually help and in the next few videos we will see why, and we will see how you can avoid spending a lot of time collecting more training data in settings where it is just not going to help. Other things you might try are to well maybe try a smaller set of features. So if you have some set of features such as x1, x2, x3 and so on, maybe a large number of features. Maybe you want to spend time carefully selecting some small subset of them to prevent overfitting. Or maybe you need to get additional features. Maybe the current set of features aren’t informative enough and you want to collect more data in the sense of getting more features. And once again this is the sort of project that can scale up the huge projects can you imagine getting phone surveys to find out more houses, or extra land surveys to find out more about the pieces of land and so on, so a huge project. And once again it would be nice to know in advance if this is going to help before we spend a lot of time doing something like this. We can also try adding polynomial features things like x2 square x2 square and product features x1, x2. We can still spend quite a lot of time thinking about that and we can also try other things like decreasing lambda, the regularization parameter or increasing lambda. Given a menu of options like these, some of which can easily scale up to six month or longer projects. Unfortunately, the most common method that people use to pick one of these is to go by gut feeling. In which what many people will do is sort of randomly pick one of these options and maybe say, “Oh, lets go and get more training data.” And easily spend six months collecting more training data or maybe someone else would rather be saying, “Well, let’s go collect a lot more features on these houses in our data set.” And I have a lot of times, sadly seen people spend, you know, literally 6 months doing one of these avenues that they have sort of at random only to discover six months later that that really wasn’t a promising avenue to pursue. Fortunately, there is a pretty simple technique that can let you very quickly rule out half of the things on this list as being potentially promising things to pursue. And there is a very simple technique, that if you run, can easily rule out many of these options, and potentially save you a lot of time pursuing something that’s just is not going to work. In the next two videos after this, I’m going to first talk about how to evaluate learning algorithms. And in the next few videos after that, I’m going to talk about these techniques, which are called the machine learning diagnostics. And what a diagnostic is, is a test you can run, to get insight into what is or isn’t working with an algorithm, and which will often give you insight as to what are promising things to try to improve a learning algorithm’s performance. We’ll talk about specific diagnostics later in this video sequence. But I should mention in advance that diagnostics can take time to implement and can sometimes, you know, take quite a lot of time to implement and understand but doing so can be a very good use of your time when you are developing learning algorithms because they can often save you from spending many months pursuing an avenue that you could have found out much earlier just was not going to be fruitful. So in the next few videos, I’m going to first talk about how evaluate your learning algorithms and after that I’m going to talk about some of these diagnostics which will hopefully let you much more effectively select more of the useful things to try mixing if your goal to improve the machine learning system. 01_evaluating-a-learning-algorithmEvaluating a HypothesisOnce we have done some trouble shooting for errors in our predictions by: Getting more training examples Trying smaller sets of features Trying additional features Trying polynomial features Increasing or decreasing λ We can move on to evaluate our new hypothesis. A hypothesis may have a low error for the training examples but still be inaccurate (because of overfitting). Thus, to evaluate a hypothesis, given a dataset of training examples, we can split up the data into two sets: a training set and a test set . Typically, the training set consists of 70 % of your data and the test set is the remaining 30 %. The new procedure using these two sets is then: Learn $\Theta$ and minimize $J_{train}(\Theta)$ using the training set Compute the test set error $J_{test}(\Theta)$ The test set error For linear regression: $J_{test}(\Theta) = \dfrac{1}{2m_{test}} \sum_{i=1}^{m_{test}}(h_\Theta(x^{(i)}_{test}) - y^{(i)}_{test})^2$ For classification ~ Misclassification error (aka 0/1 misclassification error): $$err(h_\Theta(x),y) = \begin{cases} 1 &amp; \mbox{if } h_\Theta(x) \geq 0.5\ and\ y = 0\ or\ h_\Theta(x) &lt; 0.5\ and\ y = 1\newline 0 &amp; \mbox otherwise \end{cases}$$ This gives us a binary 0 or 1 error result based on a misclassification. The average test error for the test set is: $$\text{Test Error} = \dfrac{1}{m_{test}} \sum^{m_{test}}_{i=1} err(h_\Theta(x^{(i)}_{test}), y^{(i)}_{test})$$ This gives us the proportion of the test data that was misclassified. Model Selection and Train/Validation/Test SetsJust because a learning algorithm fits a training set well, that does not mean it is a good hypothesis. It could overfit and as a result your predictions on the test set would be poor. The error of your hypothesis as measured on the data set with which you trained the parameters will be lower than the error on any other data set. Given many models with different polynomial degrees, we can use a systematic approach to identify the ‘best’ function. In order to choose the model of your hypothesis, you can test each degree of polynomial and look at the error result. One way to break down our dataset into the three sets is: ​ Training set: 60% ​ Cross validation set: 20% ​ Test set: 20% We can now calculate three separate error values for the three different sets using the following method: ​ Optimize the parameters in $Θ $ using the training set for each polynomial degree. ​ Find the polynomial degree d with the least error using the cross validation set. ​ Estimate the generalization error using the test set with $J_{test}(Θ^{(d)})$, (d = theta from polynomial with lower error); This way, the degree of the polynomial d has not been trained using the test set. Training error:$$J_{train}\left(\theta\right) = \frac{1}{2m}\sum_\limits{i=1}^{m}\left(h_{\theta}\left(x^{(i)}\right)-y^{(i)}\right)^2$$Cross Validation error:$$J_{cv}\left(\theta\right) = \frac{1}{2m_{cv}}\sum_\limits{i=1}^{m}\left(h_{\theta}\left(x^{(i)}_{cv}\right)-y^{(i)}_{cv}\right)^2$$Test error:$$J_{cv}\left(\theta\right) = \frac{1}{2m_{cv}}\sum_\limits{i=1}^{m}\left(h_{\theta}\left(x^{(i)}_{cv}\right)-y^{(i)}_{cv}\right)^2$$ 02_bias-vs-varianceDiagnosing Bias vs. VarianceIn this section we examine the relationship between the degree of the polynomial d and the underfitting or overfitting of our hypothesis. We need to distinguish whether bias or variance is the problem contributing to bad predictions. High bias is underfitting and high variance is overfitting. Ideally, we need to find a golden mean between these two. The training error will tend to decrease as we increase the degree d of the polynomial. At the same time, the cross validation error will tend to decrease as we increase d up to a point, and then it will increase as d is increased, forming a convex curve. High bias (underfitting): both $J_{train}(Θ)$ and $J_{CV}(Θ)$ will be high. Also, $J_{CV}(Θ)≈J_{train}(Θ)$. High variance (overfitting): $J_{train}(Θ)$ will be low and $J_{CV}(Θ)$ will be much greater than $J_{train}(Θ)$. The is summarized in the figure below: Regularization and Bias/VarianceNote: [The regularization term below and through out the video should be $\frac \lambda {2m} \sum _{j=1}^n \theta_j ^2$ and NOT $\frac \lambda {2m} \sum _{j=1}^m \theta_j ^2$] In the figure above, we see that as $\lambda$ increases, our fit becomes more rigid. On the other hand, as $\lambda$ approaches 0, we tend to over overfit the data. So how do we choose our parameter $\lambda$ to get it ‘just right’ ? In order to choose the model and the regularization term $λ$ , we need to: Create a list of lambdas (i.e.$ λ∈\{0,0.01,0.02,0.04,0.08,0.16,0.32,0.64,1.28,2.56,5.12,10.24\}$); Create a set of models with different degrees or any other variants. Iterate through the $\lambda$s and for each $\lambda$ go through all the models to learn some $\Theta$ . Compute the cross validation error using the learned $Θ$ (computed with $λ$) on the $J_{CV}(\Theta)$ without regularization or $λ = 0$. Select the best combo that produces the lowest error on the cross validation set. Using the best combo $Θ$ and λ, apply it on $J_{test}(\Theta)$ to see if it has a good generalization of the problem. Learning CurvesTraining an algorithm on a very few number of data points (such as 1, 2 or 3) will easily have 0 errors because we can always find a quadratic curve that touches exactly those number of points. Hence: As the training set gets larger, the error for a quadratic function increases. The error value will plateau out after a certain m, or training set size. Experiencing high bias: Low training set size : causes $J_{train}(\Theta)$ to be low and $J_{CV}(\Theta)$ to be high. Large training set size : causes both $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ to be high with $J_{train}(\Theta)$≈$J_{CV}(\Theta)$. If a learning algorithm is suffering from high bias, getting more training data will not (by itself) help much. Experiencing high variance: Low training set size : $J_{train}(\Theta)$ will be low and $J_{CV}(\Theta)$ will be high. Large training set size: $J_{train}(\Theta)$ increases with training set size and $J_{CV} (\Theta$) continues to decrease without leveling off. Also, $J_{train}(\Theta)$ &lt; $J_{CV}(\Theta)$ but the difference between them remains significant. If a learning algorithm is suffering from high variance, getting more training data is likely to help. Deciding What to Do Next Revisited Our decision process can be broken down as follows: ​ Getting more training examples: Fixes high variance ​ Trying smaller sets of features: Fixes high variance ​ Adding features: Fixes high bias ​ Adding polynomial features: Fixes high bias ​ Decreasing λ: Fixes high bias ​ Increasing λ: Fixes high variance. Diagnosing Neural Networks A neural network with fewer parameters is prone to underfitting. It is also computationally cheaper. A large neural network with more parameters is prone to overfitting. It is also computationally expensive. In this case you can use regularization (increase λ) to address the overfitting. Using a single hidden layer is a good starting default. You can train your neural network on a number of hidden layers using your cross validation set. You can then select the one that performs best. Model Complexity Effects: Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently. Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance. In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well. Model SelectionChoosing M the order of polynomials.How can we tell which parameters Θ to leave in the model (known as “model selection”)?There are several ways to solve this problem: Get more data (very difficult). Choose the model which best fits the data without overfitting (very difficult). Reduce the opportunity for overfitting through regularization . Bias: approximation error (Difference between expected value and optimal value) High Bias = UnderFitting (BU) $J_{train}(\Theta)$ and $J_{CV}(\Theta)$ both will be high and $J_{train}(\Theta)$ ≈ $J_{CV}(\Theta)$ Variance: estimation error due to finite data High Variance = OverFitting (VO) $J_{train}(\Theta)$ is low and $J_{CV}(\Theta)$ ≫$J_{train}(\Theta)$ Intuition for the bias-variance trade-off: Complex model =&gt; sensitive to data =&gt; much affected by changes in X =&gt; high variance, low bias. Simple model =&gt; more rigid =&gt; does not change as much with changes in X =&gt; low variance, high bias. One of the most important goals in learning: finding a model that is just right in the bias-variance trade-off. Regularization Effects: Small values of λ allow model to become finely tuned to noise leading to large variance =&gt; overfitting. Large values of λ pull weight parameters to zero leading to large bias =&gt; underfitting. Model Complexity Effects: Lower-order polynomials (low model complexity) have high bias and low variance. In this case, the model fits poorly consistently. Higher-order polynomials (high model complexity) fit the training data extremely well and the test data extremely poorly. These have low bias on the training data, but very high variance. In reality, we would want to choose a model somewhere in between, that can generalize well but also fits the data reasonably well. A typical rule of thumb when running diagnostics is: More training examples fixes high variance but not high bias. Fewer features fixes high variance but not high bias. Additional features fixes high bias but not high variance. The addition of polynomial and interaction features fixes high bias but not high variance. When using gradient descent, decreasing lambda can fix high bias and increasing lambda can fix high variance (lambda is the regularization parameter). When using neural networks, small neural networks are more prone to under-fitting and big neural networks are prone to over-fitting. Cross-validation of network size is a way to choose alternatives.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[09_neural-networks-learning note9]]></title>
    <url>%2F2018%2F01%2F09%2F09_neural-networks-learning%2F</url>
    <content type="text"><![CDATA[NoteThis personal note is written after studying the opening course on the coursera website, Machine Learning by Andrew NG. And images, audios of this note all comes from the opening course. 01_cost-function-and-backpropagation Cost FunctionLet’s first define a few variables that we will need to use: ​ L = total number of layers in the network ​ $s_l$ = number of units (not counting bias unit) in layer l ​ K = number of output units/classes Recall that in neural networks, we may have many output nodes. We denote $h_Θ(x)_k$ as being a hypothesis that results in the $k^{th}$output. Our cost function for neural networks is going to be a generalization of the one we used for logistic regression. Recall that the cost function for regularized logistic regression was :$$J(\theta) = - \frac{1}{m} \sum_{i=1}^m [ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$$For neural networks, it is going to be slightly more complicated :$$\begin{gather} J(\Theta) = - \frac{1}{m} \sum_{i=1}^m \sum_{k=1}^K \left[y^{(i)}_k \log ((h_\Theta (x^{(i)}))_k) + (1 - y^{(i)}_k)\log (1 - (h_\Theta(x^{(i)}))_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_{l+1}} ( \Theta_{j,i}^{(l)})^2\end{gather}$$We have added a few nested summations to account for our multiple output nodes. In the first part of the equation, before the square brackets, we have an additional nested summation that loops through the number of output nodes. In the regularization part, after the square brackets, we must account for multiple theta matrices. The number of columns in our current theta matrix is equal to the number of nodes in our current layer (including the bias unit). The number of rows in our current theta matrix is equal to the number of nodes in the next layer (excluding the bias unit). As before with logistic regression, we square every term. Note: the double sum simply adds up the logistic regression costs calculated for each cell in the output layer the triple sum simply adds up the squares of all the individual $Θ_s$ in the entire network. the i in the triple sum does not refer to training example i . Backpropagation Algorithm“Backpropagation” is neural-network terminology for minimizing our cost function, just like what we were doing with gradient descent in logistic and linear regression. Our goal is to compute:$$\min_\Theta J(\Theta)$$That is, we want to minimize our cost function $J$ using an optimal set of parameters in theta. In this section we’ll look at the equations we use to compute the partial derivative of $J(Θ)$:$$\dfrac{\partial}{\partial \Theta_{i,j}^{(l)}}J(\Theta)$$To do so, we use the following algorithm: Back propagation Algorithm Given training set $\lbrace (x^{(1)}, y^{(1)}) \cdots (x^{(m)}, y^{(m)})\rbrace$ Set $\Delta^{(l)}_{i,j}$ := 0 for all (l,i,j), (hence you end up having a matrix full of zeros) For training example t =1 to m: Set $a^{(1)} := x^{(t)}$ Perform forward propagation to compute $a^{(l)}$ for l=2,3,…,L Using $y^{(t)}$, compute $\delta^{(L)} = a^{(L)} - y^{(t)}$ Where L is our total number of layers and $a^{(L)}$ is the vector of outputs of the activation units for the last layer. So our “error values” for the last layer are simply the differences of our actual results in the last layer and the correct outputs in y. To get the delta values of the layers before the last layer, we can use an equation that steps us back from right to left : Compute $\delta^{(L-1)}, \delta^{(L-2)},\dots,\delta^{(2)}$ using $\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)})\ .\ a^{(l)}\ .\ (1 - a^{(l)})$ The delta values of layer l are calculated by multiplying the delta values in the next layer with the theta matrix of layer l. We then element-wise multiply that with a function called g’, or g-prime, which is the derivative of the activation function g evaluated with the input values given by $z^{(l)}$. The g-prime derivative terms can also be written out as: $$g’(z^{(l)}) = a^{(l)}\ .*\ (1 - a^{(l)})$$ $\Delta^{(l)}_{i,j} := \Delta^{(l)}_{i,j} + a_j^{(l)} \delta_i^{(l+1)}$ or with vectorization, $\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T$ Hence we update our new $\Delta$ matrix. $D^{(l)}_{i,j} := \dfrac{1}{m}\left(\Delta^{(l)}_{i,j} + \lambda\Theta^{(l)}_{i,j}\right)$, if j≠0. $D^{(l)}_{i,j} := \dfrac{1}{m}\Delta^{(l)}_{i,j}$ If j=0 The capital-delta matrix D is used as an “accumulator” to add up our values as we go along and eventually compute our partial derivative. Thus we get $\frac \partial {\partial \Theta_{ij}^{(l)}} J(\Theta)$= $D_{ij}^{(l)}$ Backpropagation IntuitionNote: [4:39, the last term for the calculation for $z^3_1$ (three-color handwritten formula) should be $a^2_2$ instead of $a^2_1$. 6:08 - the equation for cost(i) is incorrect. The first term is missing parentheses for the log() function, and the second term should be $(1-y^{(i)})\log(1-h{_\theta}{(x^{(i)}}))$. 8:50 - $\delta^{(4)} = y - a^{(4)}$ is incorrect and should be $\delta^{(4)} = a^{(4)} - y$.] Recall that the cost function for a neural network is: $$\begin{gather}J(\Theta) = - \frac{1}{m} \sum_{t=1}^m\sum_{k=1}^K \left[ y^{(t)}_k \ \log (h_\Theta (x^{(t)}))_k + (1 - y^{(t)}_k)\ \log (1 - h_\Theta(x^{(t)})_k)\right] + \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l} \sum_{j=1}^{s_l+1} ( \Theta_{j,i}^{(l)})^2\end{gather}$$ If we consider simple non-multiclass classification (k = 1) and disregard regularization, the cost is computed with: $$cost(t) =y^{(t)} \ \log (h_\Theta (x^{(t)})) + (1 - y^{(t)})\ \log (1 - h_\Theta(x^{(t)}))$$ Intuitively, $\delta_j^{(l)}$ is the “error” for $a^{(l)}_j$ (unit j in layer l). More formally, the delta values are actually the derivative of the cost function: $$\delta_j^{(l)} = \dfrac{\partial}{\partial z_j^{(l)}} cost(t)$$ Recall that our derivative is the slope of a line tangent to the cost function, so the steeper the slope the more incorrect we are. Let us consider the following neural network below and see how we could calculate some $\delta_j^{(l)}$: In the image above, to calculate $\delta_2^{(2)}$, we multiply the weights $\Theta_{12}^{(2)}$ and $\Theta_{22}^{(2)}$ by their respective $\delta$ values found to the right of each edge. So we get $\delta_2^{(2)}$= $\Theta_{12}^{(2)}$$\delta_1^{(3)}$+$\Theta_{22}^{(2)}$$\delta_2^{(3)}$. To calculate every single possible $\delta_j^{(l)}$, we could start from the right of our diagram. We can think of our edges as our $\Theta_{ij}$. Going from right to left, to calculate the value of $\delta_j^{(l)}$, you can just take the over all sum of each weight times the $\delta$ it is coming from. Hence, another example would be $\delta_2^{(3)}$=$\Theta_{12}^{(3)}$*$\delta_1^{(4)}$. 02_backpropagation-in-practiceImplementation Note: Unrolling ParametersWith neural networks, we are working with sets of matrices: $$\begin{align*} \Theta^{(1)}, \Theta^{(2)}, \Theta^{(3)}, \dots \newline D^{(1)}, D^{(2)}, D^{(3)}, \dots \end{align*}$$ In order to use optimizing functions such as “fminunc()”, we will want to “unroll” all the elements and put them into one long vector: 12thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ]deltaVector = [ D1(:); D2(:); D3(:) ] If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11, then we can get back our original matrices from the “unrolled” versions as follows: 123Theta1 = reshape(thetaVector(1:110),10,11)Theta2 = reshape(thetaVector(111:220),10,11)Theta3 = reshape(thetaVector(221:231),1,11) To summarize: Gradient CheckingGradient checking will assure that our backpropagation works as intended. We can approximate the derivative of our cost function with:$$\dfrac{\partial}{\partial\Theta}J(\Theta) \approx \dfrac{J(\Theta + \epsilon) - J(\Theta - \epsilon)}{2\epsilon}$$With multiple theta matrices, we can approximate the derivative with respect to $Θ_j$ as follows:$$\dfrac{\partial}{\partial\Theta_j}J(\Theta) \approx \dfrac{J(\Theta_1, \dots, \Theta_j + \epsilon, \dots, \Theta_n) - J(\Theta_1, \dots, \Theta_j - \epsilon, \dots, \Theta_n)}{2\epsilon}$$A small value for ϵ (epsilon) such as $ϵ=10^{−4}$ , guarantees that the math works out properly. If the value for ϵ is too small, we can end up with numerical problems. Hence, we are only adding or subtracting epsilon to the $Θ_j$ matrix. In octave we can do it as follows: 12345678epsilon = 1e-4;for i = 1:n, thetaPlus = theta; thetaPlus(i) += epsilon; thetaMinus = theta; thetaMinus(i) -= epsilon; gradApprox(i) = (J(thetaPlus) - J(thetaMinus))/(2*epsilon)end; We previously saw how to calculate the deltaVector. So once we compute our gradApprox vector, we can check that gradApprox ≈ deltaVector. Once you have verified once that your backpropagation algorithm is correct, you don’t need to compute gradApprox again. The code to compute gradApprox can be very slow. Random InitializationInitializing all theta weights to zero does not work with neural networks. When we backpropagate, all nodes will update to the same value repeatedly. Instead we can randomly initialize our weights for our ΘΘ matrices using the following method: Hence, we initialize each $Θ^{(l)}_{ij}$ to a random value between $[−ϵ,ϵ]$ . Using the above formula guarantees that we get the desired bound. The same procedure applies to all the $Θ’s$. Below is some working code you could use to experiment. 12345If the dimensions of Theta1 is 10x11, Theta2 is 10x11 and Theta3 is 1x11.Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;Theta2 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;Theta3 = rand(1,11) * (2 * INIT_EPSILON) - INIT_EPSILON; rand(x,y) is just a function in octave that will initialize a matrix of random real numbers between 0 and 1. (Note: the epsilon used above is unrelated to the epsilon from Gradient Checking) Putting it TogetherFirst, pick a network architecture; choose the layout of your neural network, including how many hidden units in each layer and how many layers in total you want to have. Number of input units = dimension of features $x^{(i)}$ Number of output units = number of classes Number of hidden units per layer = usually more the better (must balance with cost of computation as it increases with more hidden units) Defaults: 1 hidden layer. If you have more than 1 hidden layer, then it is recommended that you have the same number of units in every hidden layer. Training a Neural Network ​ Randomly initialize the weights ​ Implement forward propagation to get $h_Θ(x^{(i)})$ for any $x^{(i)}$ ​ Implement the cost function ​ Implement backpropagation to compute partial derivatives ​ Use gradient checking to confirm that your backpropagation works. Then disable gradient checking. ​ Use gradient descent or a built-in optimization function to minimize the cost function with the weights in theta. When we perform forward and back propagation, we loop on every training example: 123for i = 1:m, Perform forward propagation and backpropagation using example (x(i),y(i)) (Get activations a(l) and delta terms d(l) for l = 2,...,L The following image gives us an intuition of what is happening as we are implementing our neural network: Ideally, you want $h_Θ(x^{(i)}) ≈ y(i)$ . This will minimize our cost function. However, keep in mind that $J(Θ)$ is not convex and thus we can end up in a local minimum instead.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[08_neural-networks-representation note8]]></title>
    <url>%2F2018%2F01%2F08%2F08_neural-networks-representation%2F</url>
    <content type="text"><![CDATA[NoteThis personal note is written after studying the opening course on the coursera website, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. Motivationsnon-linear-hypothesesIn order to motivate the discussion of neural networks, let me start by showing you a few examples of machine learning problems where we need to learn complex non-linear hypotheses. Consider a supervised learning classification problem where you have a training set like this. If you want to apply logistic regression to this problem, one thing you could do is apply logistic regression with a lot of nonlinear features like that. So here, g as usual is the sigmoid function, and we can include lots of polynomial terms like these. And, if you include enough polynomial terms then, you know, maybe you can get a hypotheses that separates the positive and negative examples.This particular method works well when you have only, say, two features - x1 and x2 because you can then include all those polynomial terms of x1 and x2. House PredictionBut for many interesting machine learning problems would have a lot more features than just two. We’ve been talking for a while about housing prediction, and suppose you have a housing classification problem rather than a regression problem, like maybe if you have different features of a house, and you wantto predict what are the odds that your house will be sold within the next six months, so that will be a classification problem. And as we saw we can come up with quite a lot of features, maybe a hundred different features of different houses. For a problem like this, if you were to include all the quadratic terms, all of these, even all of the quadratic that is the second or the polynomial terms, there would be a lot of them. There would be terms like x1 squared, x1x2, x1x3, you know, x1x4 up to x1x100 and then you have x2 squared, x2x3 and so on. And if you include just the second order terms, that is, the terms that are a product of, you know, two of these terms, x1 times x1 and so on, then, for the case of n equals 100, you end up with about five thousand features. And, asymptotically, the number of quadratic features grows roughly as order n squared, where n is the number of the original features, like x1 through x100 that we had. And its actually closer to n squared over two. So including all the quadratic features doesn’t seem like it’s maybe a good idea, because that is a lot of features and you might up overfitting the training set, and it can also be computationally expensive, you know, to be working with that many features. One thing you could do is include only a subset of these, so if you include only the features x1 squared, x2 squared, x3 squared, up to maybe x100 squared, then the number of features is much smaller. Here you have only 100 such quadratic features, but this is not enough features and certainly won’t let you fit the data set like that on the upper left. In fact, if you include only these quadratic features together with the original x1, and so on, up to x100 features, then you can actually fit very interesting hypotheses. So, you can fit things like, you know, access a line of the ellipses like these, but you certainly cannot fit a more complex data set like that shown here. So 5000 features seems like a lot, if you were to include the cubic, or third order known of each others, the x1, x2, x3. You know, x1 squared, x2, x10 and x11, x17 and so on. You can imagine there are gonna be a lot of these features. In fact, they are going to be order and cube such features and if any is 100 you can compute that, you end up with on the order of about 170,000 such cubic features and so including these higher auto-polynomial features when your original feature set end is large this really dramatically blows up your feature space and this doesn’t seem like a good way to come up with additional features with which to build none many classifiers when n is large. Car recognitionFor many machine learning problems, n will be pretty large. Here’s an example. Let’s consider the problem of computer vision. And suppose you want to use machine learning to train a classifier to examine an image and tell us whether or not the image is a car. Many people wonder why computer vision could be difficult. I mean when you and I look at this picture it is so obvious what this is. You wonder how is it that a learning algorithm could possibly fail to know what this picture is. To understand why computer vision is hard let’s zoom into a small part of the image like that area where the little red rectangle is. It turns out that where you and I see a car, the computer sees that. What it sees is this matrix, or this grid, of pixel intensity values that tells us the brightness of each pixel in the image.So the computer vision problem is to look at this matrix of pixel intensity values, and tell us that these numbers represent the door handle of a car. Concretely, when we use machine learning to build a car detector, what we do is we come up with a label training set, with, let’s say, a few label examples of cars and a few label examples of things that are not cars, then we give our training set to the learning algorithm trained a classifier and then, you know, we may test it and show the new image and ask, “What is this new thing?”. And hopefully it will recognize that that is a car. To understand why we need nonlinear hypotheses, let’s take a look at some of the images of cars and maybe non-cars that we might feed to our learning algorithm. Let’s pick a couple of pixel locations in our images, so that’s pixel one location and pixel two location, and let’s plot this car, you know, at the location, at a certain point, depending on the intensities of pixel one and pixel two. And let’s do this with a few other images. So let’s take a different example of the car and you know, look at the same two pixel locations and that image has a different intensity for pixel one and a different intensity for pixel two. So, it ends up at a different location on the figure. And then let’s plot some negative examples as well. That’s a non-car, that’s a non-car. And if we do this for more and more examples using the pluses(+) to denote cars and minuses(-) to denote non-cars, what we’ll find is that the cars and non-cars end up lying in different regions of the space, and what we need therefore is some sort of non-linear hypotheses to try to separate out the two classes. What is the dimension of the feature space? Suppose we were to use just 50 by 50 pixel images. Now that suppose our images were pretty small ones, just 50 pixels on the side. Then we would have 2500 pixels, and so the dimension of our feature size will be N equals 2500 where our feature vector x is a list of all the pixel testings, you know, the pixel brightness of pixel one, the brightness of pixel two, and so on down to the pixel brightness of the last pixel where, you know, in a typical computer representation, each of these may be values between say 0 to 255 if it gives us the grayscale value. So we have n equals 2500, and that’s if we were using grayscale images. If we were using RGB images with separate red, green and blue values, we would have n equals 7500. So, if we were to try to learn a nonlinear hypothesis by including all the quadratic features, that is all the terms of the form, you know, $X_i$ times $X_j$, while with the 2500 pixels we would end up with a total of three million features. And that’s just too large to be reasonable; the computation would be very expensive to find and to represent all of these three million features per training example. So, simple logistic regression together with adding in maybe the quadratic or the cubic features that’s just not a good way to learn complex nonlinear hypotheses when n is large because you just end up with too many features. In the next few videos, I would like to tell you about Neural Networks, which turns out to be a much better way to learn complex hypotheses, complex nonlinear hypotheses even when your input feature space, even when n is large. And along the way I’ll also get to show you a couple of fun videos of historically important applications of Neural networks as well that I hope those videos that we’ll see later will be fun for you to watch as well. neurons and the brainNeural Networks are a pretty old algorithm that was originally motivated by the goal of having machines that can mimic the brain. Now in this class, of course I’m teaching Neural Networks to you because they work really well for different machine learning problems and not, certainly not because they’re logically motivated. In this video, I’d like to give you some of the background on Neural Networks. So that we can get a sense of what we can expect them to do. Both in the sense of applying them to modern day machinery problems, as well as for those of you that might be interested in maybe the big AI dream of someday building truly intelligent machines. Also, how Neural Networks might pertain to that. The origins of Neural Networks was as algorithms that try to mimic the brain and those a sense that if we want to build learning systems while why not mimic perhaps the most amazing learning machine we know about, which is perhaps the brain. Neural Networks came to be very widely used throughout the 1980’s and 1990’s and for various reasons as popularity diminished in the late 90’s. But more recently, Neural Networks have had a major recent resurgence. One of the reasons for this resurgence is that Neural Networks are computationally some what more expensive algorithm and so, it was only, you know, maybe somewhat more recently that computers became fast enough to really run large scale Neural Networks and because of that as well as a few other technical reasons which we’ll talk about later, modern Neural Networks today are the state of the art technique for many applications. So, when you think about mimicking the brain while one of the human brain does tell me same things, right? The brain can learn to see process images than to hear, learn to process our sense of touch. We can, you know, learn to do math, learn to do calculus, and the brain does so many different and amazing things. It seems like if you want to mimic the brain it seems like you have to write lots of different pieces of software to mimic all of these different fascinating, amazing things that the brain tell us, but does is this fascinating hypothesis that the way the brain does all of these different things is not worth like a thousand different programs, but instead, the way the brain does it is worth just a single learning algorithm. This is just a hypothesis but let me share with you some of the evidence for this. This part of the brain, that little red part of the brain, is your auditory cortex and the way you’re understanding my voice now is your ear is taking the sound signal and routing the sound signal to your auditory cortex and that’s what’s allowing you to understand my words. Neuroscientists have done the following fascinating experiments where you cut the wire from the ears to the auditory cortex and you re-wire, in this case an animal’s brain, so that the signal from the eyes to the optic nerve eventually gets routed to the auditory cortex. If you do this it turns out, the auditory cortex will learn to see. And this is in every single sense of the word see as we know it. So, if you do this to the animals, the animals can perform visual discrimination task and as they can look at images and make appropriate decisions based on the images and they’re doing it with that piece of brain tissue. Here’s another example. That red piece of brain tissue is your somatosensory cortex. That’s how you process your sense of touch. If you do a similar re-wiring process then the somatosensory cortex will learn to see. Because of this and other similar experiments, these are called neuro-rewiring experiments. There’s this sense that if the same piece of physical brain tissue can process sight or sound or touch then maybe there is one learning algorithm that can process sight or sound or touch. And instead of needing to implement a thousand different programs or a thousand different algorithms to do, you know, the thousand wonderful things that the brain does, maybe what we need to do is figure out some approximation or to whatever the brain’s learning algorithm is and implement that and that the brain learned by itself how to process these different types of data. To a surprisingly large extent, it seems as if we can plug in almost any sensor to almost any part of the brain and so, within the reason, the brain will learn to deal with it. Sensor representations in the brainHere are a few more examples. On the upper left is an example of learning to see with your tongue. The way it works is–this is actually a system called BrainPort undergoing FDA trials now to help blind people see–but the way it works is, you strap a grayscale camera to your forehead, facing forward, that takes the low resolution grayscale image of what’s in front of you and you then run a wire to an array of electrodes that you place on your tongue so that each pixel gets mapped to a location on your tongue where maybe a high voltage corresponds to a dark pixel and a low voltage corresponds to a bright pixel and, even as it does today, with this sort of system you and I will be able to learn to see, you know, in tens of minutes with our tongues. Here’s a second example of human echo location or human sonar. So there are two ways you can do this. You can either snap your fingers, or click your tongue. I can’t do that very well. But there are blind people today that are actually being trained in schools to do this and learn to interpret the pattern of sounds bouncing off your environment - that’s sonar. So, if after you search on YouTube, there are actually videos of this amazing kid who tragically because of cancer had his eyeballs removed, so this is a kid with no eyeballs. But by snapping his fingers, he can walk around and never hit anything. He can ride a skateboard. He can shoot a basketball into a hoop and this is a kid with no eyeballs. Third example is the Haptic Belt where if you have a strap around your waist, ring up buzzers and always have the northmost one buzzing. You can give a human a direction sense similar to maybe how birds can, you know, sense where north is. And, some of the bizarre example, but if you plug a third eye into a frog, the frog will learn to use that eye as well. So, it’s pretty amazing to what extent is as if you can plug in almost any sensor to the brain and the brain’s learning algorithm will just figure out how to learn from that data and deal with that data. And there’s a sense that if we can figure out what the brain’s learning algorithm is, and, you know, implement it or implement some approximation to that algorithm on a computer, maybe that would be our best shot at, you know, making real progress towards the AI, the artificial intelligence dream of someday building truly intelligent machines. Now, of course, I’m not teaching Neural Networks, you know, just because they might give us a window into this far-off AI dream, even though I’m personally, that’s one of the things that I personally work on in my research life. But the main reason I’m teaching Neural Networks in this class is because it’s actually a very effective state of the art technique for modern day machine learning applications. So, in the next few videos, we’ll start diving into the technical details of Neural Networks so that you can apply them to modern-day machine learning applications and get them to work well on problems. But for me, you know, one of the reasons the excite me is that maybe they give us this window into what we might do if we’re also thinking of what algorithms might someday be able to learn in a manner similar to humankind. neural-networksModel Representation ILet’s examine how we will represent a hypothesis function using neural networks. At a very simple level, neurons are basically computational units that take inputs (dendrites) as electrical inputs (called “spikes”) that are channeled to outputs (axons). In our model, our dendrites are like the input features $x_1⋯x_n​$, and the output is the result of our hypothesis function. In this model our $x_0​$ input node is sometimes called the “bias unit“. It is always equal to 1. In neural networks, we use the same logistic function as in classification, $\frac{1}{1+e^{−θ^Tx}}​$, yet we sometimes call it a sigmoid (logistic) activation function. In this situation, our “theta” parameters are sometimes called “weights”. Visually, a simplistic representation looks like:$$\begin{bmatrix}x_0 \newline x_1 \\ x_2 \\ \end{bmatrix}\rightarrow\begin{bmatrix}\ \ \ \\ \end{bmatrix}\rightarrow h_\theta(x)$$Our input nodes (layer 1), also known as the “input layer“, go into another node (layer 2), which finally outputs the hypothesis function, known as the “output layer“. We can have intermediate layers of nodes between the input and output layers called the “hidden layers“. In this example, we label these intermediate or “hidden” layer nodes $a^{(2)}_0 ⋯ a^{(2)}_n$ and call them “activation units“.$$\begin{align*}& a_i^{(j)} = \text{"activation" of unit $i$ in layer $j$} \\ & \Theta^{(j)} = \text{matrix of weights controlling function mapping from layer $j$ to layer $j+1$}\end{align*}$$If we had one hidden layer, it would look like:$$\begin{bmatrix}x_0 \\ x_1 \\ x_2 \\ x_3\end{bmatrix}\rightarrow\begin{bmatrix}a_1^{(2)} \\ a_2^{(2)} \\ a_3^{(2)} \\ \end{bmatrix}\rightarrow h_\theta(x)$$The values for each of the “activation” nodes is obtained as follows:$$\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \\ a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \\ a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \\ h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \\ \end{align*}$$This is saying that we compute our activation nodes by using a $3×4​$ matrix of parameters. We apply each row of the parameters to our inputs to obtain the value for one activation node. Our hypothesis output is the logistic function applied to the sum of the values of our activation nodes, which have been multiplied by yet another parameter matrix $Θ^{(2)}​$ containing the weights for our second layer of nodes. Each layer gets its own matrix of weights, $Θ^{(j)}$. The dimensions of these matrices of weights is determined as follows: If network has $s_j$ units in layer j and $s_{j+1}$ units in layer j+1, then $Θ^{(j)}$ will be of dimension $s_{j+1}×(s_j+1)$.If network has $s_j$ units in layer $j$ and $s_{j+1}$ units in layer j+1, then $Θ^(j)$ will be of dimension $s_{j+1}×(s_j+1)$. The +1 comes from the addition in $Θ^{(j)}$ of the “bias nodes,” $x_0$ and $Θ^{(j)}_0$. In other words the output nodes will not include the bias nodes while the inputs will. The following image summarizes our model representation: Example: If layer 1 has 2 input nodes and layer 2 has 4 activation nodes. Dimension of $Θ^{(1)}$ is going to be $4×3$ where $s_j=2$ and $s_j+1=4$ so $s_{j+1}×(sj+1)=4×3$ . Model Representation IITo re-iterate, the following is an example of a neural network:$$\begin{align*} a_1^{(2)} = g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2 + \Theta_{13}^{(1)}x_3) \\ a_2^{(2)} = g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2 + \Theta_{23}^{(1)}x_3) \\ a_3^{(2)} = g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2 + \Theta_{33}^{(1)}x_3) \\ h_\Theta(x) = a_1^{(3)} = g(\Theta_{10}^{(2)}a_0^{(2)} + \Theta_{11}^{(2)}a_1^{(2)} + \Theta_{12}^{(2)}a_2^{(2)} + \Theta_{13}^{(2)}a_3^{(2)}) \\ \end{align*}$$In this section we’ll do a vectorized implementation of the above functions. We’re going to define a new variable $z^{(j)}_k$ that encompasses the parameters inside our g function. In our previous example if we replaced by the variable z for all the parameters we would get:$$\begin{align*}a_1^{(2)} = g(z_1^{(2)}) \\ a_2^{(2)} = g(z_2^{(2)}) \\ a_3^{(2)} = g(z_3^{(2)}) \\ \end{align*}$$In other words, for layer j=2 and node k, the variable z will be:$$z_k^{(2)} = \Theta_{k,0}^{(1)}x_0 + \Theta_{k,1}^{(1)}x_1 + \cdots + \Theta_{k,n}^{(1)}x_n$$The vector representation of $x$ and $z_j$ is :$$\begin{align*}x = \begin{bmatrix}x_0 \\ x_1 \\\cdots \\ x_n\end{bmatrix}, z^{(j)} = \begin{bmatrix}z_1^{(j)} \\ z_2^{(j)} \\\cdots \\ z_n^{(j)}\end{bmatrix}\end{align*}$$Setting $x=a^{(1)}$, we can rewrite the equation as:$$z^{(j)} = \Theta^{(j-1)}a^{(j-1)}$$We are multiplying our matrix $Θ^{(j−1)}$ with dimensions $s_j×(n+1)$ (where $s_j$ is the number of our activation nodes) by our vector $a^{(j−1)}$ with height $(n+1)$. This gives us our vector $z(j)$ with height $s_j$. Now we can get a vector of our activation nodes for layer $j$ as follows:$$a^{(j)} = g(z^{(j)})$$Where our function g can be applied element-wise to our vector $z^{(j)}$. We can then add a bias unit (equal to 1) to layer j after we have computed $a^{(j)}$. This will be element $a^{(j)}_0$ and will be equal to 1. To compute our final hypothesis, let’s first compute another $z$ vector:$$z^{(j+1)} = \Theta^{(j)}a^{(j)}$$We get this final $z$ vector by multiplying the next theta matrix after $Θ^{(j−1)}$ with the values of all the activation nodes we just got. This last theta matrix $Θ^{(j)}$ will have only one row which is multiplied by one column $a^{(j)}$ so that our result is a single number. We then get our final result with:$$h_\Theta(x) = a^{(j+1)} = g(z^{(j+1)})$$ Notice that in this last step, between layer j and layer j+1, we are doing exactly the same thing as we did in logistic regression. Adding all these intermediate layers in neural networks allows us to more elegantly produce interesting and more complex non-linear hypotheses. Neural network learning its own features let’s say I cover up the left path of this picture for now. If you look at what’s left in this picture. This looks a lot like logistic regression where what we’re doing is we’re using that note, that’s just the logistic regression unit and we’re using that to make a prediction h of x. And concretely, what the hypotheses is outputting is h of x is going to be equal to g which is my sigmoid activation function times theta 0 times a0 is equal to 1 plus theta 1 plus theta 2 times a2 plus theta 3 times a3 whether values a1, a2, a3 are those given by these three given units. Now, to be actually consistent to my early notation. Actually, we need to, you know, fill in these superscript 2’s here everywhere and I also have these indices 1 there because I have only one output unit, but if you focus on the blue parts of the notation. This is, you know, this looks awfully like the standard logistic regression model, except that I now have a capital theta instead of lower case theta. And what this is doing is just logistic regression. But where the features fed into logistic regression are these values computed by the hidden layer. Just to say that again, what this neural network is doing is just like logistic regression, except that rather than using the original features x1, x2, x3, is using these new features a1, a2, a3. Again, we’ll put the superscripts there, you know, to be consistent with the notation. And the cool thing about this, is that the features a1, a2, a3, they themselves are learned as functions of the input. Concretely, the function mapping from layer 1 to layer 2, that is determined by some other set of parameters, theta 1. So it’s as if the neural network, instead of being constrained to feed the features x1, x2, x3 to logistic regression. It gets to learn its own features, a1, a2, a3, to feed into the logistic regression and as you can imagine depending on what parameters it chooses for theta 1.You can learn some pretty interesting and complex features and therefore you can end up with a better hypotheses than if you were constrained to use the raw features x1, x2 or x3 or if you will constrain to say choose the polynomial terms, you know, x1, x2, x3, and so on. But instead, this algorithm has the flexibility to try to learn whatever features at once, using these a1, a2, a3 in order to feed into this last unit that’s essentially a logistic regression here.I realized this example is described as a somewhat high level and so I’m not sure if this intuition of the neural network, you know, having more complex features will quite make sense yet, but if it doesn’t yet in the next two videos I’m going to go through a specific example of how a neural network can use this hidden there to compute more complex features to feed into this final output layer and how that can learn more complex hypotheses. So, in case what I’m saying here doesn’t quite make sense, stick with me for the next two videos and hopefully out there working through those examples this explanation will make a little bit more sense. But just the point O. Other network architecturesYou can have neural networks with other types of diagrams as well, and the way that neural networks are connected, that’s called the architecture. So the term architecture refers to how the different neurons are connected to each other. This is an example of a different neural network architecture and once again you may be able to get this intuition of how the second layer, here we have three heading units that are computing some complex function maybe of the input layer, and then the third layer can take the second layer’s features and compute even more complex features in layer three so that by the time you get to the output layer, layer four, you can have even more complex features of what you are able to compute in layer three and so get very interesting nonlinear hypotheses. By the way, in a network like this, layer one, this is called an input layer. Layer four is still our output layer, and this network has two hidden layers. So anything that’s not an input layer or an output layer is called a hidden layer. So, hopefully from this video you’ve gotten a sense of how the feed forward propagation step in a neural network works where you start from the activations of the input layer and forward propagate that to the first hidden layer, then the second hidden layer, and then finally the output layer. And you also saw how we can vectorize that computation. In the next, I realized that some of the intuitions in this video of how, you know, other certain layers are computing complex features of the early layers. I realized some of that intuition may be still slightly abstract and kind of a high level. And so what I would like to do in the two videos is work through a detailed example of how a neural network can be used to compute nonlinear functions of the input and hope that will give you a good sense of the sorts of complex nonlinear hypotheses we can get out of Neural Networks. ApplicationsExamples and Intuitions IA simple example of applying neural networks is by predicting x1 AND x2, which is the logical ‘and’ operator and is only true if both x1 and x2 are 1. The graph of our functions will look like:$$\begin{align*}\begin{bmatrix}x_0 \\ x_1 \\ x_2\end{bmatrix} \rightarrow\begin{bmatrix}g(z^{(2)})\end{bmatrix} \rightarrow h_\Theta(x)\end{align*}$$Remember that x0 is our bias variable and is always 1. Let’s set our first theta matrix as:$$\Theta^{(1)} =\begin{bmatrix}-30 &amp; 20 &amp; 20\end{bmatrix}$$This will cause the output of our hypothesis to only be positive if both x1 and x2 are 1. In other words:$$\begin{align*}& h_\Theta(x) = g(-30 + 20x_1 + 20x_2) \\ \\ & x_1 = 0 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-30) \approx 0 \\ & x_1 = 0 \ \ and \ \ x_2 = 1 \ \ then \ \ g(-10) \approx 0 \\ & x_1 = 1 \ \ and \ \ x_2 = 0 \ \ then \ \ g(-10) \approx 0 \\ & x_1 = 1 \ \ and \ \ x_2 = 1 \ \ then \ \ g(10) \approx 1\end{align*}$$ So we have constructed one of the fundamental operations in computers by using a small neural network rather than using an actual AND gate. Neural networks can also be used to simulate all the other logical gates. The following is an example of the logical operator ‘OR’, meaning either x1 is true or x2 is true, or both: Examples and Intuitions IIThe $Θ^{(1)}$ matrices for AND, NOR, and OR are :$$\begin{align*}AND:\\ \Theta^{(1)} &=\begin{bmatrix}-30 & 20 & 20\end{bmatrix} \\ NOR:\\ \Theta^{(1)} &= \begin{bmatrix}10 & -20 & -20\end{bmatrix} \\ OR:\\ \Theta^{(1)} &= \begin{bmatrix}-10 & 20 & 20\end{bmatrix} \\ \end{align*}$$We can combine these to get the XNOR logical operator (which gives 1 if $x_1$ and $x_2$ are both 0 or both 1).$$\begin{align*}\begin{bmatrix}x_0 \\ x_1 \\ x_2\end{bmatrix} \rightarrow\begin{bmatrix}a_1^{(2)} \\ a_2^{(2)} \end{bmatrix} \rightarrow\begin{bmatrix}a^{(3)}\end{bmatrix} \rightarrow h_\Theta(x)\end{align*}$$For the transition between the first and second layer, we’ll use a $Θ^{(1)}$ matrix that combines the values for AND and NOR:$$\Theta^{(1)} =\begin{bmatrix}-30 &amp; 20 &amp; 20 \\ 10 &amp; -20 &amp; -20\end{bmatrix}$$For transition between the second and third layer, we’ll use a $Θ^{(2)}$ matrix that uses the value for OR:$$\Theta^{(2)} =\begin{bmatrix}-10 &amp; 20 &amp; 20\end{bmatrix}$$Let’s write out the values for all our nodes:$$\begin{align*}& a^{(2)} = g(\Theta^{(1)} \cdot x) \\ & a^{(3)} = g(\Theta^{(2)} \cdot a^{(2)}) \\ & h_\Theta(x) = a^{(3)}\end{align*}$$And there we have the XNOR operator using a hidden layer with two nodes! The following summarizes the above algorithm: Multiclass ClassificationTo classify data into multiple classes, we let our hypothesis function return a vector of values. Say we wanted to classify our data into one of four categories. We will use the following example to see how this classification is done. This algorithm takes as input an image and classifies it accordingly: We can define our set of resulting classes as y:$$y^{(i)}=\begin{bmatrix}1\\0\\0\\0\end{bmatrix},\begin{bmatrix}0\\1\\0\\0\end{bmatrix},\begin{bmatrix}0\\0\\1\\0\end{bmatrix},\begin{bmatrix}0\\0\\0\\1\end{bmatrix}$$Each $y^{(i)}$ represents a different image corresponding to either a car, pedestrian, truck, or motorcycle. The inner layers, each provide us with some new information which leads to our final hypothesis function. The setup looks like:$$\begin{bmatrix}x_0\\x_1\\x_2\\ \cdots \\ x_n\end{bmatrix} \rightarrow \begin{bmatrix}a_0^{(2)}\\a_1^{(2)}\\ \cdots \\ a_n^{(2)}\end{bmatrix} \rightarrow \rightarrow \begin{bmatrix}a_0^{(3)}\\a_1^{(3)}\\ \cdots \\ a_n^{(3)}\end{bmatrix} \rightarrow \cdots \rightarrow \begin{bmatrix}h_\Theta(x)_1\\h_\Theta(x)_2\\h_\Theta(x)_3\\h_\Theta(x)_4\end{bmatrix}$$Our resulting hypothesis for one set of inputs may look like:$$h_\Theta(x) =\begin{bmatrix}0 \\ 0 \\ 1 \\ 0 \\ \end{bmatrix}$$In which case our resulting class is the third one down, or $h_Θ(x)_3$, which represents the motorcycle. Word Dict pertain to : relate to 与 … 相关 cortex 皮层；皮质；(尤指)大脑皮层 optic◙ adj. [usually before noun]• (technical 术语) connected with the eye or the sense of sight• 眼的；视觉的: »the optic nerve (= from the eye to the brain) 视神经 route ◙ verb• [VN , usually +adv. / prep.] to send sb / sth by a particular route• 按某路线发送: »Satellites route data all over the globe. 衞星向全球各地传递信息。 wire ~ sb / sth up (to sth) | ~ sb / sth to sth to connect sb / sth to a piece of equipment, especially a tape recorder or computer system • 将…连接到(磁带录音机、计算机等设备): »He was wired up to a police tape recorder. 他被连接到了警方的录音机上。 [C, U] a piece of wire that is used to carry an electric current or signal• 电线；导线: »overhead wires架空电线 »fuse wire保险丝 »The telephone wires had been cut.电话线被割断了。 tissue◙ noun [U] (also tis∙sues [pl.]) a collection of cells that form the different parts of humans, animals and plants • (人、动植物细胞的)组织:»muscle / brain / nerve, etc. tissue 肌肉、大脑、神经等组织»scar tissue 瘢痕组织 [C] a piece of soft paper that absorbs liquids, used especially as a handkerchief • (尤指用作手帕的)纸巾,手巾纸:»a box of tissues 一盒纸巾 (also ‘tissue paper) [U] very thin paper used for wrapping and packing things that break easily • (用于包装易碎物品的)薄纸,绵纸 【IDIOMS】◘ a tissue of &apos;lies •(literary) a story, an excuse, etc. that is full of lies • 一派谎言 far-off : 遥远的 dendrites ◙ noun• (biology 生) a short branch at the end of a nerve cell, which receives signals from other cells• 树突(位于神经元末端的细分支,接收其他神经元传来的冲动) axon ◙ noun • (biology 生) the long thin part of a nerve cell along which signals are sent to other cells • 轴突(神经细胞的突起,将信号发送到其他细胞) propagate 传播，宣传]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[07_regularization note7]]></title>
    <url>%2F2018%2F01%2F07%2F07_regularization%2F</url>
    <content type="text"><![CDATA[NoteThis personal note is written after studying the opening course on the coursera website, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. The Problem of OverfittingConsider the problem of predicting $y$ from $x ∈ R$. The leftmost figure below shows the result of fitting a $y = θ_0+θ_1x$ to a dataset. We see that the data doesn’t really lie on straight line, and so the fit is not very good. Instead, if we had added an extra feature $x_2$ , and fit $y=θ_0+θ_1x+θ_2x^2$ , then we obtain a slightly better fit to the data (See middle figure). Naively, it might seem that the more features we add, the better. However, there is also a danger in adding too many features: The rightmost figure is the result of fitting a $5^{th}$ order polynomial $y=\sum_{j=0}^{5}\theta_jx^j$ . We see that even though the fitted curve passes through the data perfectly, we would not expect this to be a very good predictor of, say, housing prices (y) for different living areas(x). Without formally defining what these terms mean, we’ll say the figure on the left shows an instance of underfitting —in which the data clearly shows structure not captured by the model—and the figure on the right is an example of overfitting . Underfitting, or high bias, is when the form of our hypothesis function h maps poorly to the trend of the data. It is usually caused by a function that is too simple or uses too few features. At the other extreme, overfitting, or high variance, is caused by a hypothesis function that fits the available data but does not generalize well to predict new data. It is usually caused by a complicated function that creates a lot of unnecessary curves and angles unrelated to the data. This terminology is applied to both linear and logistic regression. There are two main options to address the issue of overfitting: 1) Reduce the number of features: ​ Manually select which features to keep. ​ Use a model selection algorithm (studied later in the course, such as PCA). 2) Regularization ​ Keep all the features, but reduce the magnitude of parameters $θ_j$. ​ Regularization works well when we have a lot of slightly useful features. Cost FunctionNote: [5:18 - There is a typo. It should be $\sum_{j=1}^{n} \theta _j ^2$ instead of $\sum_{i=1}^{n} \theta _j ^2$] If we have overfitting from our hypothesis function, we can reduce the weight that some of the terms in our function carry by increasing their cost. Say we wanted to make the following function more quadratic:$$\theta_0 + \theta_1x + \theta_2x^2 + \theta_3x^3 + \theta_4x^4$$We’ll want to eliminate the influence of $θ_3x_3$ and $θ_4x_4$ . Without actually getting rid of these features or changing the form of our hypothesis, we can instead modify our cost function :$$min_\theta\ \dfrac{1}{2m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + 1000\cdot\theta_3^2 + 1000\cdot\theta_4^2$$We’ve added two extra terms at the end to inflate the cost of $θ_3$ and $θ_4$. Now, in order for the cost function to get close to zero, we will have to reduce the values of $θ_3$ and $θ_4$ to near zero. This will in turn greatly reduce the values of $θ_3x_3$ and $θ_4x_4$ in our hypothesis function. As a result, we see that the new hypothesis (depicted by the pink curve) looks like a quadratic function but fits the data better due to the extra small terms $θ_3x_3$ and $θ_4x_4$. We could also regularize all of our theta parameters in a single summation as:$$min_\theta\ \dfrac{1}{2m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda\ \sum_{j=1}^n \theta_j^2$$The λ, or lambda, is the regularization parameter . It determines how much the costs of our theta parameters are inflated. Using the above cost function with the extra summation, we can smooth the output of our hypothesis function to reduce overfitting. If lambda is chosen to be too large, it may smooth out the function too much and cause underfitting. Hence, what would happen if λ=0 or is too small ? Regularized Linear RegressionNote: [8:43 - It is said that X is non-invertible if m ≤ n. The correct statement should be that X is non-invertible if m &lt; n, and may be non-invertible if m = n. We can apply regularization to both linear regression and logistic regression. We will approach linear regression first.$$J(\theta)=\frac{1}{2m} \left[ \sum\limits_{i=1}^{m}\left(h_\theta(x^{(i)})-y^{(i)}\right)^2 + \lambda\sum_{j=1}^{n}\theta_j^2 \right]$$ Gradient DescentWe will modify our gradient descent function to separate out θ0θ0 from the rest of the parameters because we do not want to penalize$$\begin{align*} & \text{Repeat}\ \lbrace \\ & \ \ \ \ \theta_0 := \theta_0 - \alpha\ \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_0^{(i)} \\ & \ \ \ \ \theta_j := \theta_j - \alpha\ \left[ \left( \frac{1}{m}\ \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \right) + \frac{\lambda}{m}\theta_j \right] &\ \ \ \ \ \ \ \ \ \ j \in \lbrace 1,2...n\rbrace\\ & \rbrace \end{align*}$$The term $\frac{λ}{m} θ_j$ performs our regularization. With some manipulation our update rule can also be represented as:$$\theta_j := \theta_j(1 - \alpha\frac{\lambda}{m}) - \alpha\frac{1}{m}\sum_{i=1}^m(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)}$$The first term in the above equation, $1−α\frac{λ}{m}$ will always be less than 1. Intuitively you can see it as reducing the value of $θ_j$ by some amount on every update. Notice that the second term is now exactly the same as it was before. Normal EquationNow let’s approach regularization using the alternate method of the non-iterative normal equation. To add in regularization, the equation is the same as our original, except that we add another term inside the parentheses:$$\begin{align*}& \theta = \left( X^TX + \lambda \cdot L \right)^{-1} X^Ty \\ & \text{where}\ \ L = \begin{bmatrix} 0 & & & & \\ & 1 & & & \\ & & 1 & & \\ & & & \ddots & \\ & & & & 1 \\\end{bmatrix}\end{align*}$$L is a matrix with 0 at the top left and 1’s down the diagonal, with 0’s everywhere else. It should have dimension (n+1)×(n+1). Intuitively, this is the identity matrix (though we are not including $x_0$), multiplied with a single real number λ. Recall that if m &lt; n, then $X^TX$ is non-invertible. However, when we add the term λ⋅L, then $X^TX + λ⋅L$ becomes invertible.(Note: which is proved) Regularized Logistic RegressionWe can regularize logistic regression in a similar way that we regularize linear regression. As a result, we can avoid overfitting. The following image shows how the regularized function, displayed by the pink line, is less likely to overfit than the non-regularized function represented by the blue line: Cost FunctionRecall that our cost function for logistic regression was:$$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)})) \large]$$We can regularize this equation by adding a term to the end:$$J(\theta) = - \frac{1}{m} \sum_{i=1}^m \large[ y^{(i)}\ \log (h_\theta (x^{(i)})) + (1 - y^{(i)})\ \log (1 - h_\theta(x^{(i)}))\large] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2$$The second sum, $∑_n^{j=1}θ^2_j$ means to explicitly exclude the bias term, $θ_0$. I.e. the θ vector is indexed from 0 to n (holding n+1 values, $θ_0$ through $θ_n$), and this sum explicitly skips $θ_0$, by running from 1 to n, skipping 0. Thus, when computing the equation, we should continuously update the two following equations: 12345678910import numpy as np;def costReg(theta, X, y, learningRate): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X*theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X*theta.T))) reg = (learningRate / (2 * len(X)) * np.sum(np.power(theta[:,1:theta.shape[1]],2)) return np.sum(first - second) / (len(X)) + reg Advanced optimizationLet’s talk about how to get regularized linear regression to work using the more advanced optimization methods. And just to remind you for those methods what we needed to do was to define the function that’s called the cost function, that takes us input the parameter vector theta and once again in the equations we’ve been writing here we used 0 index vectors. So we had theta 0 up to theta N. But because Octave indexes the vectors starting from 1. Theta 0 is written in Octave as theta 1. Theta 1 is written in Octave as theta 2, and so on down to theta N plus 1. And what we needed to do was provide a function. Let’s provide a function called cost function that we would then pass in to what we have, what we saw earlier. We will use the fminunc and then you know at cost function, and so on, right. But the fminunc was function minimization unconstrained in Octave and this will work with fminunc was what will take the cost function and minimize it for us. So the two main things that the cost function needed to return were first J-val. And for that, we need to write code to compute the costfunction J of theta. Now, when we’re using regularized logistic regression, of course the costfunction J of theta changes and, in particular, now a cost function needs to include this additional regularization term at the end as well. So, when you compute j of theta be sure to include that term at the end. And then, the other thing that this cost function thing needs to derive with a gradient. So gradient one needs to be set to the partial derivative of J of theta with respect to theta zero, gradient two needs to be set to that, and so on. Once again, the index is off by one. Right, because of the indexing from one Octave users. And looking at these terms. This term over here. We actually worked this out on a previous slide is actually equal to this. It doesn’t change. Because the derivative for theta zero doesn’t change. Compared to the version without regularization. And the other terms do change. And in particular the derivative respect to theta one. We worked this out on the previous slide as well. Is equal to, you know, the original term and then minus londer M times theta 1. Just so we make sure we pass this correctly. And we can add parentheses here. Right, so the summation doesn’t extend. And similarly, you know, this other term here looks like this, with this additional term that we had on the previous slide, that corresponds to the gradient from their regularization objective. So if you implement this cost function and pass this into fminunc or to one of those advanced optimization techniques, that will minimize the new regularized cost function J of theta. And the parameters you get out will be the ones that correspond to logistic regression with regularization. So, now you know how to implement regularized logistic regression. When I walk around Silicon Valley, I live here in Silicon Valley, there are a lot of engineers that are frankly, making a ton of money for their companies using machine learning algorithms. And I know we’ve only been, you know, studying this stuff for a little while. But if you understand linear regression, the advanced optimization algorithms and regularization, by now, frankly, you probably know quite a lot more machine learning than many, certainly now, but you probably know quite a lot more machine learning right now than frankly, many of the Silicon Valley engineers out there having very successful careers. You know, making tons of money for the companies. Or building products using machine learning algorithms. So, congratulations. You’ve actually come a long ways. And you can actually, you actually know enough to apply this stuff and get to work for many problems. So congratulations for that. But of course, there’s still a lot more that we want to teach you, and in the next set of videos after this, we’ll start to talk about a very powerful cause of non-linear classifier. So whereas linear regression, logistic regression, you know, you can form polynomial terms, but it turns out that there are much more powerful nonlinear quantifiers that can then sort of polynomial regression. And in the next set of videos after this one, I’ll start telling you about them. So that you have even more powerful learning algorithms than you have now to apply to different problems.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[06_logistic-regression note6]]></title>
    <url>%2F2018%2F01%2F06%2F06_logistic-regression%2F</url>
    <content type="text"><![CDATA[NoteThis personal note is written after studying the opening course on the coursera website, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. ClassificationTo attempt classification, one method is to use linear regression and map all predictions greater than 0.5 as a 1 and all less than 0.5 as a 0. However, this method doesn’t work well because classification is not actually a linear function. The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which $y$ can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.) For instance, if we are trying to build a spam classifier for email, then $x^{(i)}$ may be some features of a piece of email, and $y$ may be 1 if it is a piece of spam mail, and 0 otherwise. Hence, $y∈\{0,1\}$ . 0 is also called the negative class, and 1 the positive class, and they are sometimes also denoted by the symbols “-” and “+” . Given $x^{(i)}$, the corresponding $y^{(i)}$ is also called the label for the training example. Hypothesis RepresentationWe could approach the classification problem ignoring the fact that $y$ is discrete-valued, and use our old linear regression algorithm to try to predict $y$ given $x$. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn’t make sense for $h_{θ}(x)$ to take values larger than 1 or smaller than $0$ when we know that $y ∈ \{0, 1\}$. To fix this, let’s change the form for our hypotheses $h_{θ}(x)$ to satisfy $0≤h_{θ}(x)≤1$. This is accomplished by plugging $θ^Tx$ into the Logistic Function . Our new form uses the “Sigmoid Function” , also called the “Logistic Function” :$$\begin{align*}& h_\theta (x) = g ( \theta^T x ) \\ \\& z = \theta^T x \\& g(z) = \dfrac{1}{1 + e^{-z}}\end{align*}$$Using python to implement it : 123import numpy as np;def sigmoid(z): return 1 / (1 + np.exp(-z)) The following image shows us what the sigmoid function looks like: The function $g(z)$ , shown here, maps any real number to the $(0, 1)$ interval, making it useful for transforming an arbitrary-valued function into a function better suited for classification. $h_{θ}(x)$ will give us the probability that our output is 1. For example, $h_{θ}(x)=0.7$ gives us a probability of $70\%$ that our output is 1. Our probability that our prediction is 0 is just the complement of our probability that it is 1 (e.g. if probability that it is 1 is $70\%$, then the probability that it is 0 is $30\%$).$$\begin{align*}& h_\theta(x) = P(y=1 | x ; \theta) = 1 - P(y=0 | x ; \theta) \\ & P(y = 0 | x;\theta) + P(y = 1 | x ; \theta) = 1\end{align*}$$ Decision BoundaryIn order to get our discrete 0 or 1 classification, we can translate the output of the hypothesis function as follows:$$\begin{align*}& h_\theta(x) \geq 0.5 \rightarrow y = 1 \\ & h_\theta(x) < 0.5 \rightarrow y = 0 \\ \end{align*}$$The way our logistic function g behaves is that when its input is greater than or equal to zero, its output is greater than or equal to 0.5:$$\begin{align*}& g(z) \geq 0.5 \\ & when \; z \geq 0\end{align*}$$From these statements we can now say:$$\begin{align*}& \theta^T x \geq 0 \Rightarrow y = 1 \\ & \theta^T x < 0 \Rightarrow y = 0 \\ \end{align*}$$The decision boundary is the line that separates the area where y = 0 and where y = 1. It is created by our hypothesis function. Example :$$\begin{align*}& \theta = \begin{bmatrix}5 \\ -1 \\ 0\end{bmatrix} \\ & y = 1 \; if \; 5 + (-1) x_1 + 0 x_2 \geq 0 \\ & 5 - x_1 \geq 0 \\ & - x_1 \geq -5 \\& x_1 \leq 5 \\ \end{align*}$$In this case, our decision boundary is a straight vertical line placed on the graph where $x_1=5$ , and everything to the left of that denotes $y = 1$ , while everything to the right denotes $y = 0$ . Again, the input to the sigmoid function g(z) (e.g. $θ^TX$ ) doesn’t need to be linear, and could be a function that describes a circle (e.g. $z=θ_0+θ_1x^2_1+θ_2x^2_2$ ) or any shape to fit our data. Cost FunctionWe cannot use the same cost function that we use for linear regression because the Logistic Function will cause the output to be wavy, causing many local optima. In other words, it will not be a convex function. Instead, our cost function for logistic regression looks like:$$\begin{align*}& J(\theta) = \dfrac{1}{m} \sum_{i=1}^m \mathrm{Cost}(h_\theta(x^{(i)}),y^{(i)}) \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(h_\theta(x)) \; & \text{if y = 1} \newline & \mathrm{Cost}(h_\theta(x),y) = -\log(1-h_\theta(x)) \; & \text{if y = 0}\end{align*}$$$$\begin{align*}& \mathrm{Cost}(h_\theta(x),y) = 0 \text{ if } h_\theta(x) = y \\ & \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 0 \; \mathrm{and} \; h_\theta(x) \rightarrow 1 \\ & \mathrm{Cost}(h_\theta(x),y) \rightarrow \infty \text{ if } y = 1 \; \mathrm{and} \; h_\theta(x) \rightarrow 0 \\ \end{align*}$$ If our correct answer ‘y’ is 0, then the cost function will be 0 if our hypothesis function also outputs 0. If our hypothesis approaches 1, then the cost function will approach infinity. If our correct answer ‘y’ is 1, then the cost function will be 0 if our hypothesis function outputs 1. If our hypothesis approaches 0, then the cost function will approach infinity. Note that writing the cost function in this way guarantees that $J(θ)$ is convex for logistic regression. Simplified Cost Function and Gradient DescentNote: [6:53 - the gradient descent equation should have a 1/m factor] We can compress our cost function’s two conditional cases into one case:$$\mathrm{Cost}(h_\theta(x),y) = - y \; \log(h_\theta(x)) - (1 - y) \log(1 - h_\theta(x))$$Notice that when y is equal to 1, then the second term $(1−y)log(1−h_θ(x))$ will be zero and will not affect the result. If y is equal to 0, then the first term $−ylog(h_θ(x))$ will be zero and will not affect the result. We can fully write out our entire cost function as follows: $$J(\theta) = \frac{1}{m} \displaystyle \sum_{i=1}^m [-y^{(i)}\log (h_\theta (x^{(i)})) - (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))]$$ A vectorized implementation$$\begin{align*} & h = g(X\theta)\newline & J(\theta) = \frac{1}{m} \cdot \left(-y^{T}\log(h)-(1-y)^{T}\log(1-h)\right) \end{align*}$$ Using python to implement it : 12345678import numpy as npdef cost(theta, X, y): theta = np.matrix(theta) X = np.matrix(X) y = np.matrix(y) first = np.multiply(-y, np.log(sigmoid(X* theta.T))) second = np.multiply((1 - y), np.log(1 - sigmoid(X* theta.T))) return np.sum(first - second) / (len(X)) Gradient DescentRemember that the general form of gradient descent is:$$\begin{align*}& Repeat \; \lbrace \\ & \; \theta_j := \theta_j - \alpha \dfrac{\partial}{\partial \theta_j}J(\theta) \\ & \rbrace\end{align*}$$We can work out the derivative part using calculus to get:$$\begin{align*} & Repeat \; \lbrace \\ & \; \theta_j := \theta_j - \frac{\alpha}{m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \\ & \rbrace \end{align*}$$The detail mathematical process :$$\begin{align*} J(\theta) &= \frac{1}{m} \displaystyle \sum_{i=1}^m [-y^{(i)}\log (h_\theta (x^{(i)})) - (1 - y^{(i)})\log (1 - h_\theta(x^{(i)}))] \\ &= -\frac{1}{m} \displaystyle \sum_{i=1}^m [{{y}^{(i)}}\log ( {h_\theta} {{x}^{(i)}} ) )+ ( 1-{{y}^{(i)}} )\log ( 1-{h_\theta} ( {{x}^{(i)}} ) )] \\ & = -\frac{1}{m} \displaystyle \sum_{i=1}^m [{{y}^{(i)}}\log ( \frac{1}{1+{{e}^{-{\theta^T}{{x}^{(i)}}}}} )+( 1-{{y}^{(i)}})\log ( 1-\frac{1}{1+{{e}^{-{\theta^T}{{x}^{(i)}}}}} )] \\ & = -\frac{1}{m} \displaystyle \sum_{i=1}^m [ -{{y}^{(i)}}\log ( 1+{{e}^{-{\theta^T}{{x}^{(i)}}}} )- ( 1-{{y}^{(i)}} )\log ( 1+{{e}^{{\theta^T}{{x}^{(i)}}}} )] \\ \end{align*}$$ $$\begin{align*} \frac{\partial }{\partial {\theta_{j}}}J\left( \theta \right) &= -\frac{1}{m}\frac{\partial }{\partial {\theta_{j}}}[\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( 1+{{e}^{-{\theta^{T}}{{x}^{(i)}}}} \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1+{{e}^{{\theta^{T}}{{x}^{(i)}}}} \right)]}] \\ & =\frac{\partial }{\partial {\theta_{j}}}[-\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\log \left( 1+{{e}^{-{\theta^{T}}{{x}^{(i)}}}} \right)-\left( 1-{{y}^{(i)}} \right)\log \left( 1+{{e}^{{\theta^{T}}{{x}^{(i)}}}} \right)]}] \\ & =-\frac{1}{m}\sum\limits_{i=1}^{m}{[-{{y}^{(i)}}\frac{-x_{j}^{(i)}{{e}^{-{\theta^{T}}{{x}^{(i)}}}}}{1+{{e}^{-{\theta^{T}}{{x}^{(i)}}}}}-\left( 1-{{y}^{(i)}} \right)\frac{x_j^{(i)}{{e}^{{\theta^T}{{x}^{(i)}}}}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}}}] \\ & =-\frac{1}{m}\sum\limits_{i=1}^{m}{{y}^{(i)}}\frac{x_j^{(i)}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}}-\left( 1-{{y}^{(i)}} \right)\frac{x_j^{(i)}{{e}^{{\theta^T}{{x}^{(i)}}}}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}}] \\ & =-\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{{{y}^{(i)}}x_j^{(i)}-x_j^{(i)}{{e}^{{\theta^T}{{x}^{(i)}}}}+{{y}^{(i)}}x_j^{(i)}{{e}^{{\theta^T}{{x}^{(i)}}}}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}}} \\ & =-\frac{1}{m}\sum\limits_{i=1}^{m}{\frac{{{y}^{(i)}}\left( 1\text{+}{{e}^{{\theta^T}{{x}^{(i)}}}} \right)-{{e}^{{\theta^T}{{x}^{(i)}}}}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}}x_j^{(i)}} \\ & =-\frac{1}{m}\sum\limits_{i=1}^{m}{({{y}^{(i)}}-\frac{{{e}^{{\theta^T}{{x}^{(i)}}}}}{1+{{e}^{{\theta^T}{{x}^{(i)}}}}})x_j^{(i)}} \\ & =-\frac{1}{m}\sum\limits_{i=1}^{m}{({{y}^{(i)}}-\frac{1}{1+{{e}^{-{\theta^T}{{x}^{(i)}}}}})x_j^{(i)}} \\ & =-\frac{1}{m}\sum\limits_{i=1}^{m}{[{{y}^{(i)}}-{h_\theta}\left( {{x}^{(i)}} \right)]x_j^{(i)}} \\ & =\frac{1}{m}\sum\limits_{i=1}^{m}{[{h_\theta}\left( {{x}^{(i)}} \right)-{{y}^{(i)}}]x_j^{(i)}} \end{align*}$$ Notice that this algorithm is identical to the one we used in linear regression. We still have to simultaneously update all values in theta. A vectorized implementation$$\theta := \theta - \frac{\alpha}{m} X^{T} (g(X \theta ) - \vec{y})$$ NoteThe idea of feature scaling also applies to gradient descent for logistic regression. And yet we have features that are on different scale, then applying feature scaling can also make grading descent run faster for logistic regression. Advanced OptimizationNote: [7:35 - ‘100’ should be 100 instead. The value provided should be an integer and not a character string.] “Conjugate gradient”, “BFGS”, and “L-BFGS” are more sophisticated, faster ways to optimize $θ$ that can be used instead of gradient descent. We suggest that you should not write these more sophisticated algorithms yourself (unless you are an expert in numerical computing) but use the libraries instead, as they’re already tested and highly optimized. Octave provides them. These algorithms actually do more sophisticated things than just pick a good learning rate, and so they often end up converging much faster than gradient descent. These algorithms actually do more sophisticated things than just pick a good learning rate, and so they often end up converging much faster than gradient descent, but detailed discussion of exactly what they do is beyond the scope of this course. In fact, I actually used to have used these algorithms for a long time, like maybe over a decade, quite frequently, and it was only, you know, a few years ago that I actually figured out for myself the details of what conjugate gradient, BFGS and O-BFGS do. So it is actually entirely possible to use these algorithms successfully and apply to lots of different learning problems without actually understanding the inter-loop of what these algorithms do. If these algorithms have a disadvantage, I’d say that the main disadvantage is that they’re quite a lot more complex than gradient descent. And in particular, you probably should not implement these algorithms - conjugate gradient, L-BGFS, BFGS - yourself unless you’re an expert in numerical computing. Instead, just as I wouldn’t recommend that you write your own code to compute square roots of numbers or to compute inverses of matrices, for these algorithms also what I would recommend you do is just use a software library. So, you know, to take a square root what all of us do is use some function that someone else has written to compute the square roots of our numbers. And fortunately, Octave and the closely related language MATLAB - we’ll be using that - Octave has a very good. Has a pretty reasonable library implementing some of these advanced optimization algorithms. And so if you just use the built-in library, you know, you get pretty good results. I should say that there is a difference between good and bad implementations of these algorithms. And so, if you’re using a different language for your machine learning application, if you’re using C, C++, Java, and so on, you might want to try out a couple of different libraries to make sure that you find a good library for implementing these algorithms. Because there is a difference in performance between a good implementation of, you know, contour gradient or LPFGS versus less good implementation of contour gradient or LPFGS. We first need to provide a function that evaluates the following two functions for a given input value $θ$: We can write a single function that returns both of these: 1234function [jVal, gradient] = costFunction(theta) jVal = [...code to compute J(theta)...]; gradient = [...code to compute derivative of J(theta)...];end Then we can use octave’s “fminunc()” optimization algorithm along with the “optimset()” function that creates an object containing the options we want to send to “fminunc()”. (Note: the value for MaxIter should be an integer, not a character string - errata in the video at 7:30) 123options = optimset(&apos;GradObj&apos;, &apos;on&apos;, &apos;MaxIter&apos;, 100);initialTheta = zeros(2,1); [optTheta, functionVal, exitFlag] = fminunc(@costFunction, initialTheta, options); We give to the function “fminunc()” our cost function, our initial vector of theta values, and the “options” object that we created beforehand. Example Having implemented this cost function, you would, you can then call the advanced optimization function called the ‘fminunc’ - it stands for function minimization unconstrained in Octave -and the way you call this is as follows. You set a few options. This is a options as a data structure that stores the options you want. So ‘GradObj’ and ‘on’ , these set the gradient objective parameter to on . It just means you are indeed going to provide a gradient to this algorithm ( which tells fminunc that our function returns both the cost and the gradient. This allows fminunc to use the gradient when minimizing the function ). I’m going to set the maximum number of iterations to, let’s say, one hundred. We’re going give it an initial guess for $\theta$ . There’s a 2 by 1 vector. And then this command calls fminunc. This ‘@’ symbol presents a pointer to the cost function that we just defined up there. And if you call this, this will compute, you know, will use one of the more advanced optimization algorithms. And if you want to think it as just like gradient descent. But automatically choosing the learning rate alpha for so you don’t have to do so yourself. But it will then attempt to use the sort of advanced optimization algorithms. Like gradient descent on steroids. To try to find the optimal value of theta for you. Let me actually show you what this looks like in Octave. So I’ve written this cost function of theta function exactly as we had it on the previous line. It computes J-val which is the cost function. And it computes the gradient with the two elements being the partial derivatives of the cost function with respect to, you know, the two parameters, theta one and theta two. 123456function [jVal, gradient]=costFunction(theta) jVal=(theta(1)-5)^2+(theta(2)-5)^2; gradient=zeros(2,1); gradient(1)=2*(theta(1)-5); gradient(2)=2*(theta(2)-5);end Now let’s switch to my Octave window. I’m gonna type in those commands I had just now. So, options equals optimset. 123options=optimset(&apos;GradObj&apos;,&apos;on&apos;,&apos;MaxIter&apos;,100);initialTheta=zeros(2,1);[optTheta, functionVal, exitFlag]=fminunc(@costFunction, initialTheta, options); This is the notation for setting my parameters on my options, for my optimization algorithm. And if I hit enter this will run the optimization algorithm. And it returns pretty quickly. This funny formatting that’s because my line, you know, my code wrapped around. So, this funny thing is just because my command line had wrapped around. But what this says is that numerically renders, you know, think of it as gradient descent on steroids, they found the optimal value of a theta is theta 1 equals 5, theta 2 equals 5, exactly as we’re hoping for. The functionValue at the optimum is essentially 10-to -the-minus-30-power. So that’s essentially zero, which is also what we’re hoping for . And the exitFlag is 1, and this shows what the convergence status of this. And if you want you can do help fminunc to read the documentation for how to interpret the exit flag. But the exitFlag let’s you verify whether or not this algorithm thing has converged. So that’s how you run these algorithms in Octave. I should mention, by the way, that for the Octave implementation, this value of theta, your parameter : vector of theta, its dimension is at least greater than or equal to 2. So if theta is just a real number. So, if it is not at least a two-dimensional vector or some higher than two-dimensional vector, this fminunc may not work, so and if in case you have a one-dimensional function that you use to optimize, you can look in the octave documentation for fminunc for additional details. So, that’s how we optimize our trial example of this simple quick driving cost function. However, we apply this to let’s just say progression. In logistic regression we have a parameter vector theta, and I’m going to use a mix of octave notation and sort of math notation. But I hope this explanation will be clear, but our parameter vector theta comprises these parameters theta 0 through theta n because octave indexes, vectors using indexing from 1, you know, theta 0 is actually written theta 1 in octave, theta 1 is gonna be written. So, if theta 2 in octave and that’s gonna be a written theta n+1, right? And that’s because Octave indexes is vectors starting from index of 1 and so the index of 0. So what we need to do then is write a cost function that captures the cost function for logistic regression. Concretely, the cost function needs to return jVal, which is, you know, jVal as you need some codes to compute J of theta and we also need to give it the gradient. So, gradient 1 is going to be some code to compute the partial derivative in respect to theta 0, the next partial derivative respect to theta 1 and so on. Once again, this is gradient 1, gradient 2 and so on, rather than gradient 0, gradient 1 because octave indexes is vectors starting from one rather than from zero. But the main concept I hope you take away from this slide is, that what you need to do, is write a function that returns the cost function and returns the gradient. And so in order to apply this to logistic regression or even to linear regression, if you want to use these optimization algorithms for linear regression. What you need to do is plug in the appropriate code to compute these things over here. So, now you know how to use these advanced optimization algorithms. Because, using, because for these algorithms, you’re using a sophisticated optimization library, it makes the just a little bit more opaque and so just maybe a little bit harder to debug. But because these algorithms often run much faster than gradient descent, often quite typically whenever I have a large machine learning problem, I will use these algorithms instead of using gradient descent. And with these ideas, hopefully, you’ll be able to get logistic progression and also linear regression to work on much larger problems. So, that’s it for advanced optimization concepts. And in the next and final video on Logistic Regression, I want to tell you how to take the logistic regression algorithm that you already know about and make it work also on multi-class classification problems. Multiclass Classification: One-vs-allNow we will approach the classification of data when we have more than two categories. Instead of y = {0,1} we will expand our definition so that y = {0,1…n}.$$\begin{align*}& y \in \lbrace0, 1 ... n\rbrace \\ & h_\theta^{(0)}(x) = P(y = 0 | x ; \theta) \\ & h_\theta^{(1)}(x) = P(y = 1 | x ; \theta) \\ & \cdots \\ & h_\theta^{(n)}(x) = P(y = n | x ; \theta) \\ & \mathrm{prediction} = \max_i( h_\theta ^{(i)}(x) )\\ \end{align*}$$Since y = {0,1…n}, we divide our problem into n+1 (+1 because the index starts at 0) binary classification problems; in each one, we predict the probability that ‘y’ is a member of one of our classes. To summarize: Train a logistic regression classifier $h_θ(x)$ for each class￼ to predict the probability that ￼ ￼$y = i￼ $￼. To make a prediction on a new $x$ , pick the class ￼that maximizes $h_θ(x)$]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[04_linear-regression-with-multiple-variables note4]]></title>
    <url>%2F2018%2F01%2F04%2F04_linear-regression-with-multiple-variables%2F</url>
    <content type="text"><![CDATA[Multiple FeaturesNote: [7:25 - $θ^T$ is a 1 by (n+1) matrix and not an (n+1) by 1 matrix] Linear regression with multiple variables is also known as “multivariate linear regression”. We now introduce notation for equations where we can have any number of input variables. $$ \begin{align*}x_j^{(i)} &= \text{value of feature } j \text{ in the }i^{th}\text{ training example} \\ x^{(i)}& = \text{the input (features) of the }i^{th}\text{ training example} \\ m &= \text{the number of training examples} \\ n &= \text{the number of features} \end{align*} $$ The multivariable form of the hypothesis function accommodating these multiple features is as follows: $$h_\theta (x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \theta_3 x_3 + \cdots + \theta_n x_n$$ In order to develop intuition about this function, we can think about $θ_0$ as the basic price of a house, $θ_1$ as the price per square meter, $θ_2$ as the price per floor, etc. $x_1$ will be the number of square meters in the house, $x_2$ the number of floors, etc. Using the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as : $$ \begin{align*}h_\theta(x) =\begin{bmatrix}\theta_0 \hspace{2em} \theta_1 \hspace{2em} ... \hspace{2em} \theta_n\end{bmatrix}\begin{bmatrix}x_0 \\ x_1 \\ \vdots \\ x_n\end{bmatrix}= \theta^T x\end{align*} $$ This is a vectorization of our hypothesis function for one training example; see the lessons on vectorization to learn more. Remark : Note that for convenience reasons in this course we assume $x^{(i)}_0=1 \text{ for }(i∈1,…,m)$ . This allows us to do matrix operations with theta and x. Hence making the two vectors ‘ $θ$ ‘ and $x^{(i)}$ match each other element-wise (that is, have the same number of elements: n+1) .] Gradient Descent For Multiple VariablesThe gradient descent equation itself is generally the same form; we just have to repeat it for our ‘n’ features : $$ \begin{align*} & \text{repeat until convergence:} \; \lbrace \newline \; & \theta_0 := \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_0^{(i)}\newline \; & \theta_1 := \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_1^{(i)} \newline \; & \theta_2 := \theta_2 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_2^{(i)} \newline & \cdots \newline \rbrace \end{align*} $$ In other words : $$ \begin{align*}& \text{repeat until convergence:} \; \lbrace \newline \; & \theta_j := \theta_j - \alpha \frac{1}{m} \sum\limits_{i=1}^{m} (h_\theta(x^{(i)}) - y^{(i)}) \cdot x_j^{(i)} \; & \text{for j := 0...n}\newline \rbrace\end{align*} $$ The following image compares gradient descent with one variable to gradient descent with multiple variables: Gradient Descent in Practice I - Feature Scaling Note: [6:20 - The average size of a house is 1000 but 100 is accidentally written instead] We can speed up gradient descent by having each of our input values in roughly the same range. This is because $θ​$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven. The way to prevent this is to modify the ranges of our input variables so that they are all roughly the same. Ideally:$$−1 ≤ x_{(i)} ≤ 1 \text{ or } −0.5 ≤ x_{(i)} ≤ 0.5$$These aren’t exact requirements; we are only trying to speed things up. The goal is to get all input variables into roughly one of these ranges, give or take a few. Two techniques to help with this are feature scaling and mean normalization . Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula: $$x_i := \dfrac{x_i - \mu_i}{s_i}$$ Where $μ_i$ is the average of all the values for feature (i) and $s_i$ is the range of values (max - min), or $s_i$ is the standard deviation. Note that dividing by the range, or dividing by the standard deviation, give different results. The quizzes in this course use range - the programming exercises use standard deviation. For example, if $x_i$ represents housing prices with a range of 100 to 2000 and a mean value of 1000, then, $x_i := \dfrac{price-1000}{1900}$ Gradient Descent in Practice II - Learning RateNote: [5:20 - the x -axis label in the right graph should be $θ$ rather than No. of iterations ] Debugging gradient descent. Make a plot with number of iterations on the x-axis. Now plot the cost function, $J(θ)$ over the number of iterations of gradient descent. If $J(θ)$ ever increases, then you probably need to decrease $α$. Automatic convergence test. Declare convergence if $J(θ)$ decreases by less than E in one iteration, where E is some small value such as $10^{−3}$. However in practice it’s difficult to choose this threshold value. It has been proven that if learning rate $α$ is sufficiently small, then $J(θ)$ will decrease on every iteration. To summarize: If $α$ is too small: slow convergence. If $α$ is too large: ￼$J(\theta)$ may not decrease on every iteration and thus may not converge. Features and Polynomial RegressionWe can improve our features and the form of our hypothesis function in a couple different ways. We can combine multiple features into one. For example, we can combine $x_1$ and $x_2$ into a new feature $x_3$ by taking $x_1⋅x_2$ . Polynomial RegressionOur hypothesis function need not be linear (a straight line) if that does not fit the data well. We can change the behavior or curve of our hypothesis function by making it a quadratic, cubic or square root function (or any other form). For example, if our hypothesis function is $h_\theta(x) = \theta_0 + \theta_1 x_1$, then we can create additional features based on $x_1$, to get the quadratic function $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2$, or the cubic function $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_1^2 + \theta_3 x_1^3$ . In the cubic version, we have created new features $x_2$ and $x_3$ where $x_2=x_1^2$ and $x_3=x_1^3$. To make it a square root function, we could do: $h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 \sqrt{x_1}$ One important thing to keep in mind is, if you choose your features this way then feature scaling becomes very important. eg. if $x_1$ has range 1 - 1000 then range of $x_1^2$ becomes 1 - 1000000 and that of $x_1^3$ becomes 1 - 1000000000 Normal EquationNote: [8:00 to 8:44 - The design matrix $X$ (in the bottom right side of the slide) given in the example should have elements $x$ with subscript 1 and superscripts varying from 1 to m because for all m training sets there are only 2 features $x_0$ and $x_1$. 12:56 - The $X$ matrix is m by (n+1) and NOT n by n. ] Gradient descent gives one way of minimizing $J$ . Let’s discuss a second way of doing so, this time performing the minimization explicitly and without resorting to an iterative algorithm. In the “Normal Equation” method, we will minimize $J$ by explicitly taking its derivatives with respect to the θj’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:$$\theta = (X^T X)^{-1}X^T y$$ There is no need to do feature scaling with the normal equation. The following is a comparison of gradient descent and the normal equation: Gradient Descent Normal Equation Need to choose alpha No need to choose alpha Needs many iterations No need to iterate O ( $kn^2$ ) O ($n^3$), need to calculate inverse of $X^TX$ Works well when n is large Slow if n is very large With the normal equation, computing the inversion has complexity $O(n^3)$ . So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process. Normal Equation NoninvertibilityWhen implementing the normal equation in octave we want to use the pinv function rather than inv The pinvfunction will give you a value of $θ$ even if $X^TX$ is not invertible. ( pinv(A) means calculating the pseudo inverse of matrix A ) If $X^TX$ is noninvertible, the common causes might be having : Redundant features, where two features are very closely related (i.e. they are linearly dependent) Too many features (e.g. m ≤ n). In this case, delete some features or use “regularization” (to be explained in a later lesson). Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[03_linear-algebra-review note3]]></title>
    <url>%2F2018%2F01%2F03%2F03_linear-algebra-review%2F</url>
    <content type="text"><![CDATA[NoteThis personal note is written after studying the coursera opening course, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. Matrices and VectorsMatrices are 2-dimensional arrays: $$\begin{bmatrix}a&amp;b&amp;c\\d&amp;e&amp;f\\g&amp;h&amp;i\\j&amp;k&amp;l\\\end{bmatrix}$$The above matrix has four rows and three columns, so it is a $4 \times 3$ matrix.A vector is a matrix with one column and many rows:$$\begin{bmatrix}w\\x\\y\\z\end{bmatrix}$$ So vectors are a subset of matrices. The above vector is a $4 \times 1$ matrix. Notation and terms : ​ $A_{ij}$ refers to the element in the ith row and jth column of matrix A. ​ A vector with ‘n’ rows is referred to as an ‘n’-dimensional vector. ​ $v_i$ refers to the element in the ith row of the vector. ​ In general, all our vectors and matrices will be 1-indexed. Note that for some programming languages, the arrays are 0-indexed. ​ Matrices are usually denoted by uppercase names while vectors are lowercase. ​ “Scalar” means that an object is a single value, not a vector or matrix. ​ $\mathbb{R}$ refers to the set of scalar real numbers. ​ $\mathbb{R}^n$ refers to the set of n-dimensional vectors of real numbers. Run the cell below to get familiar with the commands in Octave/Matlab. Feel free to create matrices and vectors and try out different things. 1234567891011121314151617% The ; denotes we are going back to a new row.A = [1, 2, 3; 4, 5, 6; 7, 8, 9; 10, 11, 12]% Initialize a vector v = [1;2;3] % Get the dimension of the matrix A where m = rows and n = columns[m,n] = size(A)% You could also store it this waydim_A = size(A)% Get the dimension of the vector v dim_v = size(v)% Now let's index into the 2nd row 3rd column of matrix AA_23 = A(2,3) Addition and Scalar MultiplicationAddition and subtraction are element-wise , so you simply add or subtract each corresponding element: $$\begin{bmatrix} a &amp; b \\ c &amp; d \\ \end{bmatrix} + \begin{bmatrix} w &amp; x \\ y &amp; z \\ \end{bmatrix} = \begin{bmatrix} a+w &amp; b+x \\ c+y &amp; d+z \\ \end{bmatrix}$$ Subtracting Matrices: $$\begin{bmatrix} a &amp; b \\ c &amp; d \\ \end{bmatrix} - \begin{bmatrix} w &amp; x \\ y &amp; z \\ \end{bmatrix} =\begin{bmatrix} a-w &amp; b-x \\ c-y &amp; d-z \\ \end{bmatrix}$$ To add or subtract two matrices, their dimensions must be the same . In scalar multiplication, we simply multiply every element by the scalar value: $$ \begin{bmatrix} a & b \\ c & d \\ \end{bmatrix} * x =\begin{bmatrix} a*x & b*x \\ c*x & d*x \\ \end{bmatrix} $$ In scalar division, we simply divide every element by the scalar value:$$\begin{bmatrix} a &amp; b \\ c &amp; d \\ \end{bmatrix} / x =\begin{bmatrix} a /x &amp; b/x \\ c /x &amp; d /x \\ \end{bmatrix}$$ Experiment below with the Octave/Matlab commands for matrix addition and scalar multiplication. Feel free to try out different commands. Try to write out your answers for each command before running the cell below. 123456789101112131415161718192021% Initialize matrix A and B A = [1, 2, 4; 5, 3, 2]B = [1, 3, 4; 1, 1, 1]% Initialize constant s s = 2% See how element-wise addition worksadd_AB = A + B % See how element-wise subtraction workssub_AB = A - B% See how scalar multiplication worksmult_As = A * s% Divide A by sdiv_As = A / s% What happens if we have a Matrix + scalar?add_As = A + s Matrix-Vector MultiplicationWe map the column of the vector onto each row of the matrix, multiplying each element and summing the result. $$ \begin{bmatrix} a & b \\ c & d \\ e & f \end{bmatrix} *\begin{bmatrix} x \\ y \\ \end{bmatrix} =\begin{bmatrix} a*x + b*y \\ c*x + d*y \\ e*x + f*y\end{bmatrix} $$ The result is a vector. The number of columns of the matrix must equal the number of rows of the vector. An m x n matrix multiplied by an n x 1 vector results in an m x 1 vector . Below is an example of a matrix-vector multiplication. Make sure you understand how the multiplication works. Feel free to try different matrix-vector multiplications. 12345678% Initialize matrix A A = [1, 2, 3; 4, 5, 6;7, 8, 9] % Initialize vector v v = [1; 1; 1] % Multiply A * vAv = A * v Matrix-Matrix MultiplicationWe multiply two matrices by breaking it into several vector multiplications and concatenating the result. $$ \begin{bmatrix} a & b \\ c & d \\ e & f \end{bmatrix} *\begin{bmatrix} w & x \\ y & z \\ \end{bmatrix} =\begin{bmatrix} a*w + b*y & a*x + b*z \\ c*w + d*y & c*x + d*z \\ e*w + f*y & e*x + f*z\end{bmatrix} $$ An m x n matrix multiplied by an n x o matrix results in an m x o matrix. In the above example, a 3 x 2 matrix times a 2 x 2 matrix resulted in a 3 x 2 matrix. To multiply two matrices, the number of columns of the first matrix must equal the number of rows of the second matrix. For example: 12345678910% Initialize a 3 by 2 matrix A = [1, 2; 3, 4;5, 6]% Initialize a 2 by 1 matrix B = [1; 2] % We expect a resulting matrix of (3 by 2)*(2 by 1) = (3 by 1) mult_AB = A*B% Make sure you understand why we got that result Matrix Multiplication Properties ​ Matrices are not commutative: $A∗B≠B∗A,A∗B≠B∗A$ ​ Matrices are associative: $(A∗B)∗C=A∗(B∗C)$ The identity matrix , when multiplied by any matrix of the same dimensions, results in the original matrix. It’s just like multiplying numbers by 1. The identity matrix simply has 1’s on the diagonal (upper left to lower right diagonal) and 0’s elsewhere.$$\begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; 1 &amp; 0 \\ 0 &amp; 0 &amp; 1 \\ \end{bmatrix}$$When multiplying the identity matrix after some matrix (A∗I), the square identity matrix’s dimension should match the other matrix’s columns. When multiplying the identity matrix before some other matrix (I∗A), the square identity matrix’s dimension should match the other matrix’s rows . 12345678910111213141516171819202122% Initialize random matrices A and B A = [1,2;4,5]B = [1,1;0,2]% Initialize a 2 by 2 identity matrixI = eye(2)% The above notation is the same as I = [1,0;0,1]% What happens when we multiply I*A ? IA = I*A % How about A*I ? AI = A*I % Compute A*B AB = A*B % Is it equal to B*A? BA = B*A % Note that IA = AI but AB != BA Inverse and Transpose The inverse of a matrix $A$ is denoted $A^{−1}$. Multiplying by the inverse results in the identity matrix. A non square matrix does not have an inverse matrix. We can compute inverses of matrices in octave with the pinv(A) function and in Matlab with the inv(A) function. Matrices that don’t have an inverse are singular or degenerate . The transposition of a matrix is like rotating the matrix 90 ° in clockwise direction and then reversing it. We can compute transposition of matrices in matlab with the transpose(A) function or A&#39; :$$A = \begin{bmatrix} a &amp; b \\ c &amp; d \\ e &amp; f \end{bmatrix}$$ $$A^T = \begin{bmatrix} a &amp; c &amp; e \\ b &amp; d &amp; f \\ \end{bmatrix}$$ In other words: $$A_{ij} = A^T_{ji}$$ 1234567891011% Initialize matrix A A = [1,2,0;0,5,6;7,0,9]% Transpose A A_trans = A' % Take the inverse of A A_inv = inv(A)% What is A^(-1)*A? A_invA = inv(A)*A]]></content>
      <categories>
        <category>英文</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[02_linear-regression-with-one-variable note2]]></title>
    <url>%2F2018%2F01%2F02%2F02_linear-regression-with-one-variable%2F</url>
    <content type="text"><![CDATA[NoteThis personal note is written after studying the coursera opening course, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. Model Representation To establish notation for future use, we’ll use $x^{(i)}$ to denote the “input”variables (living area in this example), also called input features, and $y^{(i)}$ to denote the “output” or target variable that we are trying to predict(price). A pair ( $x^{(i)},y^{(i)}$ ) is called a training example, and the dataset that we’ll be using to learn—a list of m training examples ( $x^{(i)},y^{(i)} ) ;i=1,…,m$ — is called a training set. Note that the superscript “(i)” in the notation is simply an index into the training set, and has nothing to do with exponentiation. We will also use X to denote the space of input values, and Y to denote the space of output values. In this example, X = Y = ℝ. To describe the supervised learning problem slightly more formally, our goal is, given a training set, to learn a function h : X → Y so that h(x) is a “good” predictor for the corresponding value of y. For historical reasons, this function h is called a hypothesis. Seen pictorially, the process is therefore like this: When the target variable that we’re trying to predict is continuous, such as in our housing example, we call the learning problem a regression problem.When y can take on only a small number of discrete values (such as if, given the living area, we wanted to predict if a dwelling is a house or an apartment, say), we call it a classification problem. Cost FunctionWe can measure the accuracy of our hypothesis function by using a cost function . This takes an average difference (actually a fancier version of an average) of all the results of the hypothesis with inputs from x’s and the actual output y’s. $ J(θ_0,θ_1)={1\over2m}\sum\limits_{i=1}^m (\hat{y}_i−y_i)^2=\frac{1}{2m}\sum\limits_{i=1}^m(h_{θ(xi)}−y_i)^2$ To break it apart, it is ${1\over 2}\bar{x}$ where $\bar{x}$ is the mean of the squares of $h_{θ(xi)}−y_i$ , or the difference between the predicted value and the actual value. This function is otherwise called the “Squared error function”, or “Mean squared error”. The mean is halved $({1\over 2})$ as a convenience for the computation of the gradient descent, as the derivative term of the square function will cancel out the $({1\over 2})$ term. The following image summarizes what the cost function does: Cost Function - Intuition I If we try to think of it in visual terms, our training data set is scattered on the x-y plane. We are trying to make a straight line (defined by $h_{θ(x)}$ ) which passes through these scattered data points. Our objective is to get the best possible line. The best possible line will be such so that the average squared vertical distances of the scattered points from the line will be the least. Ideally, the line should pass through all the points of our training data set. In such a case, the value of $J(θ_0,θ_1)$ will be $0$. The following example shows the ideal situation where we have a cost function of $0$. ​When $θ_1=1$, we get a slope of 1 which goes through every single data point in our model. Conversely, when $θ_1=0.5$, we see the vertical distance from our fit to the data points increase. This increases our cost function to $0.58​$. Plotting several other points yields to the following graph: Thus as a goal, we should try to minimize the cost function. In this case, $θ_1=1$ is our global minimum. Cost Function - Intuition II A contour plot is a graph that contains many contour lines. A contour line of a two variable function has a constant value at all points of the same line. An example of such a graph is the one to the right below. Taking any color and going along the ‘circle’, one would expect to get the same value of the cost function. For example, the three green points found on the green line above have the same value for $J(θ_0,θ_1)$ and as a result, they are found along the same line. The circled x displays the value of the cost function for the graph on the left when $θ_0 = 800$ and $θ_1= -0.15$ . Taking another $h(x)$ and plotting its contour plot, one gets the following graphs: When $θ_0 = 360$ and $θ_1 = 0$, the value of $J(θ_0,θ_1)$ in the contour plot gets closer to the center thus reducing the cost function error. Now giving our hypothesis function a slightly positive slope results in a better fit of the data. The graph above minimizes the cost function as much as possible and consequently, the result of $\theta_1$ and $\theta_0$ tend to be around $0.12$ and $250$ respectively. Plotting those values on our graph to the right seems to put our point in the center of the inner most ‘circle’. Gradient DescentSo we have our hypothesis function and we have a way of measuring how well it fits into the data. Now we need to estimate the parameters in the hypothesis function. That’s where gradient descent comes in. Imagine that we graph our hypothesis function based on its fields $θ_0$ and $θ_1$ (actually we are graphing the cost function as a function of the parameter estimates). We are not graphing x and y itself, but the parameter range of our hypothesis function and the cost resulting from selecting a particular set of parameters. We put $θ_0$ on the x axis and $θ_1$ on the y axis, with the cost function on the vertical z axis. The points on our graph will be the result of the cost function using our hypothesis with those specific theta parameters. The graph below depicts such a setup. We will know that we have succeeded when our cost function is at the very bottom of the pits in our graph, i.e. when its value is the minimum. The red arrows show the minimum points in the graph. The way we do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter $α$ , which is called the learning rate. For example, the distance between each ‘star’ in the graph above represents a step determined by our parameter $α$ . A smaller $α$ would result in a smaller step and a larger $α$ results in a larger step. The direction in which the step is taken is determined by the partial derivative of $J(θ_0,θ_1)$. Depending on where one starts on the graph, one could end up at different points. The image above shows us two different starting points that end up in two different places. The gradient descent algorithm is:$$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$$repeat until convergence: where $j=0,1$ represents the feature index number. At each iteration $j$ , one should simultaneously update the parameters $θ_1,θ_2,…,θ_n$. Updating a specific parameter prior to calculating another one on the $j^{(th)}$ iteration would yield to a wrong implementation. Gradient Descent IntuitionIn this video we explored the scenario where we used one parameter $θ_1$ and plotted its cost function to implement a gradient descent. Our formula for a single parameter was : Repeat until convergence:$$\theta_1:=\theta_1-\alpha \frac{d}{d\theta_1} J(\theta_1)$$Regardless of the slope’s sign for $\frac{d}{d\theta_1} J(\theta_1)$, eventually converges to its minimum value. The following graph shows that when the slope is negative, the value of $θ_1$ increases and when it is positive, the value of $θ_1$ decreases. On a side note, we should adjust our parameter $α$ to ensure that the gradient descent algorithm converges in a reasonable time. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong. How does gradient descent converge with a fixed step size α?The intuition behind the convergence is that $\frac{d}{d\theta_1} J(\theta_1)$ , approaches 0 as we approach the bottom of our convex function. At the minimum, the derivative will always be 0 and thus we get:$$\theta_1:=\theta_1-\alpha * 0$$ Gradient Descent For Linear Regression​ Note: [At 6:15 “ $h(x) = -900 - 0.1x$ “ should be “ $h(x) = 900 - 0.1x$ “] When specifically applied to the case of linear regression, a new form of the gradient descent equation can be derived. We can substitute our actual cost function and our actual hypothesis function and modify the equation to : $$ \begin{align*} \text{repeat until convergence: } \lbrace & \\ \theta_0 := & \theta_0 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}(h_\theta(x_{i}) - y_{i}) \\ \theta_1 := & \theta_1 - \alpha \frac{1}{m} \sum\limits_{i=1}^{m}\left((h_\theta(x_{i}) - y_{i}) x_{i}\right) \\ \rbrace& \end{align*} $$ where m is the size of the training set, $θ_0$ a constant that will be changing simultaneously with $θ_1$ and $x_i,y_i$ are values of the given training set (data). Note that we have separated out the two cases for $θ_j$ into separate equations for $θ_0$ and $θ_1$ ; and that for $θ_1$ we are multiplying $x_i$ at the end due to the derivative. The following is a derivation of $\frac{∂}{∂θ_j}J(θ)$ for a single example : $$ \begin{align*} \frac{\partial}{\partial\theta_j}J(\theta) &=& \frac{\partial}{\partial\theta_j}\frac{1}{2}(h_{\theta}(x)-y)^2 \\ &=& 2 \cdot \frac{1}{2}(h_{\theta}(x)-y)\cdot \frac{\partial}{\partial\theta_j}(h_{\theta}(x)-y) \\ &=& (h_{\theta}(x)-y)\cdot\frac{\partial}{\partial\theta_j}\left(\sum\limits_{i=0}^{n}\theta_ix_i-y\right) \\ &=& (h_{\theta}(x)-y)x_j \end{align*} $$ The point of all this is that if we start with a guess for our hypothesis and then repeatedly apply these gradient descent equations, our hypothesis will become more and more accurate. So, this is simply gradient descent on the original cost function J. This method looks at every example in the entire training set on every step, and is called batch gradient descent . Note that, while gradient descent can be susceptible to local minima in general, the optimization problem we have posed here for linear regression has only one global, and no other local, optima; thus gradient descent always converges (assuming the learning rate α is not too large) to the global minimum. Indeed, J is a convex quadratic function.Here is an example of gradient descent as it is run to minimize a quadratic function. The ellipses shown above are the contours of a quadratic function. Also shown is the trajectory taken by gradient descent, which was initialized at $(48,30)$. The $x$’s in the figure (joined by straight lines) mark the successive values of $θ$ that gradient descent went through as it converged to its minimum.]]></content>
      <categories>
        <category>english</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[05_octave-matlab-tutorial note5]]></title>
    <url>%2F2018%2F01%2F01%2F05_octave-matlab-tutorial%2F</url>
    <content type="text"><![CDATA[5.1 基本操作在这段视频中，我将教你一种编程语言：Octave语言。你能够用它来非常迅速地实现这门课中我们已经学过的，或者将要学的机器学习算法。 过去我一直尝试用不同的编程语言来教授机器学习，包括C++、Java、Python、Numpy和Octave。我发现当使用像Octave这样的高级语言时，学生能够更快更好地学习并掌握这些算法。事实上，在硅谷，我经常看到进行大规模的机器学习项目的人，通常使用的程序语言就是Octave。 Octave是一种很好的原始语言(prototyping language)，使用Octave你能快速地实现你的算法，剩下的事情，你只需要进行大规模的资源配置，你只用再花时间用C++或Java这些语言把算法重新实现就行了。开发项目的时间是很宝贵的，机器学习的时间也是很宝贵的。所以，如果你能让你的学习算法在Octave上快速的实现，基本的想法实现以后，再用C++或者Java去改写，这样你就能节省出大量的时间。 据我所见，人们使用最多的用于机器学习的原始语言是Octave、MATLAB、Python、NumPy 和R。 Octave很好，因为它是开源的。当然MATLAB也很好，但它不是每个人都买得起的。貌似国内学生喜欢用收费的matlab，matlab功能要比Octave强大的多，网上有各种D版可以下载。这次机器学习课的作业也是用matlab的。如果你能够使用MATLAB，你也可以在这门课里面使用。 如果你会Python、NumPy或者R语言，我也见过有人用 R的，据我所知，这些人不得不中途放弃了，因为这些语言在开发上比较慢，而且，因为这些语言如：Python、NumPy的语法相较于Octave来说，还是更麻烦一点。正因为这样，所以我强烈建议不要用NumPy或者R来完整这门课的作业，我建议在这门课中用Octave来写程序。 本视频将快速地介绍一系列的命令，目标是迅速地展示，通过这一系列Octave的命令，让你知道Octave能用来做什么。 启动Octave： 现在打开Octave，这是Octave命令行。 现在让我示范最基本的Octave代码： 输入5 + 6，然后得到11。输入3 – 2、5×8、1/2、2^6等等，得到相应答案。 这些都是基本的数学运算。 你也可以做逻辑运算，例如 1==2，计算结果为 false ( 假)，这里的百分号命令表示注释，1==2 计算结果为假，这里用0表示。 请注意，不等于符号的写法是这个波浪线加上等于符号 ( ~= )，而不是等于感叹号加等号( != )，这是和其他一些编程语言中不太一样的地方。 让我们看看逻辑运算 1 &amp;&amp; 0，使用双&amp;符号表示逻辑与，1 &amp;&amp; 0判断为假，1和0的或运算 1 || 0，其计算结果为真。 还有异或运算 如 XOR ( 1, 0 )，其返回值为1 从左向右写着 Octave 324.x版本，是默认的Octave提示，它显示了当前Octave的版本，以及相关的其它信息。 如果你不想看到那个提示，这里有一个隐藏的命令 输入命令 现在命令提示已经变得简化了。 接下来，我们将谈到Octave的变量。 现在写一个变量，对变量A赋值为3，并按下回车键，显示变量A等于3。 打印变量如果你想分配一个变量，但不希望在屏幕上显示结果，你可以在命令后加一个分号，可以抑制打印输出**，敲入回车后，不打印任何东西。 其中这句命令不打印任何东西。 现在举一个字符串的例子：变量b等于”hi”。 C等于3大于等于1，所以，现在C变量的值是真。 如果你想打印出变量，或显示一个变量，你可以像下面这么做： 设置A等于圆周率π，如果我要打印该值，那么只需键入A像这样 就打印出来了。 对于更复杂的屏幕输出，也可以用DISP命令显示： 这是一种，旧风格的C语言语法，对于之前就学过C语言的同学来说，你可以使用这种基本的语法来将结果打印到屏幕。 例如 sprintf命令的六个小数：0.6%f ,a，这应该打印π的6位小数形式。 也有一些控制输出长短格式的快捷命令： 下面，让我们来看看向量和矩阵： 比方说 建立一个矩阵A 对A矩阵进行赋值 考虑到这是一个三行两列的矩阵 你同样可以用向量 建立向量V并赋值1 2 3，V是一个行向量，或者说是一个3 ( 列 )×1 ( 行 )的向量，或者说，一行三列的矩阵。 如果我想，分配一个列向量，我可以写“1;2;3”，现在便有了一个3 行 1 列的向量，同时这是一个列向量。 通过增量或步长来构造矩阵下面是一些更为有用的符号，如： 1V=1:0.1:2 这个该如何理解呢：这个集合V是一组值，从数值1开始，增量或说是步长为0.1，直到增加到2，按照这样的方法对向量V操作，可以得到一个行向量，这是一个1行11列的矩阵，其矩阵的元素是11.1 1.2 1.3，依此类推，直到数值2。 1V=[1:0.5:2;3:0.5:4] 我也可以建立一个集合V并用命令“1:6”进行赋值，这样V就被赋值了1至6的六个整数。 这里还有一些其他的方法来生成矩阵 ones &amp; zeros例如“ones(2, 3)”，也可以用来生成矩阵： 元素都为2，两行三列的矩阵，就可以使用这个命令： 你可以把这个方法当成一个生成矩阵的快速方法。 w为一个一行三列的零矩阵，一行三列的A矩阵里的元素全部是零： 还有很多的方式来生成矩阵。 如果我对W进行赋值，用Rand命令建立一个一行三列的矩阵，因为使用了Rand命令，则其一行三列的元素均为随机值，如“rand(3,3)”命令，这就生成了一个3×3的矩阵，并且其所有元素均为随机。 数值介于0和1之间，所以，正是因为这一点，我们可以得到数值均匀介于0和1之间的元素。 如果，你知道什么是高斯随机变量，或者，你知道什么是正态分布的随机变量，你可以设置集合W，使其等于一个一行三列的N矩阵，并且，来自三个值，一个平均值为0的高斯分布，方差或者等于1的标准偏差。 linspace &amp; logspacelinspace(x1,x2,N) 创建一个N个元素的向量,均匀分布于x1和x2之间 logspace(x1,x2,N) 创建一个N个元素的向量,指数分布与10的x1次方和10的x2次方之间 histogram 直方图并用hist命令绘制直方图。 单位矩阵 identity matrix绘制单位矩阵： help如果对命令不清楚，建议用help命令： 以上讲解的内容都是Octave的基本操作。希望你能通过上面的讲解，自己练习一些矩阵、乘、加等操作，将这些操作在Octave中熟练运用。 在接下来的视频中，将会涉及更多复杂的命令，并使用它们在Octave中对数据进行更多的操作。 5.2 移动数据在这段关于 Octave的辅导课视频中，我将开始介绍如何在 Octave 中移动数据。 如果你有一个机器学习问题，你怎样把数据加载到 Octave 中？ 怎样把数据存入一个矩阵？ 如何对矩阵进行相乘？ 如何保存计算结果？ 如何移动这些数据并用数据进行操作？ 进入我的 Octave 窗口， 我键入 A，得到我们之前构建的矩阵 A，也就是用这个命令生成的： sizeA = [1 2; 3 4; 5 6] 这是一个3行2列的矩阵，Octave 中的 size() 命令返回矩阵的尺寸。 所以 size(A) 命令返回3 2 实际上，size() 命令返回的是一个 1×2 的矩阵，我们可以用 sz 来存放。 设置 sz = size(A) 因此 sz 就是一个1×2的矩阵，第一个元素是3，第二个元素是2。 所以如果键入 size(sz) 看看 sz 的尺寸，返回的是1 2，表示是一个1×2的矩阵，1 和 2分别表示矩阵sz的维度 。 你也可以键入 size(A, 1)，将返回3，这个命令会返回A 矩阵的第一个元素，A矩阵的第一个维度的尺寸，也就是 A 矩阵的行数。 同样，命令 size(A, 2)，将返回2，也就是 A 矩阵的列数。 length如果你有一个向量 v，假如 v = [1 2 3 4]，然后键入length(v)，这个命令将返回最大维度的大小，返回4。 你也可以键入 length(A)，由于矩阵A是一个3×2的矩阵，因此最大的维度应该是3，因此该命令会返回3。 但通常我们还是对向量使用 length 命令，而不是对矩阵使用 length 命令，比如length([1;2;3;4;5])，返回5。 加载数据和寻找数据如何在系统中加载数据和寻找数据： 当我们打开 Octave 时，我们通常已经在一个默认路径中，这个路径是 Octave的安装位置，pwd 命令可以显示出Octave 当前所处路径。 cd命令，意思是改变路径，我可以把路径改为C:\Users\ang\Desktop，这样当前目录就变为了桌面。 如果键入 ls，ls 来自于一个 Unix 或者 Linux 命令，ls命令将列出我桌面上的所有路径。 事实上，我的桌面上有两个文件：featuresX.dat 和priceY.dat，是两个我想解决的机器学习问题。 featuresX 文件如这个窗口所示，是一个含有两列数据的文件，其实就是我的房屋价格数据，数据集中有47行，第一个房子样本，面积是2104平方英尺，有3个卧室，第二套房子面积为1600，有3个卧室等等。 priceY这个文件就是训练集中的价格数据，所以 featuresX 和priceY就是两个存放数据的文档，那么应该怎样把数据读入 Octave 呢？我们只需要键入 load featuresX.dat，这样我将加载了 featuresX 文件。同样地我可以加载priceY.dat。其实有好多种办法可以完成，如果你把命令写成字符串的形式load(&#39;featureX.dat&#39;)，也是可以的，这跟刚才的命令效果是相同的，只不过是把文件名写成了一个字符串的形式，现在文件名被存在一个字符串中。Octave中使用引号来表示字符串。 另外 who 命令，能显示出 在我的 Octave工作空间中的所有变量 所以我可以键入featuresX 回车，来显示 featuresX 这些就是存在里面的数据。 还可以键入 size(featuresX)，得出的结果是 47 2，代表这是一个47×2的矩阵。 类似地，输入 size(priceY)，结果是 471，表示这是一个47维的向量，是一个列矩阵，存放的是训练集中的所有价格 Y 的值。 who and whoswho 函数能让你看到当前工作空间中的所有变量，同样还有另一个 whos命令，能更详细地进行查看。 同样也列出我所有的变量，不仅如此，还列出了变量的维度。 double 意思是双精度浮点型，这也就是说，这些数都是实数，是浮点数。 删除变量如果你想删除某个变量，你可以使用 clear 命令，我们键入 clear featuresX，然后再输入 whos 命令，你会发现 featuresX 消失了。 另外，我们怎么储存数据呢？ 我们设变量 v= priceY(1:10) 这表示的是将向量 Y 的前10个元素存入 v 中。 保存数据假如我们想把它存入硬盘，那么用 save hello.mat v 命令，这个命令会将变量v存成一个叫 hello.mat 的文件，这个命令把数据按照二进制形式储存，或者说是更压缩的二进制形式，因此，如果v是很大的数据，那么压缩幅度也更大，占用空间也更小。如果你想把数据存成一个人能看懂的形式，那么可以键入： save hello.txt v -ascii 这样就会把数据存成一个文本文档或者将数据的 ascii 码存成文本文档。 我键入了这个命令以后，我的桌面上就有了 hello.txt文件。如果打开它，我们可以发现这个文本文档存放着我们的数据。 这就是读取和储存数据的方法。 接下来我们再来讲讲操作数据的方法： 假如 A 还是那个矩阵 A(3,2)这将索引到A 矩阵的 (3,2) 元素。 取单行/列数据A(2,:) 来返回第二行的所有元素，冒号表示该行或该列的所有元素。 类似地，如果我键入 A(:,2)，这将返回 A 矩阵第二列的所有元素。 你也可以在运算中使用这些较为复杂的索引。 A([1 3],:)，这个命令意思是取 A 矩阵第一个索引值为1或3的元素，也就是说我取的是A矩阵的第一行和第三行的每一列，冒号表示的是取这两行的每一列元素，即： 可能这些比较复杂一点的索引操作你会经常用到。 我们还能做什么呢？依然是 A 矩阵，A(:,2) 命令返回第二列。 你也可以为它赋值，我可以取 A 矩阵的第二列，然后将它赋值为10 11 12，我实际上是取出了 A 的第二列，然后把一个列向量[10;11;12]赋给了它，因此现在 A 矩阵的第一列还是 1 3 5，第二列就被替换为 10 11 12。 连接矩阵列追加A = [A, [100, 101, 102]]，这样做的结果是在原矩阵的右边附加了一个新的列矩阵，就是把 A矩阵设置为原来的 A 矩阵再在右边附上一个新添加的列矩阵。 行追加A=[A;[0,0,0]] 拼接我还是把 A 重新设为 [1 2; 3 4; 5 6]，我再设一个 B为[11 12; 13 14; 15 16]，我可以新建一个矩阵 C，C = [A B]，这个意思就是把这两个矩阵直接连在一起，矩阵 A 在左边，矩阵 B 在右边，这样组成了 C 矩阵，就是直接把 A 和 B 合起来。 我还可以设C = [A; B]，这里的分号表示把分号后面的东西放到下面。所以，[A;B]的作用依然还是把两个矩阵放在一起，只不过现在是上下排列，所以现在 A 在上面 B在下面，C 就是一个 6×2 矩阵。 简单地说，分号的意思就是换到下一行，所以 C 就包括上面的A，然后换行到下面，然后在下面放上一个 B。 另外顺便说一下，这个[A B]命令跟 [A, B] 是一样的，这两种写法的结果是相同的。 矩阵变向量最后，还有一个小技巧，如果你就输入 A(:)，这是一个很特别的语法结构，意思是把 A中的所有元素放入一个单独的列向量，这样我们就得到了一个 9×1 的向量，这些元素都是A 中的元素排列起来的。 通过以上这些操作，希望你现在掌握了怎样构建矩阵，也希望我展示的这些命令能让你很快地学会怎样把矩阵放到一起，怎样取出矩阵，并且把它们放到一起，组成更大的矩阵。 5.3 计算数据现在，你已经学会了在Octave中如何加载或存储数据，如何把数据存入矩阵等等。在这段视频中，我将介绍如何对数据进行运算，稍后我们将使用这些运算操作来实现我们的学习算法。 这是我的 Octave窗口，我现在快速地初始化一些变量。比如设置A为一个3×2的矩阵，设置B为一个3 ×2矩阵，设置C为2 × 2矩阵。 矩阵乘法我想算两个矩阵的乘积，比如说 A × C，我只需键入A×C，这是一个 3×2 矩阵乘以 2×2矩阵，得到这样一个3×2矩阵。 基于元素的运算通常来说，在Octave中点号一般用来表示元素位运算。你也可以对每一个元素，做运算方法是做点乘运算 A .*B，这么做Octave将矩阵 A中的每一个元素与矩阵 B 中的对应元素相乘 A .* B 这里第一个元素1乘以11得到11，第二个元素2乘以12得到24，这就是两个矩阵的元素位运算。 这里是一个矩阵A，这里我输入A .^ 2，这将对矩阵A中每一个元素平方。 我们设V是一个向量，设V为 [1; 2; 3] 是列向量，你也可以输入1 ./V，得到每一个元素的倒数，所以这样一来，就会分别算出 1/1 1/2 1/3。 矩阵也可以这样操作，1 ./ A 得到A中每一个元素的倒数。 同样地，这里的点号还是表示对每一个元素进行操作。 求对数运算我们还可以进行求对数运算，也就是对每个元素进行求对数运算。 幂运算还有自然数e的幂次运算，就是以e为底，以这些元素为幂的运算。 绝对值运算我还可以用 abs来对 v 的每一个元素求绝对值，当然这里 v都是正数。我们换成另一个这样对每个元素求绝对值，得到的结果就是这些非负的元素。 相反数运算还有–v，给出V中每个元素的相反数，这等价于 -1 乘以 v，一般就直接用 -v 就好了，其实就等于 -1*v。 加法我们想对v中的每个元素都加1，那么我们可以这么做，首先构造一个3行1列的1向量，然后把这个1向量跟原来的向量相加，因此 v 向量从[1 2 3] 增至 [2 3 4]。我用了一个，length(v)命令，因此这样一来，ones(length(v) ,1) 就相当于ones(3,1)，然后我做的是v +ones(3,1)，也就是将 v 的各元素都加上这些1，这样就将 v 的每个元素增加了1。 另一种更简单的方法是直接用 v+1，v + 1 也就等于把 v 中的每一个元素都加上1。 转置矩阵A 如果你想要求它的转置，那么方法是用A’,将得出 A 的转置矩阵。当然，如果我写(A’)’，也就是 A 转置两次，那么我又重新得到矩阵 A。 最大值还有一些有用的函数，比如： a=[1 15 2 0.5]，这是一个1行4列矩阵，val=max(a)，这将返回A矩阵中的最大值15。 我还可以写 [val, ind] =max(a)，这将返回a矩阵中的最大值存入val，以及该值对应的索引，元素15对应的索引值为2存入ind，所以 ind 等于2 特别注意一下，如果你用命令 max(A)，A是一个矩阵的话，这样做就是对每一列求最大值。 我们还是用这个例子，这个 a 矩阵a=[1 15 2 0.5]，如果输入a&lt;3，这将进行逐元素的运算，所以元素小于3的返回1，否则返回0。 因此，返回[1 1 0 1]。也就是说，对a矩阵的每一个元素与3进行比较，然后根据每一个元素与3的大小关系，返回1和0表示真与假。 排序1. 数据的排序 在MALTAB/octave语言中，sort函数用于进行数据的排序，其使用格式如下。  l sort(X)命令将X按由小到大排序。当X为向量时，该命令返回的X为按由小到大排序后的向量；当X为矩阵时，该命令返回X矩阵的各列按由小到大排序。 l sort(X,DIM)命令在给定的维数DIM内排序。 l [Y,I]=sort(X)命令中，Y为排序结果，I中元素表示Y中对应元素在X中的位置。当X是一个向量时，那么有Y=X(I)；当X是一个m×n阶矩阵时，程序段“for j = 1:n, Y(:,j) = X(I(:,j),j); end”成立。 >&gt; X = [1 7 5; 6 4 2;9 8 3] sort(X,2) ans = 1 5 7 2 4 6 3 8 9 [Y,I]=sort(X) Y = 1 4 2 ​ 6 7 3 9 8 5 I = 1 2 2 2 1 3 3 3 1 2.按行进行数据排序 在MALTAB语言中，使用sortrows函数按行进行数据的排序，其使用格式如下。 l Y=sortrows(X)命令将X按由小到大以行的方式进行排序。当X为矩阵时，返回矩阵Y，并且Y是按X的第1列由小到大，以行方式排序后生成的矩阵。 l sortrows (X, COL)命令按指定列COL由小到大进行排序。 l [Y, I] = sortrows(X, COL)命令中，Y为排序的结果，I表示Y中第COL列元素在X中位置。 find如果我写 find(a&lt;3)，这将告诉我a 中的哪些元素是小于3的。 如果我输入 [r,c] = find(A&gt;=7)，这将找出所有A矩阵中大于等于7的元素，因此，r 和c分别表示行和列，这就表示，第一行第一列的元素大于等于7，第三行第二列的元素大于等于7，第二行第三列的元素大于等于7。 顺便说一句，其实我从来都不去刻意记住这个 find 函数，到底是怎么用的，我只需要会用help 函数就可以了，每当我在使用这个函数，忘记怎么用的时候，我就可以用 help函数，键入 help find 来找到帮助文档。 magic设A = magic(3)，magic 函数将返回一个矩阵，称为魔方阵或幻方 (magic squares)，它们具有以下这样的数学性质：它们所有的行和列和对角线加起来都等于相同的值。 当然据我所知，这在机器学习里基本用不上，但我可以用这个方法很方便地生成一个3行3列的矩阵，而这个魔方矩阵这神奇的方形屏幕。每一行、每一列、每一个对角线三个数字加起来都是等于同一个数。 在其他有用的机器学习应用中，这个矩阵其实没多大作用。 sum\ceil\floor最后再讲两个内容，一个是求和函数，这是 a 矩阵： 键入 sum(a)，就把 a 中所有元素加起来了。 如果我想把它们都乘起来，键入 prod(a)，prod 意思是product(乘积)，它将返回这四个元素的乘积。 floor(a) 是向下四舍五入，因此对于 a 中的元素0.5将被下舍入变成0。 还有 ceil(a)，表示向上四舍五入，所以0.5将上舍入变为最接近的整数，也就是1。 键入 type(3)，这通常得到一个3×3的矩阵 每行\列最大值假如我输入max(A,[],1)，这样做会得到每一列的最大值。 所以第一列的最大值就是8，第二列是9，第三列的最大值是7，这里的1表示取A矩阵第一个维度的最大值。 相对地，如果我键入max(A,[],2)，这将得到每一行的最大值，所以，第一行的最大值是等于8，第二行最大值是7，第三行是9。 所以你可以用这个方法来求得每一行或每一列的最值，另外，你要知道，默认情况下max(A)返回的是每一列的最大值，如果你想要找出整个矩阵A的最大值，你可以输入max(max(A))，或者你可以将A 矩阵转成一个向量，然后键入 max(A(:))，这样做就是把 A 当做一个向量，并返回 A向量中的最大值。 如果键入 max(rand(3),rand(3))，这样做的结果是返回两个3×3的随机矩阵，并且逐元素比较取最大值。 最后，让我们把 A设为一个9行9列的魔方阵，魔方阵具有的特性是每行每列和对角线的求和都是相等的。 列行和这是一个9×9的魔方阵，我们来求一个 sum(A,1)，这样就得到每一列的总和，这也验证了一个9×9的魔方阵确实每一列加起来都相等，都为369。 现在我们来求每一行的和，键入sum(A,2)，这样就得到了A 中每一行的和加起来还是369。 对角元素和现在我们来算A 的对角线元素的和。我们现在构造一个9×9 的单位矩阵， 键入 eye(9) 然后我们要用 A 逐点乘以这个单位矩阵，除了对角线元素外，其他元素都会得到0。 键入sum(sum(A.*eye(9)) 这实际上是求得了，这个矩阵对角线元素的和确实是369。 你也可以求另一条对角线的和也是是369：sum(flipup(A)*eye(9)) 矩阵向上/向下翻转flipup/flipud 表示向上/向下翻转。 同样地，如果你想求这个矩阵的逆矩阵，键入pinv(A)，通常称为伪逆矩阵，你就把它看成是矩阵 A 求逆，因此这就是 A矩阵的逆矩阵。 设 temp = pinv(A)，然后再用temp 乘以 A，这实际上得到的就是单位矩阵，对角线为1，其他元素为0。 5.4 绘图数据当开发学习算法时，往往几个简单的图，可以让你更好地理解算法的内容，并且可以完整地检查下算法是否正常运行，是否达到了算法的目的。 例如在之前的视频中，我谈到了绘制成本函数 $J(θ)$，可以帮助确认梯度下降算法是否收敛。通常情况下，绘制数据或学习算法所有输出，也会启发你如何改进你的学习算法。幸运的是，Octave有非常简单的工具用来生成大量不同的图。当我用学习算法时，我发现绘制数据、绘制学习算法等，往往是我获得想法来改进算法的重要部分。在这段视频中，我想告诉你一些Octave的工具来绘制和可视化你的数据。 我们先来快速生成一些数据用来绘图。 如果我想绘制正弦函数，这是很容易的，我只需要输入plot(t,y1)，并回车，就出现了这个图： 横轴是t变量，纵轴是y1，也就是我们刚刚所输出的正弦函数。 axis([0.5 1 -1 1]) 改变坐标轴范围，横坐标：[0.5，1] 纵坐标：[-1，1] 让我们设置y2 Octave将会消除之前的正弦图，并且用这个余弦图来代替它，这里纵轴cos(x)从1开始， 如果我要同时表示正弦和余弦曲线。我要做的就是，输入：plot(t, y1)，得到正弦函数，我使用函数hold on，hold on函数的功能是将新的图像绘制在旧的之上 ，我现在绘制y2，输入：plot(t, y2)。 我要以不同的颜色绘制余弦函数，所以我在这里输入带引号的r绘制余弦函数，r表示所使用的颜色：plot(t,y2,’r’)，再加上命令xlabel(&#39;time&#39;)，来标记X轴即水平轴，输入ylabel(&#39;value&#39;)，来标记垂直轴的值。 同时我也可以来 标记函数曲线，用这个命令 legend(&#39;sin&#39;,&#39;cos&#39;) 将这个图例放在右上方，表示这两条曲线表示的内容。最后输入title(&#39;myplot&#39;)，在图像的顶部显示这幅图的标题。 如果你想 保存这幅图像，你输入print –dpng &#39;myplot.png&#39;，png是一个图像文件格式，如果你这样做了，它可以让你保存为一个文件。 Octave也可以保存为很多其他的格式，你可以键入help plot。 最后如果你想，删掉这个图像，用命令close会让这个图像关掉。 Octave也可以让你为图像标号你键入figure(1); plot(t, y1);将显示第一张图，绘制了变量t y1。键入figure(2); plot(t, y2); 将显示第一张图，绘制了变量t y2。 subplotsubplot命令，我们要使用subplot(1,2,1)，它将图像分为一个1*2的格子，也就是前两个参数，然后它使用第一个格子，也就是最后一个参数1的意思。 我现在使用第一个格子，如果键入plot(t,y1)，现在这个图显示在第一个格子。如果我键入subplot(1,2,2)，那么我就要使用第二个格子，键入plot(t,y2)；现在y2显示在右边，也就是第二个格子。 最后一个命令，你可以改变轴的刻度，比如改成[0.5 1 -1 1]，输入命令：axis([0.5 1 -1 1])也就是设置了右边图的x轴和y轴的范围。具体而言，它将右图中的横轴的范围调整至0.5到1，竖轴的范围为-1到1。 你不需要记住所有这些命令，如果你需要改变坐标轴，或者需要知道axis命令，你可以用Octave中用help命令了解细节。最后，还有几个命令。 Clf（清除一幅图像）。 让我们设置A等于一个5×5的magic方阵： 我有时用一个巧妙的方法来可视化矩阵，也就是imagesc(A)命令，它将会绘制一个55的矩阵，一个55的彩色格图，不同的颜色对应A矩阵中的不同值。 我还可以使用函数colorbar，让我用一个更复杂的命令 imagesc(A)，colorbar，colormap gray。这实际上是在同一时间运行三个命令：运行imagesc，然后运行，colorbar然后运行colormap gray。 它生成了一个颜色图像，一个灰度分布图，并在右边也加入一个颜色条。所以这个颜色条显示不同深浅的颜色所对应的值。 你可以看到在不同的方格，它对应于一个不同的灰度。 输入imagesc(magic(15))，colorbar，colormap gray 这将会是一幅15*15的magic方阵值的图。 最后，总结一下这段视频。你看到我所做的是使用逗号连接函数调用。如果我键入a=1,b=2,c=3然后按Enter键，其实这是将这三个命令同时执行，或者是将三个命令一个接一个执行，它将输出所有这三个结果。 这很像a=1; b=2;c=3;如果我用分号来代替逗号，则没有输出出任何东西。 这里我们称之为逗号连接的命令或函数调用。 用逗号连接是另一种Octave中更便捷的方式，将多条命令例如imagesc colorbar colormap，将这多条命令写在同一行中。 现在你知道如何绘制Octave中不同的图像，在下面的视频中，我将告诉你怎样在Octave中，写控制语句，比如ifwhile for语句，并且定义和使用函数。 5.5 控制语句：for，while，if语句在这段视频中，我想告诉你怎样为你的 Octave 程序写控制语句。诸如：”for” “while” “if” 这些语句，并且如何定义和使用方程。 我先告诉你如何使用 “for” 循环。首先，我要将 v 值设为一个10行1列的零向量。 接着我要写一个 “for” 循环，让 i 等于 1 到 10，写出来就是 i = 1:10。我要设 v(i)的值等于 2 的 i 次方，循环最后写上“end”。向量 v 的值就是这样一个集合 2的一次方、2的二次方，依此类推。这就是我的 i 等于 1 到 10的语句结构，让 i 遍历 1 到 10的值。 另外，你还可以通过设置你的 indices (索引) 等于 1一直到10，来做到这一点。这时indices 就是一个从1到10的序列。 你也可以写 i = indices，这实际上和我直接把 i 写到 1 到 10 是一样。你可以写 disp(i)，也能得到一样的结果。所以 这就是一个 “for” 循环。 如果你对 “break” 和 “continue” 语句比较熟悉，Octave里也有 “break” 和 “continue”语句，你也可以在 Octave环境里使用那些循环语句。但是首先让我告诉你一个 while 循环是如何工作的： 这是什么意思呢：我让 i 取值从 1 开始，然后我要让 v(i) 等于 100，再让 i 递增 1，直到 i 大于 5停止。 现在来看一下结果，我现在已经取出了向量的前五个元素，把他们用100覆盖掉，这就是一个while循环的句法结构。现在我们来分析另外一个例子： 这里我将向你展示如何使用break语句。比方说 v(i) = 999，然后让 i = i+1，当 i 等于6的时候 break (停止循环)，结束 (end)。 当然这也是我们第一次使用一个 if 语句，所以我希望你们可以理解这个逻辑，让 i 等于1 然后开始下面的增量循环，while语句重复设置 v(i) 等于999，不断让i增加，然后当 i 达到6，做一个中止循环的命令，尽管有while循环，语句也就此中止。所以最后的结果是取出向量 v 的前5个元素，并且把它们设置为999。 所以，这就是if 语句和 while 语句的句法结构。并且要注意要有end，上面的例子里第一个 end 结束的是 if语句，第二个 end 结束的是 while 语句。 现在让我告诉你使用 if-else 语句： 最后，提醒一件事：如果你需要退出 Octave，你可以键入exit命令然后回车就会退出 Octave，或者命令quit也可以。 function最后，让我们来说说函数 (functions)，如何定义和调用函数。 我在桌面上存了一个预先定义的文件名为 “squarethisnumber.m”，这就是在 Octave 环境下定义的函数。 让我们打开这个文件。请注意，我使用的是微软的写字板程序来打开这个文件，我只是想建议你，如果你也使用微软的Windows系统，那么可以使用写字板程序，而不是记事本来打开这些文件。如果你有别的什么文本编辑器也可以，记事本有时会把代码的间距弄得很乱。如果你只有记事本程序，那也能用。我建议你用写字板或者其他可以编辑函数的文本编辑器。 现在我们来说如何在 Octave 里定义函数： 这个文件只有三行： 第一行写着 function y = squareThisNumber(x)，这就告诉 Octave，我想返回一个 y值，我想返回一个值，并且返回的这个值将被存放于变量 y 里。另外，它告诉了Octave这个函数有一个参数，就是参数 x，还有定义的函数体，也就是 y 等于 x 的平方。 还有一种更高级的功能，这只是对那些知道“search path (搜索路径)”这个术语的人使用的。所以如果你想要修改Octave的搜索路径，你可以把下面这部分作为一个进阶知识，或者选学材料，仅适用于那些熟悉编程语言中搜索路径概念的同学。 你可以使用addpath 命令添加路径，添加路径“C:\Users\ang\desktop”将该目录添加到Octave的搜索路径，这样即使你跑到其他路径底下，Octave依然知道会在 Users\ang\desktop目录下寻找函数。这样，即使我现在在不同的目录下，它仍然知道在哪里可以找到“SquareThisNumber” 这个函数。 但是，如果你不熟悉搜索路径的概念，不用担心，只要确保在执行函数之前，先用 cd命令设置到你函数所在的目录下，实际上也是一样的效果。 Octave还有一个其他许多编程语言都没有的概念，那就是它可以允许你定义一个函数，使得返回值是多个值或多个参数。这里就是一个例子，定义一个函数叫： “SquareAndCubeThisNumber(x)” (x的平方以及x的立方) 这说的就是函数返回值是两个： y1 和 y2 接下来就是y1是被平方后的结果，y2是被立方后的结果，这就是说，函数会真的返回2个值。 有些同学可能会根据你使用的编程语言，比如你们可能熟悉的C或C++，通常情况下，认为作为函数返回值只能是一个值，但Octave 的语法结构就不一样，可以返回多个值。如果我键入 [a,b] = SquareAndCubeThisNumber(5)，然后，a 就等于25，b 就等于5的立方125。所以说如果你需要定义一个函数并且返回多个值，这一点常常会带来很多方便。 最后，我来给大家演示一下一个更复杂一点的函数的例子。 比方说，我有一个数据集，像这样，数据点为[1,1], [2,2],[3,3]，我想做的事是定义一个 Octave 函数来计算代价函数 $J(θ)$，就是计算不同 $θ$ 值所对应的代价函数值 $J$。 首先让我们把数据放到 Octave 里，我把我的矩阵设置为X = [1 1; 1 2; 1 3]; 请仔细看一下这个函数的定义，确保你明白了定义中的每一步。 现在当我在 Octave 里运行时，我键入 $J = costFunctionJJ (X, y, theta)$，它就计算出 $j$ 等于0，这是因为如果我的数据集x 为 [1;2;3]， y 也为 [1;2;3] 然后设置 $θ_0$ 等于0，$θ_1$ 等于1，这给了我恰好45度的斜线，这条线是可以完美拟合我的数据集的。 而相反地，如果我设置theta 等于[0;0]，那么这个假设就是0是所有的预测值，和刚才一样，设置 $θ_0$ = 0，$θ_1$ 也等于0，然后我计算的代价函数，结果是2.333。实际上，他就等于1的平方，也就是第一个样本的平方误差，加上2的平方，加上3的平方，然后除以2m，也就是训练样本数的两倍，这就是2.33。 因此这也反过来验证了我们这里的函数，计算出了正确的代价函数。这些就是我们用简单的训练样本尝试的几次试验，这也可以作为我们对定义的代价函数J进行了完整性检查。确实是可以计算出正确的代价函数的。至少基于这里的 X和 y是成立的。也就是我们这几个简单的训练集，至少是成立的。 现在你知道如何在 Octave 环境下写出正确的控制语句，比如 for 循环、while 循环和 if语句，以及如何定义和使用函数。在接下来的Octave 教程视频里，我会讲解一下向量化，这是一种可以使你的 Octave程序运行非常快的思想。 5.6 向量化在这段视频中，我将介绍有关向量化的内容，无论你是用Octave，还是别的语言，比如MATLAB或者你正在用Python、NumPy 或 Java C C++，所有这些语言都具有各种线性代数库，这些库文件都是内置的，容易阅读和获取，他们通常写得很好，已经经过高度优化，通常是数值计算方面的博士或者专业人士开发的。 而当你实现机器学习算法时，如果你能好好利用这些线性代数库，或者数值线性代数库，并联合调用它们，而不是自己去做那些函数库可以做的事情。如果是这样的话，那么通常你会发现：首先，这样更有效，也就是说运行速度更快，并且更好地利用你的计算机里可能有的一些并行硬件系统等等；其次，这也意味着你可以用更少的代码来实现你需要的功能。因此，实现的方式更简单，代码出现问题的有可能性也就越小。 举个具体的例子：与其自己写代码做矩阵乘法。如果你只在Octave中输入a乘以b就是一个非常有效的两个矩阵相乘的程序。有很多例子可以说明，如果你用合适的向量化方法来实现，你就会有一个简单得多，也有效得多的代码。 让我们来看一些例子：这是一个常见的线性回归假设函数： 如果你想要计算hθ(x)，注意到右边是求和，那么你可以自己计算 j = 0 到 j = n 的和。但换另一种方式来想想，把 hθ(x) 看作 $θ^Tx$ ，那么你就可以写成两个向量的内积，其中 $θ$ 就是$θ_0, θ_1, θ_2$，如果你有两个特征量，如果 n = 2，并且如果你把 x 看作 $x_0, x_1, x_2$，这两种思考角度，会给你两种不同的实现方式。 比如说，这是未向量化的代码实现方式： 计算 $h_{θ(x)}$ 是未向量化的，我们可能首先要初始化变量 prediction 的值为0.0，而这个变量prediction 的最终结果就是hθ(x)hθ(x)，然后我要用一个 for 循环，j 取值 0 到n+1，变量prediction 每次就通过自身加上 theta(j) 乘以 x(j)更新值，这个就是算法的代码实现。 顺便我要提醒一下，这里的向量我用的下标是0，所以我有θ0、θ1、θ2，但因为MATLAB的下标从1开始，在 MATLAB 中θ0θ0，我们可能会用 theta(1) 来表示，这第二个元素最后就会变成，theta(2) 而第三个元素，最终可能就用theta(3)表示，因为MATLAB中的下标从1开始，这就是为什么这里我的 for 循环，j 取值从 1 直到n+1，而不是从 0 到 n。这是一个未向量化的代码实现方式，我们用一个 for 循环对 n 个元素进行加和。 作为比较，接下来是向量化的代码实现： 你把x和θ看做向量，而你只需要令变量prediction等于theta转置乘以x，你就可以这样计算。与其写所有这些for循环的代码，你只需要一行代码，这行代码就是利用 Octave 的高度优化的数值，线性代数算法来计算两个向量θ以及x的内积，这样向量化的实现更简单，它运行起来也将更加高效。这就是 Octave 所做的而向量化的方法，在其他编程语言中同样可以实现。 让我们来看一个C++ 的例子： 与此相反，使用较好的C++数值线性代数库，你可以写出像右边这样的代码，因此取决于你的数值线性代数库的内容。你只需要在C++中将两个向量相乘，根据你所使用的数值和线性代数库的使用细节的不同，你最终使用的代码表达方式可能会有些许不同，但是通过一个库来做内积，你可以得到一段更简单、更有效的代码。 现在，让我们来看一个更为复杂的例子，这是线性回归算法梯度下降的更新规则： 我们用这条规则对 j 等于 0、1、2等等的所有值，更新对象θj，我只是用θ0、θ1、θ2来写方程，假设我们有两个特征量，所以n等于2，这些都是我们需要对θ0、θ1、θ2进行更新，这些都应该是同步更新，我们用一个向量化的代码实现，这里是和之前相同的三个方程，只不过写得小一点而已。 你可以想象实现这三个方程的方式之一，就是用一个 for 循环，就是让 j等于0、等于1、等于2，来更新θj。但让我们用向量化的方式来实现，看看我们是否能够有一个更简单的方法。基本上用三行代码或者一个for 循环，一次实现这三个方程。让我们来看看怎样能用这三步，并将它们压缩成一行向量化的代码来实现。做法如下： 我打算把θ看做一个向量，然后我用θ-α 乘以某个别的向量δ 来更新θ。 这里的 δ 等于 让我解释一下是怎么回事：我要把θ看作一个向量，有一个 n+1 维向量，α 是一个实数，δ在这里是一个向量。 所以这个减法运算是一个向量减法，因为 α 乘以 δ是一个向量，所以θ就是θ - αδ得到的向量。 那么什么是向量 δ 呢 ? X(i)是一个向量 你就会得到这些不同的式子，然后作加和。 实际上，在以前的一个小测验，如果你要解这个方程，我们说过为了向量化这段代码，我们会令u = 2v +5w因此，我们说向量u等于2 乘以向量v 加上5乘以向量w。用这个例子说明，如何对不同的向量进行相加，这里的求和是同样的道理。 这就是为什么我们能够向量化地实现线性回归。 所以，我希望步骤是有逻辑的。请务必看视频，并且保证你确实能理解它。如果你实在不能理解它们数学上等价的原因，你就直接实现这个算法，也是能得到正确答案的。所以即使你没有完全理解为何是等价的，如果只是实现这种算法，你仍然能实现线性回归算法。如果你能弄清楚为什么这两个步骤是等价的，那我希望你可以对向量化有一个更好的理解，如果你在实现线性回归的时候，使用一个或两个以上的特征量。 有时我们使用几十或几百个特征量来计算线性归回，当你使用向量化地实现线性回归，通常运行速度就会比你以前用你的for循环快的多，也就是自己写代码更新θ0、θ1、θ2。 因此使用向量化实现方式，你应该是能够得到一个高效得多的线性回归算法。而当你向量化我们将在之后的课程里面学到的算法，这会是一个很好的技巧，无论是对于Octave 或者一些其他的语言 如C++、Java 来让你的代码运行得更高效。 5.7 工作和提交的编程练习在这段视频中，我想很快地介绍一下这门课程做作业的流程，以及如何使用作业提交系统。这个提交系统可以即时检验你的机器学习程序答案是否正确。 在’ml-class-ex1’目录中，我们提供了大量的文件，其中有一些需要由你自己来编辑，因此第一个文件应该符合编程练习中pdf文件的要求，其中一个我们要求你编写的文件是warmUpExercise.m这个文件，这个文件只是为了确保你熟悉提交系统。 你需要做的就是提交一个5×5的矩阵，就是A = eye(5)这将修改该函数以产生5×5的单位矩阵，现在warmUpExercise()这个方程就实现了返回5x5的单位矩阵，将它保存一下，所以我已经完成了作业的第一部分。 现在回到我的 Octave 窗口，现在来到我的目录C:\Users\ang\Desktop\ml-class-ex1如果我想确保我已经实现了程序 像这样输入’warmUpExercise()’好了它返回了我们用刚才写的代码创建的一个5x5的单位矩阵 我现在可以按如下步骤提交代码，我要在这里目录下键入submit()。我要提交第一部分 所以我选择输入’1’。这时它问我的电子邮件地址，我们打开课程网站，输入用户名密码。 按下回车键，它连接到服务器，并将其提交，然后它就会立刻告诉你：恭喜您！已成功完成作业1第1部分。这就确认了你已经做对了第一部分练习，如果你提交的答案不正确，那么它会给你一条消息，说明你没有完全答对，您还可以继续使用此提交密码，也可以生成新密码。你的密码是否会显示出来取决于你使用的操作系统。这就是提交作业的方法，你完成家庭作业的时候，我希望你都能答对。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01_introduction note1]]></title>
    <url>%2F2018%2F01%2F01%2F01_what-is-machine-learning%2F</url>
    <content type="text"><![CDATA[Need to know This personal note is written after studying the coursera opening course, Machine Learning by Andrew NG . And images, audios of this note all comes from the opening course. So, the copyright belongs to Andrew NG. What is Machine Learning?Two definitions of Machine Learning are offered Arthur Samuel described it as: “the field of study that gives computers the ability to learn without being explicitly programmed.” This is an older, informal definition. Tom Mitchell provides a more modern definition: “A computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.” Example: playing checkers.E = the experience of playing many games of checkersT = the task of playing checkers.P = the probability that the program will win the next game. In general, any machine learning problem can be assigned to one of two broad classifications: Supervised learning Unsupervised learning Supervised LearningIn supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output. Supervised learning problems are categorized into “regression” and “classification” problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories. housing price prediction:Given data about the size of houses on the real estate market, try to predict their price. Price as a function of size is a continuous output, so this is a regression problem. Let’s say you want to predict housing prices. A while back, a student collected data sets from the Institute of Portland Oregon. And let’s say you plot a data set and it looks like this. Here on the horizontal axis, the size of different houses in square feet, and on the vertical axis, the price of different houses in thousands of dollars. So. Given this data, let’s say you have a friend who owns a house that is, say 750 square feet and hoping to sell the house and they want to know how much they can get for the house. So how can the learning algorithm help you? One thing a learning algorithm might be able to do is put a straight line through the data or to fit a straight line to the data and, based on that, it looks like maybe the house can be sold for maybe about 150,000 dollars . But maybe this isn’t the only learning algorithm you can use. There might be a better one. For example, instead of sending a straight line to the data, we might decide that it’s better to fit a quadratic function or a second-order polynomial to this data. And if you do that, and make a prediction here, then it looks like, well, maybe we can sell the house for closer to $200,000. One of the things we’ll talk about later is how to choose and how to decide do you want to fit a straight line to the data or do you want to fit the quadratic function to the data and there’s no fair picking whichever one gives your friend the better house to sell. But each of these would be a fine example of a learning algorithm. So this is an example of a supervised learning algorithm.We could turn this example into a classification problem by instead making our output about whether the house “sells for more or less than the asking price.” Here we are classifying the houses based on price into two discrete categories. breast cancer(a) Regression - Given a picture of a person, we have to predict their age on the basis of the given picture(b) Classification - Given a patient with a tumor, we have to predict whether the tumor is malignant or benign. If someone discovers a breast tumor, a lump in their breast, a malignant tumor is a tumor that is harmful and dangerous and a benign tumor is a tumor that is harmless. So obviously people care a lot about this. Let’s see you want to look at medical records and try to predict of a breast cancer as malignant or benign.Let’s see a collected data set and suppose in your data set you have on your horizontal axis the size of the tumor and on the vertical axis I’m going to plot one or zero, yes or no, whether or not these are examples of tumors we’ve seen before are malignant which is one or zero if not malignant or benign.So let’s say our data set looks like this where we saw a tumor of this size that turned out to be benign. One of this size, one of this size. And so on. And sadly we also saw a few malignant tumors, one of that size, one of that size, one of that size… So on. So this example… I have five examples of benign tumors shown down here, and five examples of malignant tumors shown with a vertical axis value of one.And let’s say we have a friend who tragically has a breast tumor, and let’s say her breast tumor size is maybe somewhere around this value. The machine learning question is, can you estimate what is the probability, what is the chance that a tumor is malignant versus benign? To introduce a bit more terminology this is an example of a classification problem. The term classification refers to the fact that here we’re trying to predict a discrete value output: zero or one, malignant or benign.And it turns out that in classification problems sometimes you can have more than two values for the two possible values for the output. As a concrete example maybe there are three types of breast cancers and so you may try to predict the discrete value of zero, one, two, or three with zero being benign. Benign tumor, so no cancer. And one may mean, type one cancer, like, you have three types of cancer, whatever type one means. And two may mean a second type of cancer, a three may mean a third type of cancer. But this would also be a classification problem, because this other discrete value set of output corresponding to, you know, no cancer, or cancer type one, or cancer type two, or cancer type three. In classification problems there is another way to plot this data. Let me show you what I mean.Let me use a slightly different set of symbols to plot this data. So if tumor size is going to be the attribute that I’m going to use to predict malignancy or benignness, I can also draw my data like this. I’m going to use different symbols to denote my benign and malignant, or my negative and positive examples. So instead of drawing crosses, I’m now going to draw O’s for the benign tumors. Like so. And I’m going to keep using X’s to denote my malignant tumors. Okay? I hope this is beginning to make sense. （My Note:it isn’t a sequential problem, but for the time being, we can ignore it） All I did was I took, you know, these, my data set on top and I just mapped it down. To this real line like so. And started to use different symbols, circles and crosses, to denote malignant versus benign examples. In other machine learning problems when we have more than one feature, more than one attribute. Here’s an example. Let’s say that instead of just knowing the tumor size, we know both the age of the patients and the tumor size. In that case maybe your data set will look like this where I may have a set of patients with those ages and that tumor size and they look like this. And a different set of patients, they look a little different, whose tumors turn out to be malignant, as denoted by the crosses. So, let’s say you have a friend who tragically has a tumor. And maybe, their tumor size and age falls around there. So given a data set like this, what the learning algorithm might do is throw the straight line through the data to try to separate out the malignant tumors from the benign ones and, so the learning algorithm may decide to throw the straight line like that to separate out the two classes of tumors. And. You know, with this, hopefully you can decide that your friend’s tumor is more likely to if it’s over there, that hopefully your learning algorithm will say that your friend’s tumor falls on this benign side and is therefore more likely to be benign than malignant. In this example we had two features, namely, the age of the patient and the size of the tumor.In other machine learning problems we will often have more features, and my friends that work on this problem, they actually use other features like these, which is clump thickness, the clump thickness of the breast tumor. Uniformity of cell size of the tumor. Uniformity of cell shape of the tumor, and so on, and other features as well. And it turns out one of the interes-, most interesting learning algorithms that we’ll see in this class is a learning algorithm that can deal with, not just two or three or five features, but an infinite number of features. On this slide, I’ve listed a total of five different features. Right, two on the axes and three more up here. But it turns out that for some learning problems, what you really want is not to use, like, three or five features. But instead, you want to use an infinite number of features, an infinite number of attributes, so that your learning algorithm has lots of attributes or features or cues with which to make those predictions. So how do you deal with an infinite number of features. How do you even store an infinite number of things on the computer when your computer is gonna run out of memory. It turns out that when we talk about an algorithm called the Support Vector Machine, there will be a neat mathematical trick that will allow a computer to deal with an infinite number of features. summaryIn supervised learning, in every example in our data set, we are told what is the “correct answer” that we would have quite liked the algorithms have predicted on that example. Such as the price of the house, or whether a tumor is malignant or benign. We also talked about the regression problem. And by regression, that means that our goal is to predict a continuous valued output. And we talked about the classification problem, where the goal is to predict a discrete value output. Unsupervised Learningbreast cancerBack then, recall data sets that look like this, where each example was labeled either as a positive or negative example, whether it was a benign or a malignant tumor. So for each example in Supervised Learning, we were told explicitly what is the so-called right answer, whether it’s benign or malignant. In Unsupervised Learning, we’re given data that looks different than data that looks like this that doesn’t have any labels or that all has the same label or really no labels. So we’re given the data set and we’re not told what to do with it and we’re not told what each data point is. Instead we’re just told, here is a data set. Can you find some structure in the data? Given this data set, an Unsupervised Learning algorithm might decide that the data lives in two different clusters. And so there’s one cluster and there’s a different cluster. And yes, Supervised Learning algorithm may break these data into these two separate clusters. So this is called a clustering algorithm. And this turns out to be used in many places. google newsOne example where clustering is used is in Google News and if you have not seen this before, you can actually go to this URL news.google.com to take a look. What Google News does is everyday it goes and looks at tens of thousands or hundreds of thousands of new stories on the web and it groups them into cohesive news stories. For example, let’s look here. The URLs here link to different news stories about the BP Oil Well story. So, let’s click on one of these URL’s and we’ll click on one of these URL’s. What I’ll get to is a web page like this. Here’s a Wall Street Journal article about, you know, the BP Oil Well Spill stories of “BP Kills Macondo”, which is a name of the spill and if you click on a different URL from that group then you might get the different story. Here’s the CNN story about a game, the BP Oil Spill, and if you click on yet a third link, then you might get a different story. Here’s the UK Guardian story about the BP Oil Spill. So what Google News has done is look for tens of thousands of news stories and automatically cluster them together. So, the news stories that are all about the same topic get displayed together. It turns out that clustering algorithms and Unsupervised Learning algorithms are used in many other problems as well. DNAHere’s one on understanding genomics. Here’s an example of DNA microarray data.The idea is put a group of different individuals and for each of them, you measure how much they do or do not have a certain gene. Technically you measure how much certain genes are expressed. So these colors, red, green, gray and so on, they show the degree to which different individuals do or do not have a specific gene. And what you can do is then run a clustering algorithm to group individuals into different categories or into different types of people.So this is Unsupervised Learning because we’re not telling the algorithm in advance that these are type 1 people, those are type 2 persons, those are type 3 persons and so on and instead what were saying is yeah here’s a bunch of data. I don’t know what’s in this data. I don’t know who’s and what type. I don’t even know what the different types of people are, but can you automatically find structure in the data from the you automatically cluster the individuals into these types that I don’t know in advance? Because we’re not giving the algorithm the right answer for the examples in my data set, this is Unsupervised Learning.Unsupervised Learning or clustering is used for a bunch of other applications. large computer clustersIt’s used to organize large computer clusters. I had some friends looking at large data centers, that is large computer clusters and trying to figure out which machines tend to work together and if you can put those machines together, you can make your data center work more efficiently. social network analysisThis second application is on social network analysis. So given knowledge about which friends you email the most or given your Facebook friends or your Google+ circles, can we automatically identify which are cohesive groups of friends, also which are groups of people that all know each other? Market segmentationMany companies have huge databases of customer information. So, can you look at this customer data set and automatically discover market segments and automatically group your customers into different market segments so that you can automatically and more efficiently sell or market your different market segments together? Again, this is Unsupervised Learning because we have all this customer data, but we don’t know in advance what are the market segments and for the customers in our data set, you know, we don’t know in advance who is in market segment one, who is in market segment two, and so on. But we have to let the algorithm discover all this just from the data. astronomical data analysisFinally, it turns out that Unsupervised Learning is also used for surprisingly astronomical data analysis and these clustering algorithms gives surprisingly interesting useful theories of how galaxies are formed.All of these are examples of clustering, which is just one type of Unsupervised Learning. Let me tell you about another one. cocktail party problemI’m gonna tell you about the cocktail party problem. So, you’ve been to cocktail parties before, right? Well, you can imagine there’s a party, room full of people, all sitting around, all talking at the same time and there are all these overlapping voices because everyone is talking at the same time, and it is almost hard to hear the person in front of you. So maybe at a cocktail party with two people, two people talking at the same time, and it’s a somewhat small cocktail party. And we’re going to put two microphones in the room so there are microphones, and because these microphones are at two different distances from the speakers, each microphone records a different combination of these two speaker voices. Maybe speaker one is a little louder in microphone one and maybe speaker two is a little bit louder on microphone 2 because the 2 microphones are at different positions relative to the 2 speakers, but each microphone would cause an overlapping combination of both speakers’ voices. So here’s an actual recording of two speakers recorded by a researcher. Let me play for you the first : &lt;audio src=”http://p8o3egtyk.bkt.clouddn.com/gitpage/ml-andrew-ng/01/1.m4a&quot; controls=”controls&gt;Your browser does not support the audio element. what the first microphone sounds like. One (uno), two (dos), three (tres), four (cuatro), five (cinco), six (seis), seven (siete), eight (ocho), nine (nueve), ten (y diez). All right, maybe not the most interesting cocktail party, there’s two people counting from one to ten in two languages but you know. What you just heard was the first microphone recording, here’s the second recording. &lt;audio src=”http://p8o3egtyk.bkt.clouddn.com/gitpage/ml-andrew-ng/01/2.mp3&quot; controls=”controls&gt;Your browser does not support the audio element. Uno (one), dos (two), tres (three), cuatro (four), cinco (five), seis (six), siete (seven), ocho (eight), nueve (nine) y diez (ten). So we can do, is take these two microphone recorders and give them to an Unsupervised Learning algorithm called the cocktail party algorithm, and tell the algorithm - find structure in this data for you. And what the algorithm will do is listen to these audio recordings and say, you know it sounds like the two audio recordings are being added together or that have being summed together to produce these recordings that we had. Moreover, what the cocktail party algorithm will do is separate out these two audio sources that were being added or being summed together to form other recordings and, in fact, here’s the first output of the cocktail party algorithm. &lt;audio src=”http://p8o3egtyk.bkt.clouddn.com/gitpage/ml-andrew-ng/01/3.mp3&quot; controls=”controls&gt;Your browser does not support the audio element. One, two, three, four, five, six, seven, eight, nine, ten. So, I separated out the English voice in one of the recordings. And here’s the second of it. &lt;audio src=”http://p8o3egtyk.bkt.clouddn.com/gitpage/ml-andrew-ng/01/4.mp3&quot; controls=”controls&gt;Your browser does not support the audio element. Uno, dos, tres, quatro, cinco, seis, siete, ocho, nueve y diez. Not too bad, to give you one more example, here’s another recording of another similar situation, here’s the first microphone : &lt;audio src=”http://p8o3egtyk.bkt.clouddn.com/gitpage/ml-andrew-ng/01/5.mp3&quot; controls=”controls&gt;Your browser does not support the audio element. One, two, three, four, five, six, seven, eight, nine, ten. OK so the poor guy’s gone home from the cocktail party and he ‘s now sitting in a room by himself talking to his radio. Here’s the second microphone recording. &lt;audio src=”http://p8o3egtyk.bkt.clouddn.com/gitpage/ml-andrew-ng/01/6.mp3&quot; controls=”controls&gt;Your browser does not support the audio element. One, two, three, four, five, six, seven, eight, nine, ten. When you give these two microphone recordings to the same algorithm, what it does, is again say, you know, it sounds like there are two audio sources, and moreover, the album says, here is the first of the audio sources I found. &lt;audio src=”http://p8o3egtyk.bkt.clouddn.com/gitpage/ml-andrew-ng/01/7.mp3&quot; controls=”controls&gt;Your browser does not support the audio element. One, two, three, four, five, six, seven, eight, nine, ten. So that wasn’t perfect, it got the voice, but it also got a little bit of the music in there. Then here’s the second output to the algorithm. &lt;audio src=”http://p8o3egtyk.bkt.clouddn.com/gitpage/ml-andrew-ng/01/8.mp3&quot; controls=”controls&gt;Your browser does not support the audio element. Not too bad, in that second output it managed to get rid of the voice entirely. And just, you know, cleaned up the music, got rid of the counting from one to ten. So you might look at an Unsupervised Learning algorithm like this.Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables. We can derive this structure by clustering the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results. Summary Clustering: Take a collection of 1,000,000 different genes, and find a way to automatically group these genes into groups that are somehow similar or related by different variables, such as lifespan, location, roles, and so on. Non-clustering: The “Cocktail Party Algorithm”, allows you to find structure in a chaotic environment. (i.e. identifying individual voices and music from a mesh of sounds at a cocktail party ). References Machine Learning by Andrew NG]]></content>
      <categories>
        <category>英文</category>
      </categories>
      <tags>
        <tag>Machine Learning by Andrew NG</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典摘录-Bayes' rule贝叶斯定律]]></title>
    <url>%2F2017%2F08%2F28%2FBayes'_rule%2F</url>
    <content type="text"><![CDATA[说明：全文摘自Introduction to probability, 2nd Edition 本文讨论条件概率定律的应用，首先引入一个计算事件概率的定理。 The law of total probability设 $A_1, A_2, … , A_n$ 是一组互不相容的事件，它形成样本空间的一个分割（每个试验结果必定使得其中一个事件发生！）。又假定对每个 $i, P(A_i) &gt; 0$ 。则对任何事件 $B$ ，下列公式成立$$\begin{eqnarray}P(B) &amp;=&amp; P(A_1\cap B )+\cdots+P(A_n\cap B) \\&amp;=&amp; P(A_1)P(B|A_1)+\cdots+P(B)P(B|A_n)\end{eqnarray}$$这是总概率定律（或翻译为：全概率定律），下面有图示和证明。直观上，将样本空间分割成若干事件 $A_i$ 的并（ $A_1, \cdots, A_n$ 形成样本空间的一个分割）然后任意事件 $B$ 的概率等于事件 $B$ 在 $A_i$ 发生的情况下的条件概率的加权平均，而权重刚好等于这些事件 $A_i$ 的无条件概率。这条定理的一个主要应用是计算事件 $B$ 的概率。直接计算事件 $B$ 的概率有点难度，但是若条件概率 $P(B|A_i)$ 是已知的或是很容易推导计算时，总概率定律定理就成为了计算 $P(B)$ 的有力工具。应用这条定理的关键是找到合适的分割 $A_1,\cdots, A_n$ ，而合适的分割又与问题的实际背景有关。 由于事件 $A_1, A_2, \cdots, A_n$ 形成一个样本空间的一个分割，事件 $B$ 可以分解成不想交的 $n$ 个事件的并，即： $$B=(A_!\cap B)\cup\cdots\cup(A_n\cap B) \quad (1)$$利用可加定理，得到： $$P(B) = P(A_1 \cap B)+\cdots+P(A_n \cap B) \quad (2)$$利用条件概率的定义，得到： $$P(A_i\cap B) = P(A_i)P(B|A_i) \quad (3)$$将 $(3)$ 式子代入 $(2)$ 式子中得到： $$P(B)=P(A_1)P(B|A_1)+\cdots+P(A_n)P(B|A_n)$$也可以用等价的序列树形图来说明总概率定律（如上右边图）：叶子 $A_i \cap B$ 的概率等于由叶子到根部上的概率的乘积 $P(A_i)P(B|A_i)$ 。而事件 $B$ 由图上显示的3个叶子组成，将它们的概率相加就得到 $P(B)$ 。 总概率定律的例子例 1.13 你参加一个棋类比赛，其中 $50\%$ 是一类棋手，你赢他们的概率为 $0.3\%$ ； $25\%$ 是二类棋手，你赢他们的概率是 $0.4$ ；剩下的是三类棋手，你赢得他们的概率是 $0.5$ 。从他们中间随机地选一位棋手与你比赛，你胜算的概率有多大？ 记 $A_i$ 表示与你下棋的棋手的类别。依题意 $$P(A_1)=0.5,\quad P(A_2) =0.25, \quad P(A_3) = 0.25$$记 $B$ 为你赢得比赛的事件，那么得到： $$P(B|A_1)=0.3,\quad P(B|A_2)=0.4,\quad P(B|A_3)=0.5$$那么利用总概率定律，你在不比赛中胜出的概率为：$$\begin{eqnarray}P(B) &amp;=&amp; P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3) \\&amp;=&amp; 0.5 \cdot 0.3+ 0.25 \cdot 0.4 + 0.25 \cdot 0.5 \\&amp;=&amp; 0.375\end{eqnarray}$$ Inference and Bayes’ Rule 推断与贝叶斯定律总概率定律经常与著名的贝叶斯定律联系起来，贝叶斯定律将形如 $P(A|B)$ 的条件概率与形如 $P(B|A)$ 的条件概率联系起来。 Bayes’ Rule贝叶斯定律设 $A_1,A_2,\ldots,A_n$ 是一组互斥的事件，它形成样本空间的一个分割（每个试验结果必定使得其中一个事件发生）。又假定对每一个 $i, P(A_i)&gt;0$ ，则对于任何事件 $B$ ，只要它满足 $P(B)&gt;0$ ，下列公式成立：$$\begin{eqnarray}P(A_i|B) &amp;=&amp; \frac{P(A_i)P(B|A_i)}{P(B)}\\&amp;=&amp;\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1)+\cdots+P(A_n)P(B|A_n)}\end{eqnarray}$$为证明贝叶斯定律，只需注意到 $P(A_i)P(B|A_i)$ 与 $P(B)P(A_i|B)$ 是相等的，它们都等于 $P(A_i \cap B)$ ，这样得到了第一个等式，至于第二个等式，只需对 $P(B)$ 利用总概率定律即可。 贝叶斯定律还可以用来进行因果推理。有许多”原因“可以造成某一”结果“。现在设我们观察到某一结果，希望推断造成这个结果出现的”原因“。现在设事件 $A_1,\ldots, A_n​$ 是原因，而 $B​$ 代表由原因引起的结果。 $P(B|A_i)​$ 表示在因果模型中由”原因“ $A_i​$ 造成结果 $B​$ 的概率（见下图）。当观察到结果 $B​$ 的时候，希望反推结果 $B​$ 是由原因 $A_i​$ 造成的概率 $P(A_i|B)​$ 。 $P(A_i|B)​$ 为由于代表新近得到的信息 $B​$ 之后 $A_i​$ 出现的概率，称之为 posterior probability 后验概率，而原来的 $P(A_i)​$ 就称为 prior probability 先验概率。 贝叶斯定律进行推断的例子医学在某病人X光片中发现一个阴影，（用 $B$ 表示，代表”结果“）。希望对造成这种结果的3个原因进行分析。这3个原因互斥，并且造成这个结果的原因一定是三者之一：原因1（事件 $A_1$）是恶性肿瘤，原因2（事件 $A_2$）是良性肿瘤，原因3（事件 $A_3$）是肿瘤外的其他原因。假定已经知道 $P(A_i)$ 和 $P(B|A_i), i=1,2,3$ 。现在已经发现了阴影（事件 $B$ 发生），利用贝叶斯公式，这些原因的条件概率为： $$P(A_i|B)=\frac{P(A_i)P(B|A_i)}{P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)},i=1,2,3$$在右图给出序列树形图，可用序列树形图给出条件概率计算的另外一种等价的解释。图中第一个深灰的叶子表示恶性肿瘤并出现阴影，其概率为 $P(A_1\cap B)$ ，且所有深灰的叶子表示片子中出现阴影，其概率为 $P(B)$ ，而由恶性肿瘤造成阴影的条件概率 $P(A_1|B)$ 是两个概率相除的结果。 比赛继续使用例 1.13 你参加一个棋类比赛，其中 $50\%$ 是一类棋手，你赢他们的概率为 $0.3\%$ ； $25\%$ 是二类棋手，你赢他们的概率是 $0.4$ ；剩下的是三类棋手，你赢得他们的概率是 $0.5$ 。现在假定你已经得胜，问你的对手为一类棋手的概率有多大？用 $A_i$ 表示你与 $i$ 类棋手相遇的事件。由例中给出的条件知道：$$P(A_1)=0.5,\quad P(A_2)=0.25,\quad P(A_3)=0.25$$记 $B$ 表示你赢的比赛的事件，你胜出的概率为：$$P(B|A_1)=0.3,\quad P(B|A_2)=0.4,\quad P(B|A_3)=0.5$$利用贝叶斯公式得：$$\begin{eqnarray}P(A_1|B) &amp;=&amp; \frac{P(A_1)P(B|A_1)}{P(A_1)P(B|A_1)+P(A_2)P(B|A_2)+P(A_3)P(B|A_3)} \\&amp;=&amp; \frac{0.5\cdot 0.3}{0.5\cdot 0.3+0.25\cdot 0.4+0.25\cdot 0.5} \\&amp;=&amp; 0.4\end{eqnarray}$$ 假阳性之谜 设对于某种少见的疾病的检出率为 $0.95$ ；如果一个被检查的病人有某种疾病，其检查结果为阳性的概率为 $0.95$ ；如果该人没有这种疾病，其检查结果为阴性的概率是 $0.95$ 。现在假定某一人群中患有这种病的概率为 $0.001$ ，并从这个总体中随机地抽取一个人进行检测，检查结果为阳性。现在问这个人患有这种病的概率有多大？ 设 $A$ 为这个人有这种疾病， $B$ 为经检验这个人为阳性。利用贝叶斯公式：$$\begin{eqnarray}P(A|B) &amp;=&amp; \frac{P(A)P(B|A)}{P(A)P(B|A)+P(A^c)P(B|A^c)} \\&amp;=&amp; \frac{0.001\cdot 0.95}{0.001\cdot 0.95 + 0.999\cdot 0.05} \\&amp;=&amp; 0.0187\end{eqnarray}$$尽管检验方法非常精确，一个经检测为阳性的人仍然不大可能真正患有这种疾病（患有该疾病的概率小于 $2\%$ ）。根据《经济学人》杂志 $1999$ 年 $2$ 月 $20$ 日的报道，在一家著名的大医院中 $80\%$ 的受访者不知道这类问题的正确答案，而大部分人回答，这个经检测为阳性的人患病概率为 $0.95$ ! 连续随机变量的贝叶斯定律在许多情况下，我们会遇到一个没有观察到的对象。用随机变量 $X$ 代表这种未观察到的量，设其概率密度函数是 $ f_X(x)$ 。我们能够观察到的量是经过噪声干扰的量 $Y$ ，$Y$ 的分布函数是条件分布函数，其条件概率密度函数为： $f_{X|Y}(y|x)$ 。当 $Y$ 的值被观察到以后，它包含 $X$ 的多少信息呢？这类问题与离散随机变量的推断问题类似。现在唯一的不同之处在于处理的是连续随机变量。 上图是推断问题的框图，有一个未观察到的变量 $X$ ，其概率密度函数 $f_X$ 是已知的，同时得到一个观察到的随机变量 $Y$ ，其条件概率密度函数为 $f_{Y|X}(y|x)$ 。给定 $Y$ 的观察值 $y$ ，推断问题变成条件概率密度函数 $f_{X|Y}(x|y)$ 的计算问题。 注意到：当观察到事件 $Y=y$ 以后，所有的信息都包含在条件概率密度函数 $f_{X|Y}(x|y)$ 中，现在只需计算这个条件概率密度函数。利用公式 $f_Xf_{Y|X}=f_{X,Y}=f_Yf_{X|Y}$ 可以得到： $$f_{X|Y}(x|y)=\frac{f_X(x)f_{Y|X}(y|x)}{f_Y(y)}$$这个即所求的公式，与之等价的公式： $$f_{X|Y}(x|y)=\frac{f_X(x)f_{Y|X}(y|x)}{\int_{-\infty}^{+\infty}f_X(t)f_{Y|X}(y|t)dt}$$ 例子通用照明公司生产一种灯泡，已知其使用寿命 $Y$ 为指数随机变量，其概率密度函数为 $\lambda e^{-\lambda y}, y&gt;0$ ，按过往经验，在任意给定的一天参数 $\lambda$ 实际上是一个随机变量，其概率密度函数为区间 $[1, \frac{3}{2}]$ 上的均匀分布。现在随机地取已知灯泡进行试验，得到灯泡的寿命数据。得到数据以后，对于 $\lambda$ 的分布有什么新的认识？ 将 $\lambda$ 看成一个随机变量 $\Lambda$ ，作为对 $\lambda$ 的初始认识，那么根据题意 $\Lambda$ 的概率密度函数是： $$f_{\Lambda(\lambda)}=2, 1\le \lambda \le \frac{3}{2}$$当得到数据 $y$ 以后，关于 $\Lambda$ 的信息包含于条件概率密度函数 $f_{\Lambda, y}(\lambda|y)$ 中，利用连续贝叶斯公式得到： $$f_{ \Lambda|y}(\lambda|y)=\frac{f_\Lambda(\lambda)f_{Y|\Lambda}(y|\lambda)}{\int_{+\infty}^{-\infty}f_{\Lambda}(t)f_{Y|\Lambda}(y|t)dt}=\frac{2\lambda e^{-\lambda y}}{\int_{1}^{\frac{3}{2}}2te^{-ty}dt}，1\le \lambda \le \frac{3}{2}$$ 关于连续随机变量的推断在许多实际问题中，未观察到的随机变量可能是连续的随机变量。例如，在通信问题中传输的信号是一个二进制的信号，经过传输以后，混入的噪声是正态随机变量，这样，观测到的随机变量就是连续的随机变量；或者在医疗诊断中，观察到的量也是连续的测量值，例如：体温或血液样本中的指标。这种情况下需要将贝叶斯公式作适当改变。 现在研究一种特殊情况，未观察到的是一个事件$A$ 。不知道 $A$ 是否发生了。事件 $A$ 的概率 $P(A)$ 是已知的。设 $Y$ 是一个连续的随机变量，并且假定条件概率密度函数 $f_{Y|A}(y)$ 和 $f_{Y|A^c}(y)$ 是已知的。令人兴趣的是事件 $A$ 的条件概率密度函数 $P(A|Y=y)$ 。这个量代表得到的观察值 $y$ 以后关于事件 $A$ 的信息。 由于事件 ${Y=y}$ 是一个零概率事件，转而去考虑事件 ${y \le Y \le y+\delta}$ ，其中 $\delta$ 是一个很小的正数，然后令 $\delta$ 趋于0 。利用贝叶斯公式，令 $f_{Y}(y)&gt;0$ ，我们得到：$$\begin{eqnarray}P(A|Y=y) &amp;\approx&amp; P(A|y\le Y \le y+\delta) \\&amp;=&amp; \frac{P(A)P(y\le Y \le y+\delta|A)}{P(y\le Y \le y+\delta)} \\&amp;\approx&amp;\frac{P(A)f_{Y|A}(y)\delta}{f_Y(y)\delta} \\&amp;=&amp; \frac{P(A)f_{Y|A}(y)}{f_Y(y)}\end{eqnarray}$$利用总概率定律，可将上式的分母写成：$$f_{Y}(y)=P(A)f_{Y|A}(y)+P(A^c)f_{Y|A^c}(y)$$这样得到：$$P(A|Y=y)=\frac{P(A)f_{Y|A}(y)}{P(A)f_{Y|A}(y)+P(A^c)f_{Y|A^c}(y)}$$现在令事件 $A$ 具有形式 $\{N=n\}$ ，其中 $N$ 是一个离散的随机变量，代表未观察到的随机变量。记 $p_N$ 为 $N$ 的分布函数。令 $Y$ 为连续随机变量，对任意 $N$ 的取值 $n$，$Y$ 具有条件概率密度函数 $f_{Y|N}(y|n)$ 。 这样上面的公式变成 ：$$P(N=n|Y=y)=\frac{p_N(n)f_{Y|N}(y|n)}{f_Y(y)}$$利用下面的总概率定律：$$f_Y(y)=\sum\limits_{i}p_N(i)f_{Y|N}(y|i)$$得到：$$P(N=n|Y=y)=\frac{p_N(n)f_{Y|N}(y|n)}{\sum\limits_{i}p_N(i)f_{Y|N}(y|i)}$$ 例子-信号检测设 $S$ 是一个只取2个值的信号（signal）。记 $P(S=1)=p$ 和 $P(S=-1)=1-p$ 。在接收端，得到的信号为 $Y=N+S$ ，其中 $N$ 是一个正态分布的噪声（noise），期望为0，方差为1，并且与 $S$ 相互独立。当观察到的信号为 $y$ 的时候，$S=1$ 的概率是多少？ 对于给定的 $S=s=1, Y$ 是一个正态随机变量，期望为 $s=1$ ，方差为 $1$ 。应用刚才得到的公式：$$P(S=1|Y=y)=\frac{p_S(1)f_{Y|S}(y|1)}{f_Y(y)}=\frac{\frac{p}{\sqrt{2\pi}}e^{-\frac{(y-1)^2}{2}}}{\frac{p}{\sqrt{2\pi}}e^{-\frac{(y-1)^2}{2}}+\frac{1-p}{\sqrt{2\pi}}e^{-\frac{(y+1)^2}{2}}}$$将上式化简得：$$P(S=1|Y=y)=\frac{pe^y}{pe^y+(1-p)e^{-y}}$$注意：当 $y\rightarrow -\infty, P(S=1|Y=y)\rightarrow 0$ ，当 $y\rightarrow \infty, P(S=1|Y=y)\rightarrow 1$ 。 $y$ 在实数轴上变化时， $P(S=1|Y=y)$ 是 $y$ 的严格上升函数，这符合直观的理解。 基于离散观察值的推断在前文连续随机变量的贝叶斯定律中得到的：$$\begin{eqnarray}P(A|Y=y) &amp;\approx&amp; P(A|y\le Y \le y+\delta) \\&amp;=&amp; \frac{P(A)P(y\le Y \le y+\delta|A)}{P(y\le Y \le y+\delta)} \\&amp;\approx&amp;\frac{P(A)f_{Y|A}(y)\delta}{f_Y(y)\delta} \\&amp;=&amp; \frac{P(A)f_{Y|A}(y)}{f_Y(y)}\end{eqnarray}$$反解得到：$$f_{Y|A}(y)=\frac{f_Y(y)P(A|Y=y)}{P(A)}$$根据归一性（$\int_{-\infty}^{+\infty}f_{Y|A}(y)dy=1$），那么得到一个等价的表达式：$$f_{Y|A}(y)=\frac{f_Y(y)P(A|Y=y)}{\int_{-\infty}^{+\infty}f_Y(t)P(A|Y=t)dt}$$这个公式可以用于当事件 $A$ 被观测到时候，对随机变量 $Y$ 进行推断。对于事件 $A$ 是 $\{N=n\}$ 的形式，根据前文：$$P(N=n|Y=y)=\frac{p_N(n)f_{Y|N}(y|n)}{\sum\limits_{i}p_N(i)f_{Y|N}(y|i)}$$得到一个相似的公式对随机变量 $Y$ 进行推断：$$f_{Y|N}(y|n)=\frac{P(N=n|Y=y)\sum\limits_{i}p_N(i)f_{Y|N}(y|i)}{p_N(n)}$$ 总结令 $Y$ 为连续随机变量。 若 $X$ 为连续随机变量，则有：$$f_{X|Y}(x|y)f_Y(y)=f_X(x)f_{Y|X}(y|x)$$和$$f_{X|Y}(x|y)=\frac{f_X(x)f_{Y|X}(y|x)}{f_Y(y)}=\frac{f_X(x)f_{Y|X}(y|x)}{\int_{-\infty}^{+\infty}f_X(t)f_{Y|X}(y|t)dt}$$ 若 $N$ 为离散随机变量，则有：$$f_Y(y)P(N=n|Y=y)=p_N(n)f_{Y|N}(y|n)$$得到贝叶斯定律为：$$P(N=n|Y=y)=\frac{p_N(n)f_{Y|N}(y|n)}{f_Y(y)}=\frac{p_N(n)f_{Y|N}(y|n)}{\sum\limits_{i}p_N(i)f_{Y|N}(y|i)}$$和$$f_{Y|N}(y|n)=\frac{f_Y(y)P(N=n|Y=y)}{p_N(n)}=\frac{f_Y(y)P(N=n|Y=y)}{\int_{-\infty}^{+\infty}f_Y(t)P(N=n|Y=t)dt}$$ 对于事件 $A$ ，关于 $P(A|Y=y)$ 和 $f_{Y|A}(y)$ 具有类似的贝叶斯定律。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典摘录 - 伯努利和泊松随机变量的均值方差]]></title>
    <url>%2F2017%2F08%2F28%2Fmean_and_variance_of_Bernoulli_and_Possion_random%2F</url>
    <content type="text"><![CDATA[Example 2.5. Mean and Variance of the Bernoulli.伯努利随机变量的均值和方差Consider the experiment of tossing a coin, which comes up a head with probability $p$ and a tail with probability $1 - p$. and the Bernoulli random variable $X$ with $PMF$ :$$p_X(k)=\cases{p, &amp; if $k=1$\\1-p, &amp; if $k=0$}$$The mean. second moment. and variance of $X$ are given by the following calculations:$$E[X]=1\cdot p + 0 \cdot (1-p) = p ,\\E[X^2]=1^2\cdot p + 0 \cdot (1-p) = p, \\var(x)=E[X^2]-(E[X])^2=p-p^2=p(1-p)$$ Example 2.7. The Mean of the Poisson. 泊松随机变量的均值方差The mean of the Poisson $PMF $ :$$p_X(k)=e^{-\lambda}\frac{\lambda^k}{k!}, k=0,1,2,\ldots$$can be calculated is follows:$$\begin{eqnarray}E[X] &amp;=&amp; \sum\limits^{\infty}_{k=0}ke^{-\lambda}\frac{\lambda^k}{k!}\\&amp;=&amp; \sum\limits^{\infty}_{k=1}ke^{-\lambda}\frac{\lambda^k}{k!} \quad (\text{k=0的项等于0})\\&amp;=&amp; \lambda\sum\limits_{k=1}^{\infty}e^{-\lambda}\frac{\lambda^{k-1}}{k-1!}\\&amp;=&amp; \lambda\sum\limits_{m=0}^{\infty}e^{-\lambda}\frac{m^{k-1}}{m!} \quad (\text{让m=k-1})\\&amp;=&amp; \lambda\end{eqnarray}$$The last equality is obtained by noting that is the normalization property for the Poisson $PMF$. Example 2.20. Variance of the Binomial and the Poisson. 二项随机变量和泊松随机变量的方差We consider $n$ independent coin tosses, with each toss having probability $p$ of coming up a head. For each $i$, we let $X_i$ be the Bernoulli random variable which is equal to $1$ if the $i$th toss comes up a head, and is 0 otherwise. Then, $X = X_l + X_2 + . . . + X_n$ is a binomial random variable. Its mean is $E[X] = np$. as derived in Example 2. 10. By the independence of the coin tosses. the random variables $X_1 , . . . . X_n$ are independent, and$$var(X)=\sum\limits_{i=1}^{n}var(x_i)=np(1-p)$$As we discussed in Section 2.2. a Poisson random variable $Y$ with parameter $\lambda$ can be viewed as the “limit” of the binomial as $n\rightarrow \infty, p\rightarrow 0$. while $np = \lambda$. Thus, taking the limit of the mean and the variance of the binomial. we informally obtain the mean and variance of the Poisson: $E[Y] = var(Y) = \lambda $ . We have indeed verified the formula $E[Y] = \lambda$ in Example 2.7. To verify the formula $var(Y) = \lambda$, we write$$\begin{eqnarray}E[Y^2] &amp;=&amp; \sum\limits_{k=1}^{\infty}k^2e^{-\lambda}\frac{\lambda^k}{k!} \\&amp;=&amp; \lambda\sum\limits_{k=1}^{\infty}k\frac{e^{-\lambda}\lambda^{k-1}}{(k-1)!} \\&amp;=&amp; \lambda\sum\limits_{m=0}^{\infty}(m+1)\frac{e^{-\lambda}\lambda^{m}}{m!}, m = k-1 \\&amp;=&amp; \lambda[\sum\limits_{m=0}^{\infty}m\frac{e^{-\lambda}\lambda^{m}}{m!}+\sum\limits_{m=0}^{\infty}\frac{e^{-\lambda}\lambda^{m}}{m!}], \\&amp;=&amp; \lambda(E[Y]+1) , \\&amp;=&amp; \lambda(\lambda+1) \\\end{eqnarray}$$ from which$$var(Y)=E[Y^2]-(E[Y])^2=\lambda(\lambda+1)-\lambda^2=\lambda$$]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典摘录-分段常数概率密度函数的均值和方差]]></title>
    <url>%2F2017%2F08%2F27%2Fmean_and_variance_of_a_piecewise_constant_PDF%2F</url>
    <content type="text"><![CDATA[本文摘录自 Introduction to probability, 2nd Edition Example 3.17 Mean and Variance of a Piecewise Constant PDF 假设一个随机变量 $X$ 有分段常数的概率密度函数 $$f_x(x)=\cases{\frac{1}{3}, &amp; if $0 \le x \le 1$, \\ \frac{2}{3}, &amp; if $ 1 &lt; x \le 2$, \\0, &amp; if otherwise}$$ 考虑事件： $$A_1=\{\text{X 位于第一个区间 [0,1]}\}$$ $$A_2=\{\text{X 位于第二个区间 (1,2]}\}$$ 我们从已知的概率密度函数得到： $$P(A_1)=\int_{0}^{1}f_X(x)dx=\frac{1}{3}, \quad P(A_2)=\int_{1}^{2}f_X(x)dx=\frac{2}{3}$$ 因此，条件均值和 $X$ 的条件二阶矩容易计算，因为相关的概率密度函数 $PDF_S$： $f_{X|A_1}$ 和 $f_{X|A_2}$ 是均匀的，回忆例子3.4得到， 均匀分布在区间 $[a,b]$ 上的的随机变量的均值是：$\frac{(a+b)}{2}$ ，它的二阶矩是 $\frac{(a^2+ab+b^2)}{3}$ ，因此： $$\begin{eqnarray}E[X|A_1]&amp;=&amp;\frac{1}{2},\quad E[X|A_2]&amp;=&amp;\frac{3}{2}\\E[X^2|A_1]&amp;=&amp;\frac{1}{3},\quad E[X^2|A_2]&amp;=&amp;\frac{7}{3}\end{eqnarray}$$ 使用总期望定理得到：$$\begin{eqnarray}E[X] &amp;=&amp; P(A_1)E[X|A_1]+P(A_2)E[X|A_2] &amp;=&amp; \frac{1}{3} \cdot \frac{1}{2}+\frac{2}{3}\cdot\frac{3}{2} &amp;=&amp; \frac{7}{6} \\E[X^2] &amp;=&amp; P(A_1)E[X^2|A_1]+P(A_2)E[X^2|A_2] &amp;=&amp; \frac{1}{3}\cdot\frac{1}{3}+\frac{2}{3}\cdot\frac{7}{3} &amp;=&amp; \frac{15}{9}\end{eqnarray}$$那么可以得到方差： $$var(x)=E[X^2]-(E[X])^2=\frac{15}{9}-\frac{49}{36}=\frac{11}{36}$$ 注意： 对于计算均值和方差的方法是容易推广到多分段的常数概率密度函数。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典摘录-正态随机变量]]></title>
    <url>%2F2017%2F08%2F27%2Fnormal_random_variables%2F</url>
    <content type="text"><![CDATA[说明：全文摘自 Introduction to probability, 2nd Edition 3.3 normal random variables 正态随机变量如果一个连续的随机变量 $X$ 的概率密度具有下列形式， 那么这个随机变量称为正态(normal)的或高斯(Gaussian)的。$$f_X(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}$$其中 $u$ 和 $\sigma$ 是密度函数的两个参数，$\sigma$ 还必须是正数。可以证明，$f_X(x)$ 满足下面的概率密度函数的归一化条件（见本章关于定理的习题）：$$\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}e^{\frac{-(x-\mu)^2}{2\sigma^2}}dx=1$$下图是正态分布的密度函数和分布函数 $(\mu=1 \text{ 和 } \sigma^2=1)$ 。 由图可以看出，正态随机变量的概率密度函数是相对于均值 $\mu$ 对称的钟形曲线。当 $x$ 离开 $\mu$ 的时候，概率密度函数的表达式中的项 $e^{\frac{-(x-\mu)^2}{2\sigma^2}}$ 很快地下降。在图中，概率密度函数在区间 $[-1,3]$ 之外非常接近 $0$ 。 正态随机变量的均值和方差可由下列式子给出：$$E[X]=\mu,\quad var(X)=\sigma^2$$由于 $X$ 的概率密度函数相对于 $\mu$ 对称，其均值只能是 $\mu$ 。至于方差的公式，一句定义得：$$var(X)=\frac{1}{\sqrt{2\pi}\sigma}\int_{-\infty}^{+\infty}(x-\mu)^2e^{-\frac{(x-\mu)^2}{2\sigma^2}}dx$$将公式中的积分作积分变量替换 $y=\frac{(x-\mu)}{\sigma}$ 以及分布积分得到：$$\begin{eqnarray}var(X) &amp;=&amp; \frac{\sigma^2}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}y^2e^{-\frac{y^2}{2}}dy \\&amp;=&amp; \frac{\sigma^2}{\sqrt{2\pi}}(-ye^{-\frac{y^2}{2}})|^{+\infty}_{-\infty}+\frac{\sigma^2}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{\frac{y^2}{2}}dy \\&amp;=&amp; \frac{\sigma^2}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{-\frac{y^2}{2}}dy \\&amp;=&amp; \sigma^2\end{eqnarray}$$上面最后的等式，是由于$$\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{+\infty}e^{-\frac{y^2}{2}}dy=1$$这个公式正好是当 $\mu=0$ 和 $\sigma^2=1$ 的时候的正态随机变量的概率密度函数的归一化条件，在本章习题第14题得以证明，截图如下： 正态随机变量具有若干重要的性质。下面的性质尤其重要，并且将在 第四章 Further Topicson Random Variables 的第一节加以证明。 随机变量的正态性在线性变换之下保持不变设 $X$ 是正态随机变量，其均值为 $\mu$ ，方差为 $\sigma^2$ 。若 $a\ne 0$ 和 $b$ 为两个常数，则随机变量$$Y=aX+b$$仍然是正态随机变量，其均值和方差由下式定义给出：$$E[Y]=a\mu+b,\quad var(Y)=a^2\sigma^2$$ 标准正态随机变量设正态随机变量 $Y$ 的期望为 $0$ ，方差为 $1$，则 $Y$ 称为标准正态随机变量。以 $\Phi$ 记为它的 CDF ：$$\Phi(y)=P(Y\le y)=P(Y&lt; y)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{y}e^{\frac{-t^2}{2}}dt$$通常将它的值列成一个表——标准正态累积分布表（见下表：），这是计算有关正态随机变量的概率的重要的工具。标准正态表的每一项提供了 $\Phi(y)=P(Y\le y)$ 的数值，这里 $Y$ 是一个正态随机变量，这个表中 $y\in [0,4.09]$ 。怎么使用这个表呢？举例，为了查找 $\Phi(1.71)$ 的值，查看关于 $1.7$ 所在的行和 $0.01$ 所在的列，得到 $\Phi(1.71)=0.95637$ 。 注意到下表只列出当 $y &gt; 0， \Phi(y) $ 的值，可以利用标准正态随机变量的概率密度函数的对称性，可将 $y &lt; 0$ 时 $\Phi(y)$ 的值推导出来。例如：$$\begin{eqnarray}\Phi(-0.5) &amp;=&amp; P(Y\le -0.5)=P(Y\ge 0.5)=1-P(Y &lt; 0.5) \\&amp;=&amp; 1- \Phi(0.5) = 1-0.69146=0.30854\end{eqnarray}$$可推广$$\forall\ y&gt;0, \Phi(-y)=1-\Phi(y)$$ y +0.00 +0.01 +0.02 +0.03 +0.04 +0.05 +0.06 +0.07 +0.08 +0.09 0.0 0.50000 0.50399 0.50798 0.51197 0.51595 0.51994 0.52392 0.52790 0.53188 0.53586 0.1 0.53983 0.54380 0.54776 0.55172 0.55567 0.55966 0.56360 0.56749 0.57142 0.57535 0.2 0.57926 0.58317 0.58706 0.59095 0.59483 0.59871 0.60257 0.60642 0.61026 0.61409 0.3 0.61791 0.62172 0.62552 0.62930 0.63307 0.63683 0.64058 0.64431 0.64803 0.65173 0.4 0.65542 0.65910 0.66276 0.66640 0.67003 0.67364 0.67724 0.68082 0.68439 0.68793 0.5 0.69146 0.69497 0.69847 0.70194 0.70540 0.70884 0.71226 0.71566 0.71904 0.72240 0.6 0.72575 0.72907 0.73237 0.73565 0.73891 0.74215 0.74537 0.74857 0.75175 0.75490 0.7 0.75804 0.76115 0.76424 0.76730 0.77035 0.77337 0.77637 0.77935 0.78230 0.78524 0.8 0.78814 0.79103 0.79389 0.79673 0.79955 0.80234 0.80511 0.80785 0.81057 0.81327 0.9 0.81594 0.81859 0.82121 0.82381 0.82639 0.82894 0.83147 0.83398 0.83646 0.83891 1.0 0.84134 0.84375 0.84614 0.84849 0.85083 0.85314 0.85543 0.85769 0.85993 0.86214 1.1 0.86433 0.86650 0.86864 0.87076 0.87286 0.87493 0.87698 0.87900 0.88100 0.88298 1.2 0.88493 0.88686 0.88877 0.89065 0.89251 0.89435 0.89617 0.89796 0.89973 0.90147 1.3 0.90320 0.90490 0.90658 0.90824 0.90988 0.91149 0.91308 0.91466 0.91621 0.91774 1.4 0.91924 0.92073 0.92220 0.92364 0.92507 0.92647 0.92785 0.92922 0.93056 0.93189 1.5 0.93319 0.93448 0.93574 0.93699 0.93822 0.93943 0.94062 0.94179 0.94295 0.94408 1.6 0.94520 0.94630 0.94738 0.94845 0.94950 0.95053 0.95154 0.95254 0.95352 0.95449 1.7 0.95543 0.95637 0.95728 0.95818 0.95907 0.95994 0.96080 0.96164 0.96246 0.96327 1.8 0.96407 0.96485 0.96562 0.96638 0.96712 0.96784 0.96856 0.96926 0.96995 0.97062 1.9 0.97128 0.97193 0.97257 0.97320 0.97381 0.97441 0.97500 0.97558 0.97615 0.97670 2.0 0.97725 0.97778 0.97831 0.97882 0.97932 0.97982 0.98030 0.98077 0.98124 0.98169 2.1 0.98214 0.98257 0.98300 0.98341 0.98382 0.98422 0.98461 0.98500 0.98537 0.98574 2.2 0.98610 0.98645 0.98679 0.98713 0.98745 0.98778 0.98809 0.98840 0.98870 0.98899 2.3 0.98928 0.98956 0.98983 0.99010 0.99036 0.99061 0.99086 0.99111 0.99134 0.99158 2.4 0.99180 0.99202 0.99224 0.99245 0.99266 0.99286 0.99305 0.99324 0.99343 0.99361 2.5 0.99379 0.99396 0.99413 0.99430 0.99446 0.99461 0.99477 0.99492 0.99506 0.99520 2.6 0.99534 0.99547 0.99560 0.99573 0.99585 0.99598 0.99609 0.99621 0.99632 0.99643 2.7 0.99653 0.99664 0.99674 0.99683 0.99693 0.99702 0.99711 0.99720 0.99728 0.99736 2.8 0.99744 0.99752 0.99760 0.99767 0.99774 0.99781 0.99788 0.99795 0.99801 0.99807 2.9 0.99813 0.99819 0.99825 0.99831 0.99836 0.99841 0.99846 0.99851 0.99856 0.99861 3.0 0.99865 0.99869 0.99874 0.99878 0.99882 0.99886 0.99889 0.99893 0.99896 0.99900 3.1 0.99903 0.99906 0.99910 0.99913 0.99916 0.99918 0.99921 0.99924 0.99926 0.99929 3.2 0.99931 0.99934 0.99936 0.99938 0.99940 0.99942 0.99944 0.99946 0.99948 0.99950 3.3 0.99952 0.99953 0.99955 0.99957 0.99958 0.99960 0.99961 0.99962 0.99964 0.99965 3.4 0.99966 0.99968 0.99969 0.99970 0.99971 0.99972 0.99973 0.99974 0.99975 0.99976 3.5 0.99977 0.99978 0.99978 0.99979 0.99980 0.99981 0.99981 0.99982 0.99983 0.99983 3.6 0.99984 0.99985 0.99985 0.99986 0.99986 0.99987 0.99987 0.99988 0.99988 0.99989 3.7 0.99989 0.99990 0.99990 0.99990 0.99991 0.99991 0.99992 0.99992 0.99992 0.99992 3.8 0.99993 0.99993 0.99993 0.99994 0.99994 0.99994 0.99994 0.99995 0.99995 0.99995 3.9 0.99995 0.99995 0.99996 0.99996 0.99996 0.99996 0.99996 0.99996 0.99997 0.99997 4.0 0.99997 0.99997 0.99997 0.99997 0.99997 0.99997 0.99998 0.99998 0.99998 0.99998 现在用 $X$ 表示一个均值为 $\mu$ 和方差为 $\sigma^2$ 的正态随机变量。通过定义一个新的随机变量 $Y$ 来(“standardize”)标准化 $X$ ：$$Y=\frac{X-\mu}{\sigma}$$因为 $Y$ 是 $X$ 的线性函数，所以 $Y$ 也是正态随机变量。而且$$E[Y]=\frac{E[X]-u}{\sigma}=0,\quad var(Y)=\frac{var(X)}{\sigma^2}=1$$因此，$Y$ 是一个标准正态随机变量。这个事实可以让我们用 $Y$ 重新定义 $X$ 所表示的事件，然后使用标准正态表去计算。 使用正态分布表的例子某地区的年度降雪量是一个正态随机变量，期望为 $\mu=60$ 英寸，标准差为 $\sigma=20$ 。本年度降雪量至少为 $80$ 英寸的概率有多大？ 记 $X$ 为年降雪量，令$$Y=\frac{X-\mu}{\sigma}=\frac{X-60}{20}$$显然 $Y$ 是标准正态随机变量。$$P(X\ge 80)=P(\frac{X-60}{20} \ge \frac{80-60}{20})=P(Y\ge \frac{80-60}{20})=P(Y\ge 1)=1-\Phi(1)$$其中 $\Phi$ 为标准正态累积分布函数。通过查询上表得到：$\Phi(1)=0.84134$ ，因此$$P(X\ge 80)=1-\Phi(1)=0.15866$$推广这个例子中的方法，得到如下： 正态随机变量的累积分布函数计算对于均值为 $\mu$ 方差为 $\sigma^2$ 的正态随机变量 $X$ ，使用一下步骤： 标准化 $X$ ：先减去 $\mu$ 再除以 $\sigma$ 来获取标准随机变量 $Y$ 。 从标准正态表中读取累积分布函数值：$$P(X\le x)=P(\frac{X-\mu}{\sigma}\le \frac{x-\mu}{\sigma})=P(Y\le \frac{x-\mu}{\sigma})=\Phi(\frac{x-\mu}{\sigma})$$正态随机变量经常使用在信号处理和通信工程中去对噪音和信号失真进行建模。 例3.8 信号侦测二进制信息用信号 $s$ 传输，这个信息要么是 $-1$ 和 $+1$ 。信号在信道传输过程中会伴随一些噪声，噪声满足均值为 $\mu=0$ ，方差为 $\sigma^2$ 的正态分布。接收器会接收到混有噪音的信号 ，如果接收到的值为小于 $0$ ，那么就认为信号为 $-1$ ，如果接收到的值为大于 $0$ ，那么就认为接收到的信号为 $+1$ 。问这种判断方法的误差有多大？ 误差只有出现在下面两种情况： 实际被传输的信号为 $-1$，但是噪声变量 $N$ 值至少是 $1$ ，因此 $s+N=-1+N \ge 0$ 。 实际被传输的信号为 $+1$，但是噪声变量 $N$ 值小于 $-1$ 。因此 $s+N=1+N &lt;0$ 。 因此这种判断方法在情况1下出现误差的概率为：$$\begin{eqnarray}P(N\ge 1) &amp;=&amp; 1-P(N &lt; 1) = 1 - P(N&lt;1)=1-P(\frac{N-\mu}{\sigma}&lt;\frac{1-\mu}{\sigma}) \\&amp;=&amp; 1- \Phi(\frac{1-\mu}{\sigma}) \\&amp;=&amp; 1-\Phi(\frac{1}{\sigma})\end{eqnarray}$$在第2种情况下出现误差的概率根据正态分布的对称性得到与前一种情况一样。$\Phi(\frac{1}{\sigma})$ 能够从正态分布表得到。例如对于 $\sigma=1$ ，$\Phi(\frac{1}{\sigma})=\Phi(1)=0.84134$ ，所以出现误差的概率为 $0.15864$ 。 正态随机变量扮演一个重要的角色在各种广泛的概率模型中，其原因是在物理、工程和统计中，正态随机变量能够很好地模拟许多独立因素的叠加效应。数学上，关键事实是大量独立同分布的随机变量（不必为正态）的和的分布近似地服从正态分布，而这个事实与各个和项的具体的分布无关的。这个事实就是著名的中心极限定理，这个将在本书第五章详细说明。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典摘录-[累积]分布函数CDF]]></title>
    <url>%2F2017%2F08%2F26%2Fcumulative_distribution_function%2F</url>
    <content type="text"><![CDATA[说明：全文摘自 Introduction to probability, 2nd Edition 分布函数我们分别用概率质量函数 PMF(Probability Mass Function) 和概率密度函数 PDF(Probability Density Function) 来刻画随机变量 $X$ 的取值规律。现在希望用一个统一的数学工具去刻画随机变量的取值规律。 ​分布函数（用记号 CDF 表示简称）就能完成这个任务。 $X$ 的 CDF 是一个 $x$ 的函数，对每一个 $x$ ，$F_X(x)$ 定义为 $P(X\le x)$ 。特别地，当 $X$ 为离散或连续的情况下：$$F_X(x)=P(X\le x)=\cases{\sum\limits_{k\le x}p_X(k), \text{若 $X$ 是离散的}\\\int_{-\infty}^{x}f_X(x)dt, \text{若 $X$ 是连续的 }}$$分布函数又称为累积分布函数（cumulative distribution function），累积意味着 $F_X(x)$ 将 $X$ 取值的概率由 $-\infty\rightarrow x$。 在一个概率模型中，随机变量可以有不同的类型，可以是离散的，也可以是连续的，甚至可以是既非离散的也非连续的。但不管是什么类型的随机变量，它们有一个共同的特征，即都有一个分布函数，这是因为 $\{X\le x\}$ 是一个随机事件，这些事件的概率形成概率分布。今后，凡是通过 PMF\PDF\CDF刻画事件 $\{X\le x\}$ 概率的，都称为随机变量 $X$ 的概率律。因此离散情况下的分布列，连续情况下的概率密度函数以及一般情况下的分布函数都是相应的随机变量的概率律。 下图分别给出不同的离散随机变量和连续随机变量的 CDF 的一些说明。从这些图像以及 CDF 的定义，可以得到 CDF 的某些性质。 上图这些离散随机变量的 CDF ，通过随机变量的概率质量函数（PMF）可求得相应的分布函数：$$F_X(x)=P(X\le x)=\sum\limits_{k\le x}p_{X}(k)$$这个函数是一个阶梯函数，在具有正概率的那些点上具有跳跃。在跳跃点上， $F_X(x)$ 取较大的那个值，即 $F_X(x)$ 保持右连续。 上图的这些连续随机变量的 $CDF$ 。通过随机变量的概率密度函数（PDF）可求得相应的分布函数：$$F_X(x)=P(X\le x)=\int_{-\infty}^{+\infty}f_X(t)dt$$概率密度函数 $f_X(x)$ 可由 CDF 经求微分得到：$$f_X(x)=\frac{dF_X(x)}{dx}(x)$$对于连续随机变量，CDF 是连续的 CDF 的性质假设 $X$ 的 CDF $F_X(x)$ 是由下式定义的 ：$$F_X(x)=P(X\le x), \forall x$$并且 $F_X(x)$ 具有下列性质： $F_X(x)$ 是 $x$ 的单调非减函数：若 $x\le y$ ，则 $F_X(x)\le F_X(y)$ 。 当 $x\rightarrow -\infty$ 的时候，则 $F_X(x)\rightarrow 0$ ，当 $x\rightarrow +\infty$ ，则 $F_X(x)\rightarrow 1$ 。 当 $X$ 是离散随机变量的时候， $F_X(x)$ 为阶梯函数。 当 $X$ 是连续随机变量的时候， $F_X(x)$ 为 $x$ 的连续函数。 当 $X$ 是离散随机变量并且取整数数值的时候，分布函数和概率质量函数（PMF）可以利用求和或差分互求：$$F_X(k)=\sum\limits_{i=-\infty}^{k}p_X(i)\\p_X(k)=P(X\le k)-P(X\le k-1)=F_X(k)-F_X(k-1)$$其中 $k$ 可以是任意整数。 当 $X$ 是连续随机变量的时候，分布函数与概率密度函数可以利用积分和微分互求：$$F_X(x)=\int_{-\infty}^{x}f_X(t)dt,\quad f_X(x)=\frac{dF_X}{dx}(x)$$(第二个等式只在分布函数可微的那些点上成立) 有时候为了计算随机变量的概率质量函数或概率密度函数，首先计算随机变量的分布函数会更方便些。在连续随机变量的情况下，将在4.1节系统地介绍用该方法求随机变量的函数的分布。下面是一个离散随机变量的计算例子。 例子几个随机变量的最大值你参加某种测试，按规定三次测试的最高成绩作为你的最后成绩，设 $X=max\{X_1,X_2,X_3\}$ ，其中 $X_1,X_2,X_3$ 是三次测试成绩，$X$ 是你的最后的成绩。假设你的每次测试成绩是 1 分到 10 分之间，并且 $P(X=i)=\frac{1}{10}, i=1,…,10$ 。现在求最终成绩 $X$ 的概率质量函数。 采用间接方法求分布函数。首先计算 $X$ 的 CDF，然后通过$$p_X(k)=F_X(k)-F_X(k-1), i=1,\ldots,10$$得到 $X$ 的概率质量函数。对于 $F_X(k)$ ，得到：$$\begin{eqnarray}F_X(k) &amp;=&amp; P(X\le k) \\&amp;=&amp; P(X_1\le k, X_2\le k, X_3\le k) \\&amp;=&amp; P(X_1\le k)P(X_2\le k)P(X_3\le k) \\&amp;=&amp; (\frac{k}{10})^3\end{eqnarray}$$此处第三个等式是由事件 $\{X_1\le k\},\{X_2\le k\},\{X_3\le k\}$ 相互独立所致。这样 $X$ 的概率质量函数为：$$p_X(k)=(\frac{k}{10})^3-(\frac{k-1}{10})^3, k=1,\ldots,10$$本例的方法可推广到 $n$ 个随机变量 $X_1,\ldots,X_n$ 的情况。如果对每一个 $x$ ，事件 $\{X_1\le x\},\ldots, \{X_n\le x\}$ 相互独立，则 $X=max\{X_1,\ldots,X_n\}$ 的 CDF 为：$$F(x)=F_{X_1}(x)\cdots F_{X_n}(x)$$利用这个公式，在离散情况下通过差分可得到 $P_X(x)$ ，在连续情况下通过微分可得到 $f_X(x)$ 。 距离的分布函数和概率密度函数习题3.5 ：按照均匀分布律，在一个三角形中随机的选取一个点，设已知三角形的高，求这个点到底边的距离 $X$ 的分布函数和概率密度函数。 用 $b$ 表示底的长度，$h$ 表示三角形的高度，$A=\frac{bh}{2}$ 表示三角形的面积。随机地在三角形内选取一个点，然后画一条平行于三角形底边的辅助直线，用 $A_x$ 表示由这条辅助线构成的小三角形的面积，那么这个小三角形的高度即 $h-x$ ，它的底边按比例求得：$b\frac{h-x}{h}$ ，因此 $A_x=\frac{b(h-x)^2}{2h}$ 。对于 $x\in [0,h]$ ，得到：$$F_X(x)=P(0&lt; x \le x)=1-P(X&gt;x)=1-\frac{A_x}{A}=1-\frac{\frac{b(h-x)^2}{2h}}{\frac{bh}{2}}=1-(\frac{h-x}{h})^2$$当 $x&lt;0,$ 那么 $F_X(x)=0$ ; 当 $x&gt;h,$ 那么 $F_X(x)=1$ 。 概率密度函数可以对累积分布函数 CDF 进行求微分得到：$$f_X(x)=\frac{dF_X}{dx}(x)=\cases{\frac{2(h-x)}{h^2}, &amp; 当 $0\le x \le h$\\0, &amp; 其他情况}$$ 等待时间习题3.6 ：Jane去银行取款，有1个或0个顾客在她前面，这两种情况是等可能的。已知一个顾客的服务时间是一个指数随机变量，参数为 $\lambda$ 。那么Jane所等待的时间分布函数是？ 用 $X$ 表示等待的时间，用 $Y$ 表示在Jane之前顾客的数量。于是得到：$\forall x &lt;0, F_X(x)=0$ ，其他情况下，根据题意得：$$F_X(x)=P(X\le x)=\frac{1}{2}P(X\le x| Y=0)+\frac{1}{2}P(X\le x|Y=1)$$又因为$$P(X\le x|Y=0)=1,\quad P(X\le x|Y=1)=1-e^{-\lambda x}$$得到$$F_X(x)=\cases{\frac{1}{2}(2-e^{-\lambda x}), &amp; if $x \ge 0$ \\0, &amp; 其他情况}$$注意：这个累积分布函数 CDF 在 $x=0$ 处连续，随机变量 $X$ 既不是离散的也不是连续的。 投飞标游戏Alvin在进行飞镖游戏，飞镖的靶是一块半径为 r 的圆板。记 $X$ 为飞镖的落点到靶心的距离。假设落点在靶板上均匀地分布。 (a) 求出 $X$ 的概率密度函数、均值和方差。 $X$ 的累积分布函数比较容易求得：$$F_X(x)=\cases{P(X\le x)=\frac{\pi x^2}{\pi r^2}=(\frac{x}{r})^2, &amp; if $\forall x\in [0,r]$\\0, &amp; if $x &lt; 0$\\1, &amp; if $x&gt;r$}$$通过微分，得到概率密度函数：$$f_X(x)=\cases{\frac{2x}{r^2}, &amp; if $0\le x\le r$\\0, &amp; otherwise}$$进而通过积分得到：$$E[X]=\int_{0}^{r}\frac{2x^2}{r^2}dx=\frac{2r}{3}\\E[X^2]=\int_{0}{r}\frac{2x^3}{r^2}dx=\frac{r^2}{2}\\var(X)=E[X^2]-(E[X])^2=\frac{r^2}{2}-\frac{4r^2}{9}=\frac{r^2}{18}$$(b) 靶上画出了一个半径为 $t$ 的同心圆。若 $X\le t$ ，Alvin的得分为 $S=\frac{1}{X}$ ，其他情况 $S=0$ 。求出 $S$ 的分布函数。 $S$ 是不是连续随机变量？ 由题意得：当且仅当 $X\le t$ ，Alvin 获得一个介于 $[\frac{1}{t}, +\infty)$ 的分数s，其它情况下，他的得分为 0 。因此：$$F_S(s)=\cases{0, \quad \text{if $s&lt;0$}\\P(S\le s)=1-P(X\le t), \quad \text{if $0\le s\le \frac{1}{t}$ (即Alvin击中了内圆之外)} \\P(S\le s)=P(X\le t)P(S\le s|X\le t)+P(X&gt;t)P(S\le s|X&gt;t) \quad \text{if $s &gt; \frac{1}{t}$}}$$根据题意，得到：$$P(X\le t)=\frac{t^2}{r^2},\quad P(X&gt;t)=1-\frac{t^2}{r^2}$$而且因为当 $X&gt;t, S=0$ ， 所以 $P(S\le s|X&gt; t)=1$ 。 进而得到：$$P(S\le s| X\le t)=P(\frac{1}{X}\le s|X\le t)=P(\frac{1}{s}\le X|X\le t) = \frac{P(\frac{1}{s}\le X \le t)}{P(X\le t)} =\frac{\frac{\pi t^2 -\pi(\frac{1}{s})^2}{\pi r^2}}{\frac{\pi t^2}{\pi r^2}}=1-\frac{1}{s^2t^2}$$最后得到：$$F_S(s)=\cases{ 0, &amp; \text{if }s&lt;0 \\ 1-\frac{t^2}{r^2}, &amp; \text{if } 0\le s \le \frac{1}{t}\\ 1-\frac{1}{s^2r^2} &amp; \text{if } \frac{1}{t}&lt;s}$$因为 $F_S(s)$ 在 $s=0$ 处不连续，所以随机变量 $S$ 不是连续的。 几何和指数随机变量的分布函数由于分布函数对一切随机变量都适用，可以利用它来探索离散和连续随机变量之间的关系。特别地，此处讨论几何随机变量和指数随机变量之间的关系。 设 $X$ 是一个几何随机变量，其参数为 $p$ ，即 $X$ 是在伯努利独立试验序列中直到第一次成功所需要的试验次数，而伯努利试验的参数为 $p$ 。这样对于 $k=1,2\cdots,$ 得到 $P(X=k)=p(1-p)^{k-1}$ ，而 $X$ 的 CDF 为：$$F_{geo}(n)=\sum\limits_{k=1}^{n}p(1-p)^{k-1}=p\frac{1-(1-p)^n}{1-(1-p)}=1-(1-p)^n,\quad n=1,2,\cdots$$现在设 $X$ 是一个指数随机变量，其参数 $\lambda&gt;0$ 。其 CDF 是$$F_{exp}(x)=P(X\le x)=0,\quad x\le 0\\F_{exp}(x)=\int_{0}^{x}\lambda e^{-\lambda t}dt=-e^{-\lambda t}|^{x}_{0}=1-e^{-\lambda x},\quad x&gt;0$$现在比较两个分布函数，令 $\delta=\frac{-ln(1-p)}{\lambda}\rightarrow \delta\lambda=-ln(1 - p)$ ，这样得到：$$e^{-\lambda\delta}=1-p \quad (*)$$那么，将 $(*)​$ 代入 $F_{geo}(n)​$ 得：$1-(e^{-\lambda\delta})^n=1-e^{-n\lambda\delta}​$ ，而分布函数 $F_{exp}​$ 在 $x=n\delta​$ 处为： $1-e^{-\lambda n\delta}=1-e^{-n\lambda\delta}​$ 是与 $F_{geo}​$ 在 $n​$ 处相等的，$n=1,2,\cdots​$ ，即：$$F_{exp}(n\delta)=F_{geo}(n)=, n=1,2,\cdots$$现在假定以很快的速度抛掷一枚不均匀的硬币 (每 $\delta$ 秒抛掷一次，$\delta \ll 1$)，每次抛掷，正面向上的概率为 $p=1-e^{-\lambda\delta}$ 。这样第一次得到正面向上所抛掷的次数为 $X$ ，第一次得到正面向上的时刻为 $X\delta$ ，$X\delta$ 与参数为 $\lambda$ 的指数随机变量十分接近，这只需看它们的分布函数即可（看下图）。这在本书第六章讨论伯努利和泊松分布过程的时候，这种关系显得特别重要。 几何随机变量和指数随机变量的分布函数之间的关系。图中离散分布函数为 $X\delta$ 的分布函数，$X$ 是参数为 $p=1-e^{-\lambda x}$ 的几何随机变量。当 $\delta\rightarrow 0$ 时，$X\delta$ 的分布函数趋于指数分布函数 $1-e^{-\lambda x}$ 。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典摘录-均匀随机变量的均值和方差]]></title>
    <url>%2F2017%2F08%2F26%2Fmean_and_variance_of_uniform_random_variable%2F</url>
    <content type="text"><![CDATA[说明：全文摘自Introduction to probability, 2nd Edition 均匀分布的离散随机变量按照定义，离散均匀随机变量的取值范围由相邻的整数所组成的有限集合，而取每个整数的概率都是相等的。这样它的分布列： $$p_X(k)=\cases{\frac{1}{b-a+1}, &amp; if k=a, a+1, … ,b\\0, &amp; otherwise}$$ 其中$a,b$ 是两个整数，作为随机变量的值域的两个端点，$a&lt;b$。由于它的概率函数相对于(a+b)/2 是对称的，所以其均值为： $$E[X]=\frac{a+b}{2}$$ 为计算$X$的方差，先考虑a=1和b=n的简单情况。利用归纳法可以证明： $$E[X^2]=\frac{1}{n}\sum\limits_{k=1}^{n}k^2=\frac{1}{6}(n+1)(2n+1)$$ （具体证明过程留作习题）。这样利用一、二阶矩，可得到$X$的方差$$\begin{eqnarray*}var(X)&amp;=&amp; E[X^2]-(E[X])^2\\&amp;=&amp;\frac{1}{6}(n+1)(2n+1)-\frac{1}{4}(n+1)^2\\&amp;=&amp;\frac{n^2-1}{12}\end{eqnarray*}$$对于 $a$ 和 $b$ 的一般情况，实际上在区间 $[a,b]$上的均匀分布与在区间 $[1,b-a+1]$ 上的分布之间的差异，只是一个分布是另外一个分布的偏移，因此两者具有相同的方差（此处区间 $[a,b]$ 是指处于 $a$ 和 $b$ 之间的整数的集合）。这样在一般的情况下，$X$ 的方差只需将简单的情况下公式中的 $n$ 替换成 $b-a+1$ ，即： $$var(X)=\frac{(b-a+1)^2-1}{12}=\frac{(b-a)(b-a+2)}{12}$$ 均匀分布的连续随机变量摘录自 Example 3.4. Mean and Variance of the Uniform Random Variable 设随机变量 $X$ 的分布为 $[a,b]$ 上的均匀分布，得到：$$\begin{eqnarray*}E[X] &amp;=&amp; \int_{-\infty}^{+\infty}xf_X(x)dx \\&amp;=&amp; \int_{a}^{b}x\frac{1}{b-a}dx \\&amp;=&amp; \frac{1}{b-a}\cdot \frac{1}{2}x^2|^{b}_{a} \\&amp;=&amp; \frac{1}{b-a}\cdot\frac{b^2-a^2}{2} \\&amp;=&amp; \frac{b+a}{2}\end{eqnarray*}$$这个期望值刚好等于 $PDF$ 的对称中心 $\frac{b+a}{2}$ 。 为求得方差，先计算 $X$ 的二阶矩：$$\begin{eqnarray*}E[X^2] &amp;=&amp; \int_{a}^{b}\frac{x^2}{b-a}dx \\&amp;=&amp; \frac{1}{b-a}\cdot\int_{a}{b}x^2dx \\&amp;=&amp; \frac{1}{b-a}\cdot \frac{1}{3}x^3|_{a}^{b} \\&amp;=&amp; \frac{b^3-a^3}{3(b-a)} \\&amp;=&amp; \frac{a^2+ab+b^2}{3} \\\end{eqnarray*}$$这样随机变量 $X$ 的方差为：$$var(X)=E[X^2]-(E[X])^2=\frac{a^2+ab+b^2}{3}-\frac{(a+b)^2}{4}=\frac{(b-a)^2}{12}$$]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典摘录-样本均值的期望方差与模拟的方法估计概率]]></title>
    <url>%2F2017%2F08%2F25%2Fmean_and_variance_of_sample_and_estimating_probability_by_simulation%2F</url>
    <content type="text"><![CDATA[样本均值的期望方差摘录自：Introduction to probability, 2nd Edition Example 2.21. Mean and Variance of the Sample Mean 我们希望估计总统的支持率。为此，我们随机地选取n个选民，询问他们的看法。令 $x_i​$ 表示 $i​$ 个被问的选民的态度： $$X_i = \cases{1, \text{若第 $i$ 个被问的选民支持总统}\\0, \text{若第 $i$ 个被问的选民不支持总统}}$$ 假设$X_1,\ldots, X_n$为独立同分布的伯努利随机变量，其均值为 $p$，方差为 $p(1-p)$ 。此处我们将 $p$ 认为选民支持总统的概率，并且将对调查得到的回应进行平均处理，计算样本均值 $S_n$ ，把 $S_n$ 定义为 $$S_n=\frac{X_1+ \ldots + X_n}{n}$$ 因此，随机变量 $S_n$ 是对n个选民抽样的支持率。 由于 $S_n$ 是 $X_1, \ldots, X_n$ 的一个线性函数，我们利用均值的线性关系得到， $E[S_n]=\sum\limits_{i=1}^{n} E[\frac{X_i}{n}]=\sum\limits_{i=1}^{n}\frac{1}{n}E[X_i]= \sum\limits_{i=1}^{n}\frac{1}{n}p=p=E[X]$ 再利用$X_1,\ldots, X_n$ 的独立性，可以得到： $$var(S_n) = \sum\limits_{i=1}^{n}var(\frac{X_i}{n}) = \sum\limits_{i=1}^{n}\frac{1}{n^2}var(X_i) = \frac{p(1-p)}{n}$$ 样本均值为 $S_n$ 被认为是支持率很好的估计，这是因为它的期望值刚好是 $p$。然后反映精度的方差随着样本大小的$n$ 增大的时候，变得越来越小。 注意，上例中即使 $X_i$ 不是伯努利随机变量，结论 $$var(S_n) = \frac{var(X)}{n}$$ 仍然成立，只要 $X_i$ 之间相互独立，毕竟期望和方差与 $i$ 无关。因此，随着样本大小增加，样本均值仍然是随机变量的均值的一个很好的估计。我们将在第5章再详细讨论样本均值的这些属性，并且与大数定律结合起来。 模拟的方法估计概率摘录自：Introduction to probability, 2nd Edition Example 2.22. Estimating Probabilities by Simulation 在许多实际问题中，有时候计算一个事件的概率是十分困难的，然后我们可以用物理方法或计算机方法重复地进行试验，这些试验结果可以显示事件是否发生。利用这种模拟方法可以以很高的精度计算某事件的概率。可以独立地模拟试验 $n$ 次，并且记录 $n$ 次试验中的 $A$ 发生的次数 $m$，用 $\frac{m}{n}$ 去近似概率 $P(A)$。例如在抛掷硬币试验中，计算概率 $p=P$ （出现正面），独立地抛掷 $n$ 次，用比值（记录中出现的正面次数除以试验总次数n）去逼近概率$p$。 为计算这种方法的精确度，考虑 $n$ 个独立同分布的伯努利随机变量 $X_1,\ldots, X_n$，每个 $X_i$ 的概率质量函数： $$p_{X_i}(k)=\cases{P(A), if\ k=1\\1-P(A), if\ k=0}$$ 在模拟环境中，$X_i$ 有关于第 $i$ 次试验的结果，如果第 $i$ 次的试验结果属于 $A$ ，那么 $X_i$ 取值为1，那么随机变量的取值（$$X=\frac{X_1+X_2+\ldots+X_n}{n}$$） 就是概率 $P(A)$ 的估计值。由例 2.21 的结果知，$X$ 的期望为 $P(A)$，方差为 $\frac{P(A)(1-P(A))}{n}$。故 $n$ 很大时，$X$提供了 $P(A)$ 的精确的估计。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典摘录-指数型随机变量的均值和方差]]></title>
    <url>%2F2017%2F08%2F25%2Fexponential_random_variables%2F</url>
    <content type="text"><![CDATA[说明：全文摘自 Introduction to probability, 2nd Edition 一个指数型随机变量是拥有以下形式的概率密度函数：$$f_X(x)=\cases{ \lambda e^{-\lambda x}, &amp; if $x\ge 0$ \\ 0, &amp; 其他情况}$$这个公式中$\lambda$ 是一个正数，这是一个符合概率归一性的定义，因为：$$\int_{-\infty}^{+\infty}f_X(x)dx=\int_{-\infty}^{+\infty}\lambda e^{-\lambda}dx=-e^{\lambda x}|_0^{+\infty}=1$$注意，指数型概率密度函数具有这样的特性：$X$ 超过某个值的概率随着这个值的增加而按指数递减$$\forall a\ge 0,P(X\ge a)=\int_{a}^{\infty}\lambda e^{-\lambda x}dx=-e^{-\lambda x}|_a^{+\infty}=e^{-\lambda a}$$由概率密度函数得到累积分布函数：$$\forall x \ge 0, P(X\le x)=\int_{0}^{x}\lambda e^{-\lambda x}dx=1-e^{-\lambda x}$$ 指数型随机变量能够对直到事件发生的时间建模，例如：消息到达计算机的时间，设备的使用寿命，灯泡的寿命，事故发生的时间等等（An exponential random variable can, for example, be a good model for the amount of time until an incident of interest takes place）。将在后面的章节看到指数型随机变量与几何随机变量紧密关联，几何随机变量也与相关事件发生的（离散）时间相关联。指数型随机变量将在第六章随机过程的学习中扮演重要的角色。但是目前为止，仅仅视它为一种特殊的可分析追中的随机变量。 指数型随机变量的均值和方差：$$\begin{eqnarray}E[X] &amp;=&amp; \int_{0}^{\infty}x\lambda e^{-\lambda x}dx \\&amp;=&amp; (-xe^{-\lambda x})|_0^{\infty} + \int_{0}^{\infty}e^{-\lambda x}dx \quad\text{这一步利用分部积分法}\\&amp;=&amp; 0-\frac{e^{-\lambda x}}{\lambda}|_0^{\infty}\\&amp;=&amp; \frac{1}{\lambda}\end{eqnarray}$$在此利用分布积分法，可得到 $X$ 的二阶矩：$$\begin{eqnarray}E[X^2] &amp;=&amp; \int_{0}^{\infty}x^2\lambda e^{-\lambda x}dx \\&amp;=&amp; (-x^2e^{-\lambda x})|_0^{\infty}+\int_{0}^{\infty}2xe^{-\lambda x}dx&amp;=&amp; 0+ \frac{2}{\lambda}E[X]\\&amp;=&amp; \frac{2}{\lambda^2}\end{eqnarray}$$最后利用公式 $var(x)=E[X^2]-(E[X])^2$ ，得到：$$var(X)=\frac{2}{\lambda^2}-\frac{1}{\lambda}=\frac{1}{\lambda^2}$$ 例3.5小陨石落入非洲撒哈拉沙漠的时间是遵从指数族分布的。具体地说，从某一观察者开始观察，知道发现一个陨石落到沙漠中，这个时间被模拟成指数型随机变量，其均值为 $10$ 天，现在假定，目前时间为晚上 $12$ 点整。问第二天早晨 $6:00$ 到傍晚 $6:00$ 之间陨石首次落下的概率是多少？ 假定 $X$ 是为了观察陨石落下所需要的等待时间。由于 $X$ 满足指数型分布，均值为 $\frac{1}{\lambda}=10$ ，由此得：$\lambda=\frac{1}{10}$ 。所求的概率为：$$P(\frac{1}{4}\le X \le \frac{3}{4})=P(X\ge \frac{1}{4})-P(X\ge \frac{3}{4})=e^{-\frac{1}{40}}-e^{-\frac{3}{40}}=0.0476$$求解这个过程中利用了连续型随机变量 $P(X\ge a)=P(X&gt; a)=e^{-\lambda a}$ 。 指数随机变量的无记忆性一个灯泡的使用寿命 $T$ 是一个指数随机变量，其参数为 $\lambda$ 。Ariadne 将灯打开后离开房间，在外面呆了一段时间以后（时间长度为 $t$），他回到房间后，灯还亮着。这相当于事件 $A=\{T&gt;t\}$ 发生了。记 $X$ 为灯泡的剩余寿命，问 $X$ 的分布函数是什么？ 解答： 实际上 $X$ 是在 $A$ 发生条件下的寿命，得到：$$\begin{eqnarray}P(X&gt; x|A) &amp;=&amp; P(T&gt;t+x|T&gt;t) \\&amp;=&amp; \frac{P(T&gt;t+x, T &gt; t)}{P(T&gt;t)} \\&amp;=&amp; \frac{P(T&gt; t+x)}{P(T&gt;t)} \\&amp;=&amp; \frac{e^{-\lambda(t+x)}}{e^{-\lambda t}}\\&amp;=&amp; e^{-\lambda x}\end{eqnarray}$$灯泡的剩余寿命 $X$ 的分布函数是指数分布，其参数也是 $\lambda$ ，这和灯泡已经亮了多少个小时是无关的。指数分布的这个性质就是分布的无记忆性。一般地，若将完成某个任务所需要的时间的分布定位指数分布。那么只要这个任务没有完成，那么要完成这个任务所需要的剩余时间的分布仍然是指数分布，并且其参数也是不变化的。 应用一个粗心的教授错误地将两个学生的答疑时间安排在了同一个时间段。已知两位同学的答疑时间长度是两个相互独立的并且同分布的随机变量，分布是指数分布，期望值为 $30$ 分钟，第一个学生按时到达，5分钟以后，第二个学生也到达。从第一个学生到达起直到第二个学生离开所需要时间的期望值是？ 解答： 用 $T_{total}$ 表示教授答疑总共用时的随机变量，用 $T_{s_1}, T_{s_2} $ 表示教授分别对学生 $1$ 和学生 $2$ 答疑时间，那么$$E[T_{total}]=P(T_{s_1}&lt; 5) \cdot E[5+E[T_{s_2}]] + P(T_{s_1} \ge 5)(E[T_{s_1}|T_{s_1}\ge5]+E[T_{s_2}])$$据题目得：$E[T_{s_1}]=E[T_{s_2}]=30$ ，利用指数型随机变量的无记忆性得到：$E[T_{s_1}|T_{s_1}\ge5] = 5+E[T_{s_1}] = 35$ 。$$P(T_{s_1} \ge 5) = e^{-\frac{1}{30}\cdot5} \\P(T_{s_1} &lt; 5)=1-P(T_{s_1}\ge 5)=1-e^{-\frac{1}{30}\cdot5}\\$$因此：$$E[T_{total}]=(1-e^{-\frac{1}{30}\cdot5})\cdot (5+30)+(e^{-\frac{1}{30}\cdot5})\cdot (35+30)=35+30\cdot e^{-\frac{5}{30}}=60.394$$]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典摘录-两个信封的悖论]]></title>
    <url>%2F2017%2F08%2F24%2Fthe_two-envelopes_paradox%2F</url>
    <content type="text"><![CDATA[本文摘录自：Introduction to probability, 2nd Edition Example 2.18. The Two-Envelopes Paradox. 这是一个广泛兴趣的智力测验问题，它涉及有关条件期望的数学要点。 主持人给你两个信封，并且告诉你两个信封里有现金，其中一个信封里的钱是另一个信封里的$m$倍（$m&gt;1$且是一个整数）。当你打开一个信封的时候，看到信封里面的钱数以后，你有两个选择，一是收下这个信封里的钱作为你的奖金，二是放弃这个信封的钱，选择另一个信封里的钱作为你的奖金。那么有什么好的策略可使你拿到较多的奖金呢？ 下面有一条推理，它证明应该转向选择第二个信封的。用A表示你打开的信封，B 是你可能换的信封，$x$和$y$分别表示信封A和B中的钱。论证如下：由于要么$y={x\over m}$ 要么 $y=mx$ ，而且两种情况发生的概率都是${1\over 2}$，因此给定的 $x,y$ 的期望值为： $$\frac{1}{2}\cdot \frac{x}{m} + \frac{1}{2}\cdot mx= \frac{1}{2}(\frac{1}{m}+m)x=\frac{1+m^2}{2m}x&gt;x$$ 这样，你应该总是转向信封B。当你随机选择到B的时候，由于同样的理由，又得到转回到A。这样陷入了矛盾之中，因为按照这个推理是不管选择到信封是哪一个都要选择另外一个信封作为奖金。 其实在这个悖论中，有两个假设是有瑕疵的： 对于两个信封内的钱，你是无法预先知道的，当给定$x$的值以后，你以为知道的就是$y=\frac{x}{m}$或者$y=mx$ 。而且没有理由哪一种情况更有可能。 用随机变量$X$和$Y$表示两个信封内的钱，若$E[Y|X=x]&gt;x, x\in \forall$ 成立，那么总是转向选择另一个信封能得到更多的期望奖金。 让我们仔细审查这两个假设： 假设1是有瑕疵的，因为它依赖他一个不完整的确定的概率模型。事实上，各种模型的事件中，包含$X$和$Y$的可能取值，都必须有一个确定概率。有了这样$X$ $Y$的概率信息，$X$的值可能会揭示$Y$取值的大量信息。例如，假设下面这个概率模型：某个人选择 $Z$ 元放在一个信封内，$Z$ 的取值范围为 $[\underline{z}, \overline{z}]$ 的整数，并且服从某个概率分布（distribution ），而在另一个信封内存入$mZ$ 的钱。然后，你以等概率从两个信封中随机地抽取一个信封，看里边的钱数 $X$ 的值。当 $X$ 的值比 $Z$ 的上限 $\overline{z}$ 大的时候，你可以肯定你拿到的信封里的钱数是比较多的，因此你不必换信封。若你拿到的钱数等于 $\underline{z}$ 的值，那么你可以肯定另一个信封中的钱是比 $\underline{z}$ 多，因此你必须换信封。大致上可以这么说，如果你知道X的值域和X的值的可能性，你就能判断在信封A中的钱数X是相对比较小的还是比较大的，然后相应的做出选择。 从数学上，采用一个准确的概率模型，我们一定能够找到 $X$ 和 $Y$ 的联合概率函数。$X$ 和 $Y$ 的联合概率分可由两个信封中的钱的最小者 $Z$ 的分布律为 $P_Z$，则对一切 $z$，$p_{X,Y}(mz,z)=p_{X,Y}(z,mz)=\frac{1}{2}p_Z(z)$ ，对于不具有 $(mz, z)$ 或 $(z,mz)$ 的形式的 $(x,y)$ ，$p_{X,Y}(x,y)=0$。 当 $p_{X,Y}(x,y)$ 给定以后，我们可以用这个换信封的规则：换信封的充要条件为 $E[Y|X=x]&gt;x$ ，按照这个规则可以确定换或者不换信封。 现在的问题是：按照上诉的模型和转换规则是否可以按照某些x的值，转换信封，而另一些x的值不能换？一般情况下是可以的，例如早先局出的 $Z$ 的值域为有界集合的情况，就可以实现这样的转换规则。然而，下面的一个稍显怪癖的例子，使得你总是换信封： ​抛掷一枚均匀的硬币，直到出现正面为止。记 $N$ 为抛掷硬币的次数。此时你将 $m^N$ 元放进一个信封内，将 $m^{N-1}$ 元放进另一个信封内。令 $X$ 是你打开的那个信封（信封A）内的钱数， $Y$ 是令一个信封（信封 B）内的钱数。现在假定 A 中只有一元钱，显然 B 中含有 $m$元， 你应该换信封。当 A 内含有 $m^n$ 元的时候，B 中或者含有 $m^{n-1}$ 元钱或含有 $m^{n+1}$ 元钱。由于 $N$ 具有几何分布列， 我们有： $$\frac{P(Y=m^{m+1} | X=m^n)}{P(Y=m^{m-1} | X=m^n)} = \frac{P(Y=m^{m+1}, X=m^n)}{P(Y=m^{m-1}, X=m^n)}=\frac{P(N=n+1)}{P(N=n)}=\frac{1}{2}$$ 这样我们有: $$P(Y=m^{m-1}|X=m^n)=\frac{2}{3}，P(Y=m^{m+1}|X=m^n)=\frac{1}{3}$$ $$E\{信封B中的钱数|x=m^n\}=\frac{2}{3}m^{n-1}+\frac{1}{3}m^{n+1}=\frac{2+m^2}{3m}\cdot m^n$$ $\frac{2+m^2}{3m}&gt;1$ 的充要条件是 $m^2-3m + 2 &gt; 0$ 或 $(m-1)(m-2)&gt;0$。 若 $m&gt;2$， 则： $$E[信封 B 中的钱数 | X=m^n]&gt;m^n$$ 这样，为了获得最大的期望奖金，你应该转向信封 $B$。在这个例子中，由于对一切 $x$ 的值， $$E[Y|X=x]&gt;x$$ 你选择B。直观地看，利用全期望定理，应该有结论 $E[Y]&gt;E[X]$ 。然而，由于 $X$ 和 $Y$ 具有相同的概率函数（PMFs，probability mass function (PMF) ），结论$E[Y]&gt;E[X]$ 不可能成立。实际上，我们有： $$E[Y]=E[X]=\infty$$ 这个结论与 $E[Y|X=x]&gt;x, \forall x$ 并不矛盾。当 $E[Y]=E[X]=\infty$ 的情况下，利用关系式 $E[Y|X=x] &gt; x$ 而转换信封并不能够改进平均奖金。从而解决了悖论问题。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[经典摘录-几何随机变量的均值和方差]]></title>
    <url>%2F2017%2F08%2F23%2Fmean_and_variance_of_the_geometric%2F</url>
    <content type="text"><![CDATA[本文摘录自 Introduction to probability, 2nd Edition Example 2.17 mean_and_variance_of_the_geometric 你一次又一次地写一个计算机软件，每写一次都有一个成功的概率 $p$ 。假设每次成功与否与以前的历史记录相互独立。令 $X$ 是你一直到成功为止所写的次数（最后一次你成功了！） $X$ 的期望和方差是多少？ 由于 $X$ 是一个几何随机变量，那么我们视 $X$ 为几何随机变量，概率质量函数是： $$p_X(k)=(1-p)^{k-1}p, k = 1, 2, ….$$ 那么 $X$ 的方差和均值为： $$E[X] = \sum\limits_{k=1}^{\infty}k(1-p)^{k-1}p, var(X)=\sum\limits_{k=1}^{\infty}(k-E[X])^2(1-p)^{k-1}p$$ 但是衡量这些无限和有点麻烦。我们利用全期望定理进行计算。记 $A_1=\{X=1\}=\{\text{first try is a success}\}, A_2=\{X&gt;1\}=\{\text{first try is a failure}\}$。 如果第一次就成功，得到 $X=1​$ ，且 $$E[X|X=1]=\sum\limits_{}^{}xp_{X|X=1}=1p_{1|X=1}=1$$ 如果首次尝试失败 ( X &gt; 1)，我们将浪费一次尝试，我们重新开始，由于是在第一次失败的条件下，那么表示尝试次数的 $X$ 的均值一定是大于1的，剩余尝试的期望即 $E[X]$ 。 $$E[X|X&gt;1] = E[X+1] = 1+E[X]$$ 因此，由全期望定理： $$\begin{eqnarray}E[X] &amp;=&amp; P[X=1]E[X|X=1]+P(X&gt;1)E[X|X&gt;1] \\&amp;=&amp; p + (1 - p) (1+E[X])\end{eqnarray}$$从而可以得到： $$E[X]=\frac{1}{p}$$ 相似的推理，我们也得到 $$E[X^2|X=1]=1,\quad E[X^2|X&gt;1]=E[(1+X)^2]=1+2E[X]+E[X^2]$$ 因此， $$E[X^2]=p+(1-p)(1+2E[X]+E[X^2])$$ 联合 $E[X]=\frac{1}{p}$ 得到： $$E[X^2]=\frac{2}{p^2}-\frac{1}{p}$$ 总结： $$var(X)=E[X^2]-(E[X])^2= \frac{2}{p^2}-\frac{1}{p}-\frac{1}{p^2}=\frac{1-p}{p^2}$$]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[转载-LaTeX各种符号]]></title>
    <url>%2F2017%2F08%2F20%2FLaTeX_symbols%2F</url>
    <content type="text"><![CDATA[根据左侧文章目录，快速定位想要的符号 声调 语法 效果 语法 效果 语法 效果 \bar{x} \acute{\eta} \check{\alpha} \grave{\eta} \breve{a} \ddot{y} \dot{x} \hat{\alpha} \tilde{\iota} 函数 语法 效果 语法 效果 \sin\theta \cos\theta \arcsin\frac{L}{r} \arccos\frac{T}{r} \sinh g \cosh h \operatorname{sh}j \operatorname{argsh}k \operatorname{argch}l \operatorname{th}i k’(x)=\lim_{\Delta x\to 0}\frac{k(x)-k(x-\Delta x)}{\Deltax} \limsup S \max H \min L \sup t \exp!t \lg X \log X \ker x \deg x \Pr x \det x \arg x \dim x \tan\theta \arctan\frac{L}{T} \tanh i \operatorname{ch}h \operatorname{argth}m \liminf I \inf s \ln X \log_\alpha X \gcd(T,U,V,W,X) \hom x \lim_{t\to n}T 同余 语法 效果 语法 效果 \pmod{m} a \bmod b 微分 语法 效果 语法 效果 语法 效果 \nabla \partial x \mathrm{d}x \dot x \ddot y 集合 语法 效果 语法 效果 语法 效果 语法 效果 语法 效果 \forall \exists \empty \emptyset \varnothing \in \ni \not\in \notin \subset \subseteq \supset \supseteq \cap \bigcap \cup \bigcup \biguplus \sqsubset \sqsubseteq \sqsupset \sqsupseteq \sqcap \sqcup \bigsqcup 逻辑 语法 效果 语法 效果 语法 效果 语法 效果 p \land \wedge \bigwedge \bar{q} \to p \lor \vee \bigvee \lnot \neg q \setminus \smallsetminus 根号 语法 效果 语法 效果 \sqrt{3} \sqrt[n]{3} 关系符号 语法 效果 \Delta ABC\sim\Delta XYZ \sqrt{3}\approx1.732050808\ldots \simeq \cong \dot= \ggg \gg &gt; \ge \geqq = \leq \leqq &lt; \ll \lll (x-y)^2\equiv(-x+y)^2\equiv x^2-2xy+y^2 x\not\equiv N x\ne A x\neq C t\propto v \pm \mp 因为所以123456789101112\begin&#123;align&#125;\because\begin&#123;cases&#125;\acute&#123;a&#125;x^2+bx^2+c\gtrless0\gtrless\grave&#123;a&#125;x^2+bx^2+c\\\acute&#123;a&#125;&gt;0&gt;\grave&#123;a&#125;\end&#123;cases&#125;\\\therefore\frac&#123;-b\pm\sqrt&#123;b^2-4\acute&#123;a&#125;c&#125;&#125;&#123;2\acute&#123;a&#125;&#125;&#123;&#125;_\lessgtr^\gtrlessx_\lessgtr^\gtrless\frac&#123;-b\pm\sqrt&#123;b^2-4\grave&#123;a&#125;c&#125;&#125;&#123;2\grave&#123;a&#125;&#125;\end&#123;align&#125; ​ 几何符号 特征 语法 效果 菱形 \Diamond 正方形 \Box 三角形 Delta \Delta 图型 \triangle 角名 \angle\Alpha\Beta\Gamma 角度 \sin\!\frac{\pi}{3}=\sin60^\operatorname{\omicron}=\frac{\sqrt{3}}{2} 垂直 \perp 箭头符号 语法 效果 语法 效果 语法 效果 \leftarrow \gets \rightarrow \to \leftrightarrow \longleftarrow \longrightarrow \mapsto \longmapsto \hookrightarrow \hookleftarrow \nearrow \searrow \swarrow \nwarrow \uparrow \downarrow \updownarrow 语法 效果 语法 效果 语法 效果 语法 效果 \rightharpoonup \rightharpoondown \leftharpoonup \leftharpoondown \upharpoonleft \upharpoonright \downharpoonleft \downharpoonright 语法 效果 语法 效果 语法 效果 \Leftarrow \Rightarrow \Leftrightarrow \Longleftarrow \Longrightarrow \Longleftrightarrow (or \iff) \Uparrow \Downarrow \Updownarrow 特殊符号 语法 效果 语法 效果 语法 效果 语法 效果 语法 效果 语法 效果 \eth \S \P \% \dagger \ddagger \star * \ldots \smile \frown \wr 语法 效果 语法 效果 语法 效果 \oplus \bigoplus \otimes \bigotimes \times \cdot \div \circ \bullet \bigodot \boxtimes \boxplus 语法 效果 语法 效果 语法 效果 语法 效果 \triangleleft \triangleright \infty \bot \top \vdash \vDash \Vdash \models \lVert \rVert 语法 效果 语法 效果 语法 效果 \imath \hbar \ell \mho \Finv \Re \Im \wp \complement 语法 效果 语法 效果 语法 效果 语法 效果 \diamondsuit \heartsuit \clubsuit \spadesuit \Game \flat \natural \sharp 分数、矩阵和多行列式 上标、下标及积分等 功能 语法 效果 上标 a^2 下标 a_2 组合 a^{2+2} a_{i,j} 结合上下标 x_2^3 前置上下标 {}_1^2\!X_3^4 导数（HTML） x&#39; 导数（PNG） x^\prime 导数（错误） x\prime 导数点 \dot{x} \ddot{y} 向量 \vec{c} \overleftarrow{a b} \overrightarrow{c d} \widehat{e f g} 上弧(注: 正确应该用 \overarc, 但在这里行不通。要用建议的语法作为解决办法) \overset{\frown} {AB} 上划线 \overline{h i j} 下划线 \underline{k l m} 上括号 \overbrace{1+2+\cdots+100} \begin{matrix} 5050 \\ \overbrace{ 1+2+\cdots+100 }\end{matrix} 下括号 \underbrace{a+b+\cdots+z} \begin{matrix} \underbrace{ a+b+\cdots+z } \\ 26\end{matrix} 求和 \sum_{k=1}^N k^2 \begin{matrix} \sum_{k=1}^N k^2 \end{matrix} 求积 \prod_{i=1}^N x_i \begin{matrix} \prod_{i=1}^N x_i \end{matrix} 上积 \coprod_{i=1}^N x_i \begin{matrix} \coprod_{i=1}^N x_i\end{matrix} 极限 \lim_{n \to \infty}x_n \begin{matrix} \lim_{n \to \infty}x_n\end{matrix} 积分 \int_{-N}^{N} e^x\, dx \begin{matrix} \int_{-N}^{N} e^x\, dx\end{matrix} 双重积分 \iint_{D}^{W} \, dx\,dy 三重积分 \iiint_{E}^{V} \, dx\,dy\,dz 四重积分 \iiiint_{F}^{U} \, dx\,dy\,dz\,dt 闭合的曲线、曲面积分 \oint_{C} x^3\, dx + 4y^2\, dy 交集 \bigcap_1^{n} p 并集 \bigcup_1^{k} p 字体希腊字母斜体小写希腊字母一般用于在方程中显示变量。 正体希腊字母 特征 语法 效果 注释/外部链接 大写字母 \Alpha \Beta \Gamma \Delta \Epsilon \Zeta \Eta\Theta ΑΒ Γ ΔΕ Ζ ΗΘ \Iota \Kappa \Lambda \Mu \Nu \Xi \Omicron \Pi ΙΚ Λ ΜΝ Ξ ΟΠ \Rho \Sigma \Tau \Upsilon \Phi \Chi \Psi\Omega ΡΣ Τ ΥΦ Χ ΨΩ 小写字母 \alpha \beta \gamma \delta \epsilon \zeta \eta\theta \iota \kappa\varkappa \lambda \mu \nu \xi \omicron\pi \rho \sigma \tau \upsilon \phi \chi \psi\omega 异体字母 \Epsilon\epsilon\varepsilon \Theta\theta\vartheta \Kappa\kappa\varkappa \Pi\pi\varpi \Rho\rho\varrho \Sigma\sigma\varsigma \Phi\phi\varphi 已停用字母 \digamma Ϝ[1] 粗体希腊字母 特征 语法 效果 大写字母 \boldsymbol{\Alpha \Beta \Gamma \Delta \Epsilon \Zeta\Eta \Theta} \boldsymbol{\Iota \Kappa \Lambda \Mu \Nu \Xi \Omicron\Pi} \boldsymbol{\Rho \Sigma \Tau \Upsilon \Phi \Chi \Psi\Omega} 小写字母 \boldsymbol{\alpha \beta \gamma \delta \epsilon \zeta\eta \theta} \boldsymbol{\iota \kappa \lambda \mu \nu \xi \omicron\pi} \boldsymbol{\rho \sigma \tau \upsilon \phi \chi \psi\omega} 异体字母 \boldsymbol{\Epsilon\epsilon\varepsilon} \boldsymbol{\Theta\theta\vartheta} \boldsymbol{\Kappa\kappa\varkappa} \boldsymbol{\Pi\pi\varpi} \boldsymbol{\Rho\rho\varrho} \boldsymbol{\Sigma\sigma\varsigma} \boldsymbol{\Phi\phi\varphi} 已停用字母 \boldsymbol{\digamma} 黑板粗体 语法 \mathbb{ABCDEFGHIJKLMNOPQRSTUVWXYZ} 效果 黑板粗体（Blackboardbold）一般用于表示数学和物理学中的向量或集合的符号。 备注： 花括号中只有使用大写拉丁字母才能正常显示，使用小写字母或数字会得到其他符号。 正粗体 语法 \mathbf{012…abc…ABC…} 效果 备注 花括号{}内只能使用拉丁字母和数字，不能使用希腊字母如\alpha等。斜粗体 语法 \boldsymbol{012…abc…ABC…\alpha \beta\gamma…} 效果 备注 使用\boldsymbol{}可以加粗所有合法的符号。 斜体数字 语法 \mathit{0123456789} 效果 罗马体 语法 \mathrm{012…abc…ABC…}或\mbox{}或\operatorname{} 效果 备注 罗马体可以使用数字和拉丁字母。 哥特体 语法 \mathfrak{012…abc…ABC…} 效果 备注 哥特体可以使用数字和拉丁字母。 手写体 语法 \mathcal{ABC…} 效果 备注 手写体仅对大写拉丁字母有效。 希伯来字母 语法 \aleph\beth\gimel\daleth 效果 括号 功能 语法 显示 不好看 ( \frac{1}{2} ) 好看了 \left( \frac{1}{2} \right) 您可以使用 \left 和 \right 来显示不同的括号： 备注： 可以使用 \big, \Big, \bigg, \Bigg 控制括号的大小，比如代码 1\Bigg ( \bigg [ \Big \&#123;\big\langle \left | \| \frac&#123;a&#125;&#123;b&#125; \| \right | \big \rangle\Big\&#125;\bigg ] \Bigg ) ​ 显示︰$$\Bigg ( \bigg [ \Big \{\big\langle \left | | \frac{a}{b} | \right | \big \rangle\Big\}\bigg ] \Bigg )$$ 空格注意TEX能够自动处理大多数的空格，但是您有时候需要自己来控制。 功能 语法 显示 宽度 2个quad空格 \alpha\qquad\beta quad空格 \alpha\quad\beta 大空格 \alpha\ \beta 中等空格 \alpha\;\beta 小空格 \alpha\,\beta 没有空格 \alpha\beta 紧贴 \alpha\!\beta 颜色 语法 字体颜色︰{\color{色调}表达式} 背景颜色︰{\pagecolor{色调}表达式} 支持色调表 ＊注︰输入时第一个字母必需以大写输入，如\color{OliveGreen}。 例子 1&#123;\color&#123;Blue&#125;x^2&#125;+&#123;\color&#123;Brown&#125;2x&#125; -&#123;\color&#123;OliveGreen&#125;1&#125; $${\color{Blue}x^2}+{\color{Brown}2x} -{\color{OliveGreen}1}$$ 1x_&#123;\color&#123;Maroon&#125;1,2&#125;=\frac&#123;-b\pm\sqrt&#123;&#123;\color&#123;Maroon&#125;b^2-4ac&#125;&#125;&#125;&#123;2a&#125; $$x_{\color{Maroon}1,2}=\frac{-b\pm\sqrt{{\color{Maroon}b^2-4ac}}}{2a}$$ 颜色小型数学公式当要把分数等公式放进文字中的时候，我们需要使用小型的数学公式。 苹果原产于欧洲和中亚细亚。哈萨克的阿拉木图与新疆阿力麻里有苹果城的美誉。中国古代的林檎、柰、花红等水果被认为是中国土生苹果品种或与苹果相似的水果。苹果在中国的栽培记录可以追溯至西汉时期，汉武帝时，10的 是2。上林苑中曾栽培林檎和柰，当时多用于薰香衣裳等，亦有置于床头当香熏或置于衣服初作为香囊，总之一般不食用。但也有看法认为，林檎和柰是现在的沙果，曾被误认为苹果，真正意义上的苹果是元朝时期从中亚地区传入中国，当时只有在宫廷才可享用。并不好看。 苹果原产于欧洲和中亚细亚。哈萨克的阿拉木图与新疆阿力麻里有苹果城的美誉。中国古代的林檎、柰、花红等水果被认为是中国土生苹果品种或与苹果相似的水果。苹果在中国的栽培记录可以追溯至西汉时期，汉武帝时，10的 是2。上林苑中曾栽培林檎和柰，当时多用于薰香衣裳等，亦有置于床头当香熏或置于衣服初作为香囊，总之一般不食用。但也有看法认为，林檎和柰是现在的沙果，曾被误认为苹果，真正意义上的苹果是元朝时期从中亚地区传入中国，当时只有在宫廷才可享用。 好看些了。 可以使用 1\begin&#123;smallmatrix&#125;...\end&#123;smallmatrix&#125; 或直接使用 模板。 1&#123;&#123;Smallmath|f= f(x)=5+\frac&#123;1&#125;&#123;5&#125; &#125;&#125; 强制使用PNG假设我们现在需要一个PNG图的数学公式。 若输入 2x=1 的话： 这并不是我们想要的。 若你需要强制输出一个PNG图的数学公式的话，你可于公式的最后加上 \, （小空格，但于公式的最后是不会显示出来）。若输入 2x=1 \,的话：$2x=1 \,$ 是以PNG图输出的。你也可以使用 \,\!，这个亦能强制使用PNG图像。 阅读更多︰Help:Displayinga formula#Forced PNG rendering 本文的 巨人的肩膀 https://blog.csdn.net/garfielder007/article/details/51646604 http://zh.wikipedia.org/wiki/Help:MATH http://blog.csdn.net/anxiaoxi45/article/details/39449445]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Latex</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Trees describe the sample space]]></title>
    <url>%2F2017%2F08%2F15%2Ftrees_describe_the_sample_space%2F</url>
    <content type="text"><![CDATA[This note comes from Introduction to Probability, 2nd Edition Example 1.9 Rada DetectionIf an aircraft is present in a certain area, a radar detects it and generates an alarm signal with probability 0.99. If an aircraf is not present. the radar generates a (false) alarm, with probability 0.10. We assume that an aircraft is present with probability 0.05. What is the probability of no aircraf presence and a false alarm? What is the probability of aircraf presence and no detection? $A$ sequential representation of the experiment is appropriate here, as shown in Fig. 1.9. Let $A$ and $B$ be the events $A = \{an\ aircraft\ is\ present\}$, $B = \{the\ radar\ generates\ an\ alarm\} $, and consider also their complements $A^c=\{an\ aircraft\ is\ not present\}$$，$$B^c=\{the\ radar\ does\ not\ generate\ an\ alarm\}$。 The given probabilities are recorded along the corresponding branches of the tree describing the sample space, as shown in Fig. 1.8. Each event of interest corresponds to a leaf of the tree and its probability is equal to the product of the probabilities associated with the branches in a path from the root to the corresponding leaf. The desired probabilities of false alarm and missed detection are $$P(false\ alarm) = P(A^c ∩ B) = P(A^c)P(B | A^c) = 0.95 · 0.10 = 0.095$$，$$P(missed\ detection) = P(A ∩ B^c) = P(A)P(B^c | A) = 0.05 · 0.01 = 0.0005$$. Extending the preceding example, we have a general rule for calculating various probabilities in conjunction with a tree-based sequential description of an experiment. In particular: (a) We set up the tree so that an event of interest is associated with a leaf. We view the occurrence of the event as a sequence of steps, namely, the traversals of the branches along the path from the root to the leaf. (b) We record the conditional probabilities associated with the branches of the tree. (c) We obtain the probability of a leaf by multiplying the probabilities recorded along the corresponding path of the tree. multiplication ruleIn mathematical terms, we are dealing with an event A which occurs if and only if each one of several events $A_1, . . . , A_n$ has occurred, i.e., $A = A_1 ∩ A_2 ∩ · · · ∩ A_n$. The occurrence of $A$ is viewed as an occurrence of $A_1$, followed by the occurrence of $A_2$, then of $A_3$, etc, and it is visualized as a path on the tree with $n$ branches, corresponding to the events $A_1, . . . , A_n$. The probability of $A$ is given by the following rule (see also Fig. 1.9). The multiplication rule can be verified by writing$$P(\cap^n_{i=1} A_i)=P(A_1)\frac{P(A_1\cap A_2)}{P(A_1)}\frac{P(A_1\cap A_2\cap A_3)}{P(A_1\cap A_2)}\cdots\frac{P(\cap_{i=1}^n A_i)}{P(\cap^{n-1}_{i=1} A_i)}$$, and by using the definition of conditional probability to rewrite the right-hand side above as $$P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2)\cdots P(A_N|\cap^{n-1}_{i=1} A_i)$$. The intersection event $A = A_1∩A_2∩· · ·∩A_n$ is associated with a path on the tree of a sequential description of the experiment. We associate the branches of this path with the events $A_1, . . . , A_n$, and we record next to the branches the corresponding conditional probabilities. The final node of the path corresponds to the intersection event $A$, and its probability is obtained by multiplying the conditional probabilities recorded along the branches of the path $$P(A_1\cap A_2\cap\cdots\cap A_3)=P(A_1)P(A_2|A_1)\cdots P(A_n|A_1\cap A_2\cdots \cap A_{n-1}).$$ Note that any intermediate node along the path also corresponds to some intersection event and its probability is obtained by multiplying the corresponding conditional probabilities up to that node. For example, the event $A_1 ∩ A_2 ∩ A_3$ corresponds to the node shown in the figure, and its probability is $$P(A_1\cap A_2\cap A_3)=P(A_1)P(A_2|A_1)P(A_3|A_1\cap A_2).$$ For the case of just two events, A1 and A2, the multiplication rule is simply the definition of conditional probability.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[期望定义的由来]]></title>
    <url>%2F2017%2F08%2F14%2Fdefinition_of_expectation%2F</url>
    <content type="text"><![CDATA[前言我们很早就学到某个随机变量$X$的期望就是$X$的所有取值相对于它的概率的加权平均， 但是这是为什么呢？很多人都有疑问，后来看了MIT教授写的 Introduction to Probability, 2nd Edition 书，豁然开朗，以此小计一篇。 例子我们先以一个例子入手：假设你有机会转动一个幸运轮许多次，每次转动后幸运轮都会出现一个数字（数字即奖金数），不妨设为$m_i, i$表示第$i$次转动幸运轮，而且这些数字出现的概率分别为$p_i$，那么每次你期望得到的奖金数是多少呢？此处“每次”和”期望“都是一些不确定的词汇，我们来一一明确它们的含义。 假设一共转动幸运轮$k$次，而其中有$k_i$次转动的结果为$m_i$。你所得到的总钱数为：$\sum\limits_{i=1}^{n}m_i k_i$，那么每次转动的钱数为$M=\frac{\sum\limits_{i=1}^{n}{m_i k_i}}{k}$，现在假设$k$是一个很大的数字，那么我们可以假设概率与频率相互接近。即： $$\frac{k_i}{k}\approx p_i, i=1,\ldots,n$$ 这样你每次转动幸运轮所期望得到的钱数是： $$M=\frac{\sum\limits_{i=1}^{n}m_i k_i}{k}\approx \sum\limits_{i=1}^{n}m_i p_i$$ 有这个例子启发，才有了下面的定义。 期望的定义设随机变量$X$的概率函数是$p_X$，那么$X$的期望值（也称期望或均值）为： $$E[X]=\sum\limits_{x}xp_X(x)$$ 虽然内容较为简单，但是用频率接近概率进而引进概率的定义是很常见的思路，有了这个过程我们对期望才有了很直观的理解。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>probability</tag>
        <tag>概率论</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[复数与复矩阵]]></title>
    <url>%2F2017%2F08%2F12%2Fcomplex_and_complex_matrix%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第12讲：复数与复矩阵 之前接触的大部分线性代数知识都只考虑实数情形，但复数情形不可避免会遇到。例如$\begin{pmatrix}cos\theta&amp;-sin\theta\\sin\theta&amp;cos\theta\end{pmatrix}$没有实特征值（除了极特殊情形），目的：比较实数和复数情形的异同，注意学习复数和实数的区别联系。 复数复习 $i^2=-1$， 一个复数$a+bi=z$，$a$是实部(real part)，$b$是虚部(imaginary part)，可以把实部$a$看成x轴分量，虚部$b$看成y轴分量。复数的共轭(complex conjugate) $z=a+bi\rightarrow \bar{z}=a-bi$，长度 $|z|=\sqrt{a^2+b^2}=(a-bi)(a+bi)=z\bar{z}$（$z$的长度不能定义为$\sqrt{(a+bi)^2}$，长度必须是正值，如果把复数$z$看成一个2维向量，那么它的长度显然就是定义中给出的）， 矩阵的共轭定义为： $A=(a_{ij})_{n\times n}, a_{ij}\in C \rightarrow \bar{A}=(\overline{a_{ij}})_{n\times n}$，性质：$\overline{AB}=\bar{A}\bar{B}\ z\bar{z}=|z|^2$。 {长度为1（单位圆上）的复数}$\rightarrow${二阶旋转矩阵}，且保持乘法。$z=cos\theta+isin\theta\rightarrow A_2=\begin{pmatrix}cos\theta&amp;-sin\theta\\sin\theta&amp;cos\theta\end{pmatrix}$。验证性质：$z_1=e^{i\theta_1},z_2=e^{i\theta_2}\rightarrow A_{z_1}=\begin{pmatrix}cos\theta&amp;-sin\theta\\sin\theta&amp;cos\theta\end{pmatrix}, A_{z_2}=\begin{pmatrix}cos\theta&amp;-sin\theta\\sin\theta&amp;cos\theta\end{pmatrix}\\\rightarrow z_1z_2=e^{i(\theta_1+\theta_2)}=\begin{pmatrix}cos(\theta_1+\theta_2)&amp;-sin(\theta_1+\theta_2)\\sin(\theta_1+\theta_2)&amp;cos(\theta_1+\theta_2)\end{pmatrix}=A_{z_1z_2}$ 欧拉公式(Euler formula) ：$e^{i\theta}=cos\theta+isin\theta$，极分解(polar decomposition)： $z=re^{i\theta}=r(cos\theta+isin\theta)\rightarrow z^n=r^ne^{in\theta}=r^n(cos(n\theta)+isin(n\theta)) $，这里z的公式中三角函数部分长度为1，所以r即z的长度，这样任何一个复数都可以用$re^{i\theta}$表示。 单位根$x^n=1$有n个复根$e^{2k\pi i\over n}, k=0,1,2,\ldots,n-1$，令$\omega=e^{2\pi i\over n}\rightarrow 1+\omega+\omega^2+\cdots+\omega^{n-1}=0$，例如：求$(1+i)^8\leftarrow1+i=\sqrt{2}e^{i{\pi\over 4}}, (1+i)^8={(\sqrt{2})}^8e^{i2\pi}=16$。$\frac{x^{2n+1}-1}{x-1}$。 代数基本定理：$a_nx^n+\cdots+a_1x+a_0=0, a_i\in C$有n个复数根(可能重复)，设$a_i\in R, a_nx^n+\cdots+a_1x+a_0=0$ 的非实数的复根也是成对出现，即若$z=a+bi(b\ne0)$是它的根，则$\bar{z}=a-bi$也是它的根，复数根是成对出现的。$\Rightarrow$ 奇次实系数方程总有一个实根。（注：公开课字幕内容如下：因为我们知道复根是成对出现的，所以对一个实系数方程，它的复根实际上是2的倍数，因为它是成对出现的，但是奇数次实系数呢，所以它必然除了复根应该有一个实根，不然的话它只有偶数的根，这样就跟它奇数次矛盾）。 实系数多项式（次数$\ge 1$）的$f(x)$可分解成$f(x)=a(x-\lambda_1)^{n_1}\cdots(x-\lambda_s)^{n_s}(x^2-b_1x+c)^{e_1}\cdots(x^2-b_tx+c)^{e_{t}}$，$\lambda_i$即实数根，后$t$项即复数根给出来的，后面这种形式无法写成实根的一次形式，也就是它的判别式小于0（有复数根），不能写成前$s$项的形式。例如：$x^m-1=\prod\limits_{k=0}^{m-1}(x-\omega_k), \omega_k=e^{i2k\pi\over m}$$\omega_{m-k}=e^{i2(m-k)\pi\over m}=e^{i{2\pi(1-{k\over m})}}=cos(2\pi(1-{k\over m}))+isin(2\pi(1-{k\over m}))=cos({2k\pi\over m})-isin({2k\pi\over m})=\overline{\omega_k}, \\ {k\over m} &lt;1\Rightarrow (x-\omega_k)(x-\omega_{m-k})=x^2-(\omega_k+\omega_{m-k})x+(\omega_k\omega_{m-k})=x^2-2cos({2k\pi\over m}x)+1$， 同理可得：$x^m+1=\prod\limits_{k=0}^{m-1}(x-\xi_k), \xi_k=e^{i(\pi+2k\pi)\over m}$ 。 例题：证明$cos{\pi\over 2n+1}cos{2\pi\over 2n+1}\cdots cos{n\pi\over 2n+1}={1\over 2^n}$要证明这个需要以下3点： (1)$-1-e^{i2\theta}=-1-cos2\theta-isin2\theta=-2cos\theta(cos\theta+isin\theta)\Rightarrow |-1-cos2\theta-isin2\theta|=2|cos\theta|$ (2)设$\omega=cos{2\pi\over 2n+1}+isin{2\pi\over 2n+1}=e^{i2{\pi\over 2n+1}}\Rightarrow|-1-\omega|=2|cos({\pi\over {2n+1}})|$，那么$x^{2n}+x^{2n-1}+\cdots+1=(x-\omega)(x-\omega^2)\cdots(x-\omega^{2n})\quad (*)$ 推导如下：${x^{2n+1}-1}=(x-1)(x-\omega)(x-\omega^2)\cdots(x-\omega^{2n})\Rightarrow \frac{x^{2n+1}-1}{x-1}=(x-\omega)(x-\omega^2)\cdots(x-\omega^{2n})\\\Rightarrow {1(1-x^{2n+1})\over {1-x}}=(x-\omega)(x-\omega^2)\cdots(x-\omega^{2n})$ (3)$cos{(2n+1-k)\pi\over 2n+1}=cos{k\pi\over 2n+1}$令$(*)$等式中$x=-1$，且取两边长度$1=|(-1-\omega)(-1-\omega^2)\cdots(-1-\omega^{2n})$中右边每一项利用(1)式子得到$|-1-\omega|=2|cos{\pi\over {2n+1}}|,\\ |-1-\omega^2|=2|cos{2\pi\over {2n+1}}|,\\\ldots\|-1-\omega^n|=2|cos{n\pi\over {2n+1}}|$ 从n+1项起根据(3)得： $$|-1-\omega^{n+1}|=2|cos{(n+1)\pi\over {2n+1}}|=2|cos{(2n+1-n)\pi\over {2n+1}}|=2|cos(\pi-{n\pi\over {2n+1}})|=2|cos({n\pi\over {2n+1}})|=|-1-\omega^n|$$ $$|-1-\omega^{n+2}|=2|cos{(n+2)\pi\over {2n+1}}|=2|cos{[(2n+1)-(n-1)]\pi\over {2n+1}}|=2|cos(\pi-{(n-1)\pi\over {2n+1}})|=2|cos{(n-1)\pi\over {2n+1}}|=|-1-\omega^{n-1}|$$$$\cdots\cdots$$ $|-1-\omega^{2n}|=2|cos{2n\pi\over {2n+1}}|=2|cos{(2n+1-1)\pi\over {2n+1}}|=2|cos(\pi-{\pi\over {2n+1}})|=2|cos({\pi\over {2n+1}})|=|-1-\omega|$ 复矩阵Hermitian矩阵复数矩阵$A=(a_{ij})_{m\times n},a_{ij}\in C$, 那么称$\overline{A^T}(=\bar{A}^T)$ 为 Hermitian 矩阵，记为$A^H$。例如： $Z=\begin{pmatrix}1+i\\i\end{pmatrix}\rightarrow Z^H=\begin{pmatrix}1-i&amp;-i\end{pmatrix}$，而且发现$ZZ^H=||Z||^2$，这个可以类比实数中的$x^Tx=||x||^2$。性质：$(A^H)H=A, (AB)^H=B^HA^H$（按照共轭转置即可求得），正如在$R^n$的定义内积，在$C$上也可以定义内积：$u,v\in C^n, u^Hv=(\bar{u}_1\cdots\bar{u}_n)\begin{pmatrix}v_1\\\vdots\\v_n\end{pmatrix}=\bar{u}_1v_1+\cdots+\bar{u}_nv_n$，内积的性质：$u^Hv=\overline{v^Hu}$。 厄米特Hermite矩阵在实数矩阵中有对称矩阵的概念和作用，复数矩阵有类似的——厄米特矩阵(Hermite matrix)，定义为：$A=A^H$，即一个矩阵的共轭转置等于它本身，那么称这种矩阵为Hermite阵。例：$\begin{pmatrix}2&amp;1+i\\1-i&amp;3\end{pmatrix}$。 性质1：Hermite阵对角线元素为实数。 性质2：$z\in C, A=A^H\Rightarrow z^HAz$ 是一个实数。证明如下：${\overline{z^HAz}}^T=(z^HAz)^H=z^HA^Hz=z^HAz$ 性质3：设$A,B$是Hermite阵，则$A+B$也是，证明：$(A+B)^H=A^H+B^H=A+B$。进一步，若$AB=BA$（即乘法可交换的时候），则$AB$是Hermite阵。$\Rightarrow A^n$是Hermite阵。 性质4：设$A$是一个$n$阶复矩阵，$AA^H, A+A^H$是Hermite阵，联系对比实对称矩阵的$AA^T, A^TA, A+A^T$。 性质5：一个Hermite矩阵A的特征值是实数。证明：设$Az=\lambda_0z$，则$z^HAz=\lambda_0z^Hz$。$z^HAz$和$z^Hz$均为实数$\Rightarrow \lambda_0 (z_0\ne 0)$是实数。 性质6：一个Hermite阵的不同特征值的特征向量相互正交。证明：设$(1) Az_1=\lambda_1z_1, (2) Az_2=\lambda_2z_2, \lambda_1 \ne \lambda_2$， 在(1)两边同乘以$z_2^H$得：$(3)z_2^HAz_1=z_2^H\lambda_1z_1 \Rightarrow (4)z_2^HA^Hz_1=(Az_2)^Hz_1=\overline{\lambda_2}z_2^Hz_1=\lambda_2z_2^Hz_1$，由$(3)(4)\Rightarrow \lambda_1z_2^Hz_1=\lambda_2z_2^Hz_1\Rightarrow (\lambda_1-\lambda_2)z_2^Hz_1=0$，因为$\lambda_1\ne \lambda_2$得：$z_2^Hz_1=0$。 酉unitary矩阵酉矩阵是正交阵的复数类比。$U_{n\times n}$是酉矩阵$\Leftrightarrow$ $\forall z\in C^n, ||Uz||=||z||$，证明：$U^HU=I_n\Rightarrow |U z|^2=z^HU^HUz = z^Hz=|z|^2\Rightarrow |Uz|=|z|\Rightarrow |\lambda|=1$ 。得出与实数矩阵类似的性质1：酉矩阵乘以任何向量不改变它的模长。性质2：$U$是酉矩阵，则$U$的特征值模长为1。 例：$u=\begin{pmatrix}{1\over \sqrt{2}}&amp;-\frac{1}{\sqrt{6}}&amp;\frac{1-i\sqrt{3}}{2\sqrt{3}}\\\frac{1}{\sqrt{2}}&amp;\frac{1}{\sqrt{6}}&amp;{-1+i\sqrt{3}\over 2\sqrt{3}}\\0&amp;{1+i\sqrt{3}\over \sqrt{6}}&amp;{1\over \sqrt{3}}\end{pmatrix}$ ，$|det U|=\prod{|\lambda_i|}=1$ (行列式的长度等于特征值长度的乘积)。 而实数的正交阵，也有类似的性质。下面证明正交阵不同特征值对应的特征向量相互正交： 因为$Q$正交阵,$Q^TQ=E,|Q|=1=λ_1λ_2\ldotsλ_n$,设$λ_1,λ_2$为$Q$的两个不同的特征值,$ξ_1,ξ_2$为对应的特征向量$ (1)Qξ_1=λ_1ξ_1, (2)Qξ_2=λ_2ξ_2,(3)(ξ_2)^T Q^T=λ_2(ξ_2)^T \Rightarrow (3)(1)\Rightarrow ξ_2^TQ^TQξ_1=λ_1λ_2ξ_2^Tξ_1\Rightarrow \(λ_1λ_2-1)ξ_2^Tξ_1=0$而$|λ_1|=|λ_2|=1,λ_1≠λ_2$,得$ξ2^Tξ1=0,因此ξ_2,ξ_1$正交。 复正规阵酉阵和Hermite矩阵均为复正规矩阵，即：$A^HA=AA^H$。 酉相似：设$A,B$是；两$n$阶复矩阵，若存在酉矩阵$U$，使得$A=U^HBU$，则$A$和$B$是酉相似（联系实数矩阵的正交相似）。定理：设$A$复正规阵，则 向量$u$是$A$的关于$\lambda$的特征向量$\Leftrightarrow u$是$A^H$的关于$\bar{\lambda}$的特征向量。证明：设$Au=\lambda u\Rightarrow (A-\lambda I)u=0$令$B=A-\lambda I\Rightarrow ||B^Hu||^2=u^HBB^Hu=u^HB^HBu=||Bu||^2=0$，因为$||B^Hu||^2=0\Rightarrow B^Hu=0, (A-\lambda I)^H=B^H\Rightarrow (A^H-\bar{\lambda}I)u=0\Rightarrow A^Hu=\bar{\lambda}u$ 不同特征值的特征向量正交。证明与Hermite矩阵一样。 定理(Schur)：任意一个复矩阵$A$酉相似于一个上三角阵。即：$\exists\ U\in $ unitary matrix,$\forall\ A\in$ complex matrix, $U^H=U^{-1}, U^HAU=\begin{pmatrix}\lambda_1&amp;*&amp;* \\0&amp;\ddots&amp;*\\0&amp;0&amp;\lambda_n\end{pmatrix} \Rightarrow$任意一个复正规阵酉相似于对角阵，特别地，酉相似于$\begin{pmatrix}1\\&amp;1\\&amp;&amp;\ddots\\&amp;&amp;&amp;1\end{pmatrix}$, $U^HAU=diag(\lambda_1,\ldots,\lambda_n)\Rightarrow AU=\lambda U$。 一个实矩阵$A$是正规的$\Leftrightarrow A^TA=AA^T$。例如，$A$是正交阵或者$A$是对称（反对称）矩阵。 如果$A$是正规的，那么存在正交阵$\Omega$使得： $\Omega^TA\Omega=\begin{pmatrix}\begin{pmatrix}a_1&amp;b_1 \\ -b_1&amp;a_1\end{pmatrix}\\&amp;\ddots\\&amp;&amp;\begin{pmatrix}a_s&amp;b_s \\ -b_s&amp;a_s\end{pmatrix}\\&amp;&amp;&amp;\lambda_{2s+1}\\&amp;&amp;&amp;&amp;\ddots\\&amp;&amp;&amp;&amp;&amp;\lambda_n\end{pmatrix}$，即实正规阵正交相似于分块对角阵。 对于复正规阵酉相似对角阵$U^HAU=diag(\lambda_1,\ldots,\lambda_n)\Rightarrow AU=\lambda U$，这里如果把$U$的列向量写成$u_k=\beta+i\gamma,\ \ k\in [1,n],\ \beta,\gamma \in R_n$，例如：$\begin{pmatrix}1+i\\1-i\end{pmatrix}=\begin{pmatrix}1\\1\end{pmatrix}+i\begin{pmatrix}1 \\ -1\end{pmatrix}$。 $Au_k=\lambda_ku_k\Rightarrow A(\beta+i\gamma)=\lambda_k(\beta+i\gamma)$，令$\lambda_k=a+ib$，得：$A\beta=a\beta-b\gamma, A\gamma=b\beta+a\gamma\Rightarrow$$A(\beta, \gamma)=(\beta,\gamma)\begin{pmatrix}a&amp;b \\ -b&amp;a\end{pmatrix}$ ，所以$\Omega$的实际上是由$U$的特征向量的实部和虚部组成的这样一个形式。 $\Omega$是一个正交阵，那$\beta$和$\gamma$是不是正交的？它们的长度相等嘛？不然无法保证$\Omega$是一个正交阵。 结论：设$A$是$n$解实正交阵。若$\lambda=a+ib(b\ne 0)$是$A$的特征值，$x=x_1+ix_2,\ x_1,x_2\in R_n$是对应的特征向量，则$||x_1||=||x_2||$，且$x_1,x_2$是相互正交的。 证明：如果$\lambda=a+ib$ 是$A$的特征值，那么$\lambda=a-ib$ 也是$A$的特征值。因为$A$实正交阵，所以对$Ax=\lambda x$取两边共轭得：$\overline{Ax}=A\bar{x}=\bar{\lambda}\bar{x}$。那么得到$\lambda,\bar{\lambda}$都是$A$的特征值，由于正交阵不同特征值对应的特征向量正交，所以${\bar{x}}^Hx=0, x=x_1+ix_2, \bar{x}=x_1-ix_2\Rightarrow ||x_1||=||x_2||, x_1^Tx_2=0$。 例2：证明：$\begin{pmatrix}cos\theta&amp;-sin\theta\\sin\theta&amp;cos\theta\end{pmatrix}​$和$\begin{pmatrix}e^{i\theta}&amp;0\\0&amp;e^{-i\theta}\end{pmatrix}​$ 酉相似。$U={1\over \sqrt{2}}\begin{pmatrix}i&amp;1\\1&amp;i\end{pmatrix}​$ 例3：设$A$是Hermite阵，则$I+iA$是非奇异的。由于A的特征值是实数，那么$I+iA$特征值的是$\lambda i+1$不可能是0，行列式就不可能是0，因此是非奇异的。如果A是Hermite阵，那么$U=(I-iA)(I+iA)^{-1}$是酉阵，验证$U^H=(I-iA)^{-1}(I+iA)=(I+iA)(I-iA)^{-1}$（注：分块是相同的矩阵是可交换即变成分块对角阵），这个是用来通过实对称阵或Hermite阵构造酉矩阵。 离散傅里叶变换DFT回忆若$f(x), f’(x)$是piecewise连续的且$f(x+L)=f(x)$， 则$f(x)=a_0+\sum(a_ncos({2\pi nx\over L})+b_nsin({2\pi nx \over L})), a_n={2\over L}\int_{0}^{L}f(x)cos{2\pi nx \over L}dx,\ b_n={2\over L}\int_{0}^{L}f(x)sin{2\pi nx \over L}dx$， 令$V=\{f(x)|f(x)\text{如上条件}\}\rightarrow R^{\infty}$$f(x)\rightarrow (a_0, a_1, b_1, a_2, b_2,\ldots)$这是一个线性映射，$(a_0, a_1,b_1,\ldots)$是$f(x)$的逆傅里叶变换。 当通过$f(x)$求系数$a_i,b_i,\ldots$即傅里叶变换，当通过系数$a_i,b_i,\ldots$求$f(x)$即逆傅里叶变换。 由前文分析得到傅里叶级数的复形式是$F=\sum\limits_{k=-\infty}^{\infty}c_ke^{ikx}, c_k={1\over 2\pi}\int_{-\pi}^{\pi}f(x)e^{-ikx}dx$，通过变量代换：$x={2\pi \over L}t$ 得：$c_k={1\over L}\int_{-{L\over 2}}^{L\over 2}f(t)e^{-i{2\pi k\over L}t}dt, f(t)=\sum\limits_{k=-\infty}^{+\infty}c_ke^{-i{2\pi k\over L}t}$令$n=k$，则得到新的傅里叶级数复数形式：$f(t)=\sum\limits_{n=-\infty}^{+\infty}c_ne^{-i{2\pi n\over L}t}, c_n={1\over L}\int_{-{L\over 2}}^{L\over 2}f(t)e^{-i{2\pi n\over L}t}dt\quad (1)$令$\omega_n={2\pi n\over L}$得到傅里叶级数的频率形式：$\hat{f}(\omega)=\int_{-\infty}^{+\infty}f(t)e^{i\omega_nt}dt\quad (2)$ 对(1)(2)进行离散化：$f(t_j)=\sum\limits_{k=-\infty}^{+\infty}c_ke^{-i{2\pi k\over L}t_j}$令 $\ t_j={jL\over N}$则得到：$f(t_j)\approx \sum\limits_{k=0}^{N-1}c_ke^{i{2\pi kj\over N}}, c_k={1\over L}\int_{-{L\over 2}}^{L\over 2}f(t_j)e^{i{2\pi kj\over N}}dt_j\quad (1*)$，然后再设置$A_j=f(t_j)，a_k=c_k$得到：$f(t)\rightarrow (A_0,A_1,\cdots, A_{N-1}), (c_k)\rightarrow (a_0,a_1,\cdots, a_{N-1})$。 由上可举N=4的例子：$A_0=f(t_0)=a_{0}e^{i2\pi 00\over 4}+a_1e^{i2\pi 10\over 4}+a_2e^{i2\pi 20\over 4}+a_3e^{i2\pi 30\over 4}=a_{0}+a_1+a_2+a_3=1a_{0}+1a_1+1a_2+1a_3$$A_1=f(t_1)=a_{0}e^{i2\pi 01\over 4}+a_1e^{i2\pi 11\over 4}+a_2e^{i2\pi 21\over 4}+a_3e^{i2\pi 31\over 4}= a_{0}+ia_1-a_2-ia_3=1a_{0}+ia_1+i^2a_2+i^3a_3$$A_2=f(t_2)=a_{0}e^{i2\pi 02\over 4}+a_1e^{i2\pi 12\over 4}+a_2e^{i2\pi22\over 4}+a_3e^{i2\pi 32\over 4} = a_{0}-a_1+a_2-a_3 = 1a_{0}+i^2a_1+i^4a_2+i^6a_3$$A_3=f(t_3)=a_{0}e^{i2\pi 03\over 4}+a_1e^{i2\pi 13\over 4}+a_2e^{i2\pi 23\over 4}+a_3e^{i2\pi 33\over 4}=a_{0}-ia_1-a_2+ia_3=1a_{0}+i^3a_1+i^6a_2+i^9a_3$写成矩阵形式：$\begin{pmatrix}A_0\\A_1\\A_2\\A_3\end{pmatrix}=\begin{pmatrix}1&amp;1&amp;1&amp;1\\1&amp;i&amp;i^2&amp;i^3\\1&amp;i^2&amp;i^4&amp;i^6\\1&amp;i^3&amp;i^6&amp;i^9\end{pmatrix}\begin{pmatrix}a_0\\a_1\\a_2\\a_3\end{pmatrix}$设$F=\begin{pmatrix}1&amp;1&amp;1&amp;1\\1&amp;i&amp;i^2&amp;i^3\\1&amp;i^2&amp;i^4&amp;i^6\\1&amp;i^3&amp;i^6&amp;i^9\end{pmatrix}$，令s表示第s行，t表示第t列，则F的第s行第t列元素为$F_{s,t}=i^{(s-1)(t-1)}$，其实上文中的记号j刚好可以视为行数，k刚好表示列数。 一般地，$\begin{pmatrix}A_0\\A_1\\\vdots\\A_{N-1}\end{pmatrix}=F\begin{pmatrix}a_0\\a_1\\\vdots\\a_{N-1}\end{pmatrix}$，$F_{j, k}=e^{i{2\pi jk\over N}}$令$\omega_N=e^{i{2\pi\over N}}\Rightarrow F_{j,k}=\omega^{jk}_{N}=F_{j,k}$。F称为傅里叶矩阵，F的各列相互正交且F对称(但注意：不是Hermite矩阵)，这个矩阵跟范德蒙德行列式很像。如果令$\omega_N=e^{i{2\pi\over N}}\Rightarrow F_{s,t}=\omega^{st}_{N}=F_{t,s}$那么F表示成$F=\begin{pmatrix}1&amp;1&amp;1&amp;1\\1&amp;\omega&amp;\omega^2&amp;\omega^3\\1&amp;\omega^2&amp;\omega^4&amp;\omega^6\\1&amp;\omega^3&amp;\omega^6&amp;\omega^9\end{pmatrix}$。 对于给定的$\begin{pmatrix}A_0\\A_1\\\vdots\\A_{N-1}\end{pmatrix}$，求$\begin{pmatrix}a_0\\a_1\\\vdots\\a_{N-1}\end{pmatrix}=F^{-1}\begin{pmatrix}A_0\\A_1\\\vdots\\A_{N-1}\end{pmatrix}$，$F^{-1}={1\over N}\overline{F}$，需要$N^2$次乘法，$N(N-1)$次加法（忽略除以N的除法），计算量$=O(N^2)$。 注记：实际上由前文可得$\begin{pmatrix}a_0\\a_1\\\vdots\\a_{N-1}\end{pmatrix}=\begin{pmatrix}c_0\\c_1\\\vdots\\c_{N-1}\end{pmatrix}$，因此是向量$\begin{pmatrix}A_0\\A_1\\\vdots\\A_{N-1}\end{pmatrix}$关于某个正交向量基的投影长度，即坐标分量。$(a_0, a_1,b_1,\ldots)$是$f(x)$关于$\{1,cosx,sinx,\dots\}$的坐标。 快速傅里叶变换FFT快速傅里叶变换减少了$DFT$的计算量到$O(Nlog_2^N)$ $N$ $N^2$ $Nlog_2^N$ FFT efficiency 256 65536 1024 64:1 512 262144 2304 114:1 1024 1048576 5120 205:1 注：$\lim\limits_{N\rightarrow +\infty}{log_2^N\over N}=0$ 解释算法：$N=4，\begin{pmatrix}a_0\\a_1\\a_2\\a_3\end{pmatrix}={1\over 4}\begin{pmatrix}1&amp;1&amp;1&amp;1\\1&amp;-i&amp;-1&amp;i\\1&amp;-1&amp;1&amp;-1\\1&amp;i&amp;-1&amp;-i\end{pmatrix}\begin{pmatrix}A_0\\A_1\\A_2\\A_3\end{pmatrix}\quad i^4=1$ $\begin{equation}4a_0=(A_0+A_2)+(A_1+A_3)\\4a_1=(A_0-A_2)-i(A_1-A_3)\\4a_2=(A_0+A_2)-(A_1+A_3)\\4a_3=(A_0-A_2)+i(A_1-A_3)\end{equation}$ 注意：求$a_2$的时候，可以把在求$a_0$过程中的两个括号的值重新利用，求$a_3$的时候，可以把在求$a_1$过程中的两个括号的值重新利用。 引入记号： 将$A_0, A_1, A_2, A_3$重新排序$A_0,A_2,A_1,A_3$使用记号，则 $FFT$算法将$DFT$算法分成$log_2^N$段，每一段有${N\over 2}$个butterfly operation。 举例：$N=8$，第一步将$A_0,A_1,\ldots,A_7$重新排序。原则：考虑$0,1,\ldots,7$的二进制，设$j$的二进制数的反转为$n_j$。若$j&lt;n_j$，则交换$Aj$和$A_{n_j}$。例如1的二进制数为${001}_2$,反转为${100}_2=4, 1&lt;4$，交换$A_1$和$A_4$。 排序后为：$A_0,A_4,A_2,A_6,A_1,A_5,A_3,A_7$（奇偶分离） 奇偶分离的原因：$\begin{pmatrix}a_0\\a_1\\\vdots\\a_{N-1}\end{pmatrix}=({1\over N}\overline{F})\begin{pmatrix}A_0\\A_1\\\vdots\\A_{N-1}\end{pmatrix}$，令$p(x)=A_0+A_1x+\cdots+A_{N-1}x^{N-1}=p_e(x^2)+xp_o(x^2),p_e=A_0+A_2x^2+\cdots\quad p_o=A_1+A_3x^2+\cdots$ 注解：e代表even，o代表odd，则$a_j={1\over N}(1,\overline{\omega}_N^j,\overline{\omega}_{N}^{2j},\ldots)\begin{pmatrix}A_0\\\vdots\\A_{N-1}\end{pmatrix}={1\over N}p(\overline{\omega}_N^j)={1\over N}[p_e(\overline{\omega}_N^{2j})+\overline{\omega}_N^{j}p_o(\overline{\omega}_N^{2j})], j=0,1,\cdots,{N\over 2}-1$ $a_{N\over 2+j}={1\over N}[p_e(\overline{\omega}_N^{2({N\over 2}+j)})+\overline{\omega}_N^{N\over 2+j}p_o(\overline{\omega}_N^{2({N\over 2+j})})],j=0,1,\cdots,{N\over 2}-1​$ 再由于：$\omega_N=e^{i{2\pi\over N}}\Rightarrow\overline{\omega}_N^{2j}=\overline{\omega}_{N\over 2}^j, \overline{\omega}_N^{N\over 2+j}=-\overline{\omega}_N^j, \overline{\omega}_N^{N+2j}=\overline{\omega}_{N\over 2}^j$ 所以：$\cases{a_j={1\over N}[p_e(\overline{\omega}_{N\over 2}^{j})+\overline{\omega}_N^{j}p_o(\overline{\omega}_{N\over 2}^{j})],j=0,1,\cdots,{N\over 2}-1\\a_{N\over 2+j}={1\over N}[p_e(\overline{\omega}_{N\over 2}^{j})-\overline{\omega}_N^{j}p_o(\overline{\omega}_{N\over 2}^{j})],j=0,1,\cdots,{N\over 2}-1}$ 所以：$a_j={1\over N}p(\overline{\omega}_N^j)$，再令$b_j=p_e(\overline{\omega}_{N\over 2}^{j}), b’_j=p_o(\overline{\omega}_{N\over 2}^{j})$，那么：$\cases{a_j = {1\over N}[ b_j+\overline{\omega}_N^{j}b’_j],j=0,1,\cdots,{N\over 2}-1\\a_{N\over 2+j} = {1\over N}[b_j-\overline{\omega}_N^{j}b’_j] ,j=0,1,\cdots,{N\over 2}-1}$，那么这又是一个butterfly operation: 可以重复利用以上原理对$b_j,b’_j$讨论，$b_j=p_e(\overline{\omega}_{N\over 2}^{j}),j=0,1,\cdots,{N\over 2}-1$，令$c_j=p_{ee}(\overline{\omega}_{N\over 4}^{j}), c’_j=p_{eo}(\overline{\omega}_{N\over 4}^{j})$，那么：$\cases{b_j = {1\over N}[c_j+\overline{\omega}_{N\over 2}^{j}c’_j],j=0,1,\cdots,{N\over 4}-1\\b_{N\over 4+j} = {1\over N}[c_j-\overline{\omega}_{N\over 2}^{j}c’_j],j=0,1,\cdots,{N\over 4}-1}$，那么这又是一个butterfly operation: 不停的划分下去，即：$FFT$算法将$DFT$算法分成$log_2^N$段，每一段有${N\over 2}$个butterfly operation。 举例：]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机图像]]></title>
    <url>%2F2017%2F08%2F11%2Fcomputer_graphics%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第11讲：计算机图像 引言熟悉的三维空间的基本变换是：平移(translation)，伸缩(rescaling)，旋转(rotation)，投影(projection)和反射(reflection)。现在一个问题：平移变换只对于点才有意义，因为平移变换会改变点的坐标，可是普通向量没有位置概念，只有大小和方向。那如何区分点和向量呢？这时候引入齐次坐标系(homogeneous coordinate system)。 对于任意一个3维空间点$p$的坐标均是参照（相对于）基点（原点）的坐标，可以表示成$p=x\vec e_1+y\vec e_2+z\vec e_3+ O=x\begin{pmatrix}1\\0\\0\end{pmatrix}+y\begin{pmatrix}0\\1\\0\end{pmatrix}+z\begin{pmatrix}0\\0\\1\end{pmatrix}+\begin{pmatrix}0\\0\\0\end{pmatrix}$，然而$\vec{op}=x\vec e_1+y\vec e_2+z\vec e_3$ 是不参照任何东西的，为了在线性代数中统一表示和区分，把$p=\begin{pmatrix}1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;0\\0&amp;0&amp;0&amp;0\end{pmatrix}\begin{pmatrix}x\\y\\z\\1\end{pmatrix}\quad \vec{op}=\begin{pmatrix}1&amp;0&amp;0&amp;0\\0&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;0\\0&amp;0&amp;0&amp;0\end{pmatrix}\begin{pmatrix}x\\y\\z\\0\end{pmatrix}$， 这时三维空间中的一个点的齐次坐标是$(x,y,z,1)$或$\begin{pmatrix}x\\y\\z\\1\end{pmatrix}$，一个向量的齐次坐标是$(x,y,z,0)$或$\begin{pmatrix}x\\y\\z\\0\end{pmatrix}$，所以平移变换就不是$R^3\rightarrow R^3$。 定义 一个函数$f: R^n \rightarrow R^N $是一个刚体运动(rigid motion)，如果$\forall v,w\in R^n, ||f(v)-f(w)||=||v-w||$，即内部的各点间距离不变。定理 $R^3$上的刚体运动是平移，旋转和反射的合成。此时，$f(v)=Av+v_0$，其中$A$是三阶正交阵。三阶正交阵的分类：设$A$是一个三阶正交阵，则存在实可逆阵$P$，$P^{-1}AP=\begin{pmatrix}cos\theta&amp;-sin\theta&amp;0\\sin\theta&amp;cos\theta&amp;0\\0&amp;0&amp;\pm 1\end{pmatrix}=B$，其中$P=(\alpha_1, \alpha_2, \alpha_3)$，根据相似的性质：$|B|=\pm 1\rightarrow |A|=\pm 1$，$A$本身是一个正交阵，因此$A^TA=I_3$。 若$B_{33=1}, AP=PB\rightarrow A\alpha_3=\alpha_3$是一个旋转矩阵，旋转轴是$\alpha_3$所在直线，旋转角度是沿$\alpha_3$方向逆时针转$\theta$角；若$B_{33}=-1, AP=PB\rightarrow A\alpha_3=-\alpha_3$ 是$A$将$\alpha_3$变为$-\alpha_3$，将$\alpha_1,\alpha_2$所在平面逆时针旋转$\theta$角，此时$A$的作用就是镜面反射和旋转，这里镜面指的是x-y平面。 平移translation 伸缩rescaling 旋转rotation3个特殊情形 一般情形 旋转的性质 投影projection 反射]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[图和网络]]></title>
    <url>%2F2017%2F08%2F08%2Fgraph_and_network%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第8讲：图和网络 简介欧姆定律Ohm’s law的向量形式 图与矩阵 关联矩阵incidence matrix 邻接矩阵adjacency matrix 拉普拉斯矩阵laplacian matrix 注： 半正定证明与刚度矩阵类似 网络和加权Laplacian矩阵 电路相关的物理定律 例子不接外部源 接外部源 带权$K=A^TCA$ 关联矩阵的四个基本子空间N(A) C(A)按$C(A)$的定义得：$C(A)=\{Ax|x\in R^n\}$ 。沿用前面使用的字母：$u$是各点电势，$e$是各边电势差，$Au=e$ ，当$Au=e$ 有解 $\Leftrightarrow e \in C(A)$ 去证明：$dim(C(A))=n-1$ ，即$A$ 的任意 $n-1$个列向量是线性无关的。设$A=(a_1,a_2,\,…\,,a_n) $，不妨假设$a_1,a_2,\,…\,,a_{n-1}$线性相关，那么存在$c_1, c_2,\,…\,,c_{n-1} \in R$ 且不全为0满足：$c_1a_1+c_2a_2+…+c_{n-1}a_{n-1}+0a_n=0\Rightarrow A\begin{pmatrix}c_1\\c_2\\\vdots\\c_{n-1}\\0\end{pmatrix}={0}\Rightarrow \begin{pmatrix}c_1\\c_2\\\vdots\\c_{n-1}\\0\end{pmatrix}\in N(A), $但与$N(A)=\left\{c\begin{pmatrix}1\\\vdots\\1\end{pmatrix} \Bigg| c\in R \right\} $ 矛盾，以此类推，得以证明$C(A)$的维数是$n-1$ ，即$A$的任意$n-1$个列向量均可作为$C(A)$的一组基。 发现矩阵中对应的回路：$e\in C(A)$ 如下等式有解 $Au=e\Rightarrow \begin{pmatrix}-1&amp;1&amp;0&amp;0\\ -1&amp;0&amp;1&amp;0\\0&amp;-1&amp;1&amp;0\\0&amp;-1&amp;0&amp;1\\0&amp;0&amp;-1&amp;1\end{pmatrix}\begin{pmatrix}u_1\\u_2\\u_3\\u_4\\u_5 \end{pmatrix}=\begin{pmatrix}e_1\\e_2\\e_3\\e_4\\e_5 \end{pmatrix} \Rightarrow \begin{cases}-u_1+u_2=e_1\\ -u_1+u_3=e_2\\ -u_2+u_3=e_3\\ -u_2+u_4=e_4\\ -u_3+u_4=e_5\end{cases} \Rightarrow \begin{cases}e_1-e_2+e_3=0\\e_3-e_4+e_5=0\end{cases}$ ，即边1,2,3这3条边电势差之和为0，由图上可得边1,2,3恰好构成一个回路，边3,4,5也一样。这恰好是Kirchholff Voltage Law (KVL)。把这两个回路等式书写成矩阵形式$\begin{pmatrix}1&amp;-1&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;-1&amp;1 \end{pmatrix}\begin{pmatrix} e_1\\e_2\\e_3\\e_4\\e_5 \end{pmatrix}=0$ . 此时称矩阵$B =\begin{pmatrix}1&amp;-1&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;-1&amp;1 \end{pmatrix}$ 为回路矩阵，可以看到它的每一行代表一个回路且称为极小回路，每一列代表一条边。如果边的方向是逆时针方向则取为正号，否则取为负号。注意，此时$e\in N(B)$。 此外，$BA=\begin{pmatrix}1&amp;-1&amp;1&amp;0&amp;0\\0&amp;0&amp;1&amp;-1&amp;1\end{pmatrix}\begin{pmatrix}-1&amp;1&amp;0&amp;0\\ -1&amp;0&amp;1&amp;0\\0&amp;-1&amp;1&amp;0\\0&amp;-1&amp;0&amp;1\\0&amp;0&amp;-1&amp;1\end{pmatrix}=\begin{pmatrix}0&amp;0&amp;0&amp;0\\0&amp;0&amp;0&amp;0\end{pmatrix}$即$C(A) \subseteq N(B) $ 。$dim(N(B))=3, dim(C(A))=3$，因此$C(A)$就构成了$N(B)$的基。从理意义角度理解：$A$矩阵执行的操作表示求解各边电势之差，$B$各行刚好是回路，由$KVL$定律得结果必为0. $N(A^T)$ 由定义得：$N(A^T)=\{y\in R^m|A^Ty=0\}$。例子中，关联矩阵$A$ 各行代表一条边，各列代表一个顶点。那么$A^T$ 的行代表顶点，列代表边。$A^Ty=0\Rightarrow\begin{pmatrix}-1&amp;-1&amp;0&amp;0&amp;0\\1&amp;0&amp;-1&amp;-1&amp;0\\0&amp;1&amp;1&amp;0&amp;-1\\0&amp;0&amp;0&amp;1&amp;1\end{pmatrix}\begin{pmatrix}y_1\\y_2\\y_3\\y_4\\y_5 \end{pmatrix}=\begin{pmatrix}0\\0\\0\\0\\0\end{pmatrix} \Rightarrow \begin{cases}-y_1-y_2=0\\y_1-y_3-y_4=0\\y_2+y_3-y_5=0\\y_4+y_5=0\end{cases}$物理意义解读：$y_i$是各第$i$边上的电流，上述等式表明每一个顶点输入输出电流和为0，即Kichhoff Current Law (KCL)。 $A^Ty=0$， 由前文得到：$BA=0 \Rightarrow A^TB^T=0 \Rightarrow A^TB^T=\begin{pmatrix}-1&amp;-1&amp;0&amp;0&amp;0\\1&amp;0&amp;-1&amp;-1&amp;0\\0&amp;1&amp;1&amp;0&amp;-1\\0&amp;0&amp;0&amp;1&amp;1\end{pmatrix}\begin{pmatrix}1&amp;0\\ -1&amp;0\\1&amp;1\\0&amp;-1\\0&amp;1\end{pmatrix}=\begin{pmatrix}0&amp;0\\0&amp;0\\0&amp;0\\0&amp;0\end{pmatrix}$因此，$C(B^T) \subseteq N(A^T)$。由于$r(A)=C(A)=r=n-1, N(A^T)+C(A)=m, N(A^T)=m-r=5-3=2$， 由于$B^T$的列向量线性无关，即$B$的行向量代表回路，那么回路向量就是$N(A^T)$的一组基。 $C(A^T)$ 总结 $N(A_{m\times n})$零空间 $Au=0$ ，$N(A)=c{(1,1,\,…\,,1)^T}_{n\times 1}$ ；物理意义：各点电势相等，电势差为0。 $C(A_{m\times n})$列空间 $Au=e$(上文用的是x, b)，$A$ 中任意$n-1$ 列构成了$C(A)$ 的一组基；物理意义每个极小回路电势守恒，每个极小回路构成的极大回路电势依然守恒，诠释了KVL定律。 $N(A^T)$左零空间 $A^Ty=0$，回路向量构成了$N(A^T)$ 的一组基；诠释了无外部电流源的KCL定律。 $C(A^T)$行空间 ，$A^Ty=f$， 每个极大树子图对应关联矩阵的行向量（即边）构成了$C(A^T)$ 的一组基；诠释了有外部电流源的KCL定律。 注计N(B)=C(A) B的零空间中的任何一个向量，它都要属于A的列空间，$A$的列空间中的每一个向量的特点，比如说$A$乘上一个$x_1$到$x_n$，$x_1$到$x_n$是$n$个顶点的电势。$A$乘上这个向量得到的是各个边上的电势差，那么相应的$x_j-x_k$就是$j$和$k$两个顶点上的电势差，顶点连线，$j$和$k$连线的边上的电势差。那么我们要想说明，N(B)中的向量属于C(A)那么我们只要说明任何一个向量属于B的零空间，它最后都能写成这样一种形式，就可以了。那么设$e$属于$N(B)$，那么我们可以取定这个连通图的一个极大树子图，然后在这个极大树子图$T$上取一个顶点作为基点，那么任意的另外一个顶点$K$跟这个基点之间它们连线的路在$T$上只有一条这样的路，因为$T$是一个树，它不可能有回路，所以在$T$中有唯一的一条连接K到基点的路。定义K的电势：在这条路上各边的电势之和，各边的电势之和，我们这个$e_1$到$e_m$呢，我们可以刻画各个边上的电势，那么我们可以看到$e$属于$N(B)$我们实际上可以检查出任意边上的电势差实际上是$e_j$等$u_k$减$u_1$，那么其中的这个$k$呢为j的起点，$l$为$j$的终点，最后我们就可以得到$e=-Au$，所以$e$就属于$C(A)$就是这个地方呢，我们要使用$e$属于$N(B)$，我们才能检查出：任意边上的这个电势差等于$u_k$减$u_l$，就是要满足科尔霍夫电压定律。 欧拉公式Euler’s formula 对于$B_{x \times m}\Rightarrow C(B^T)+dim(N(B))=r_B+dim(N(B))=m\Rightarrow m-r_B=dim(N(B))=dim(C(A))=n-1$ 又因为欧拉公式：$m-l=n-1$，得：$r_B=l$，即$B$是行满秩的，其实极小回路组对应极大线性无关组。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[工程中的矩阵]]></title>
    <url>%2F2017%2F08%2F07%2Fengineering_matrices%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第七讲：工程中的矩阵 应用数学的几个原则 将非线性问题变成线性问题(Nonlinear becomes linear) 将连续问题转化为离散的(Continuous becomes discrete) 工程中的矩阵许多物理定理都是线性关系(as approximations of reality)，比如胡克定律(Hook’s law)、欧姆定律(Ohm’s law)、牛顿第二定律(F=ma)，讨论这些定律的向量形式。线性关系的向量形式以如下方式、框架来讨论： $(1)\ e=Au$$(2)\ y=Ce$$(3)\ f=A^Tw$ 其中$u$是起始未知量$(primary\ unknown)$，$f$是外部的输入$(input)$： 线性问题通常是：输入$f$，求出$u$？ 例如：胡克定律：Displacement is proportional to force f=ku欧姆定律：Current is proportional to voltage difference推广到向量形式：$f=Ku$另外的例子：最小二乘法 $A^TAx=A^Tb$ ，求$x$ 线性弹簧模型 情形(1) 情形(2) 情形(3) 情形(4) 总结 胡克定律的向量形式把它应用到了弹性力学中，这个$u$表示的是质体的上下位移$e$是弹簧和伸长或缩短量，那么它们之间的关系呢？可以通过这样$A$这个矩阵那么A非常相似于一个差分矩阵，这样弹簧的伸长和缩短量和弹簧的弹力之间可以通过胡克定律来描述，那么这若干根弹簧它们所产生的弹力我们提升到胡克定律这样一个向量形式：C的每一个对角分量表示的是一个弹性系数（$y=Ce$）。最后弹力和外力之间：当达到平衡以后，可以通过一个矩阵去描述它们的关系，这个矩阵跟前面这个矩阵正好互为转置最后把整个过程合起来到这个矩阵$K=A^TCA$，称为刚度矩阵刚度矩阵刻划了系统受外力作用的形变程度。 刚度矩阵 注：此处老师讲解具有小的跳跃性 ，渣渣注释如下： $K,T$是正定的：$C$ 矩阵表示弹性系数是正定的，$K=A^TCA$ ，当 $A$ 可逆的时候，$K$ 与 $C$ 合同的。 与正定矩阵合同的对称矩阵也是正定的 ​ 判断是实对称阵是不是正定的第一条判别法：特征值是否全正，是的话则这个实对称矩阵就是正定的。根据惯性定理，由于与正定矩阵（记为$A$）合同的矩阵（记为$B$）其特征值符号与 $A$ 一致且保持对称性，那么$B$ 的特征值也是全正的，因此 $B$ 也是正定的。 $B, C$ 是半正定的 因为弹性系数矩阵 $C$ 是正定的对角阵$\Rightarrow x^TKx=x^TA^TCAx=x^TA^T ({\sqrt{C}}^T \sqrt{C}) Ax = x^TA^T {\sqrt{C}}^T \sqrt{C} Ax = ||\sqrt{C} Ax||^2 $，因为$A$是奇异的，$x^TKx\ge 0$， 因此K是半正定的。 性质1 注：$f_i$ 是 $i$ 个质题所受的外力，例如：重力，$f_i=m_ig,\ m_i$ 是 $i$ 个质体的质量。 性质2 注：此处老师直接说一般性结论，$A, \ B$ 都正定的，那么$AB$ 可能不对称，但是$AB$ 存在正特征值。圆盘定理也是直接引用（！！渣渣工科狗表示闻所未闻！！）。 性质3 从离散到连续$f=A^TCAu$ 总结： $(1)\ e=u_i-u_{i-1}=\Delta u={du\over dx}=Au\(2)y=Ce=c(x)e(x)\(3)\ f=-(y_i-y_{i-1})=-\Delta y=-{dy\over dx}=A^Ty$]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[伪逆(广义逆)pseudo inverse]]></title>
    <url>%2F2017%2F08%2F06%2Fpseudo_inverse%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第六讲：伪逆 引言矩阵的奇异值分解可以理解成从$R^n$到$R^m$的线性变换在不同基底下矩阵表示，接下来利用矩阵的奇异值分解来定义矩阵的伪逆，然后再利用矩阵的伪逆来讨论线性方程组Ax＝b无解时的最小二乘解，线性代数的中心问题是求解线性方程组$Ax=b$，最简单的情况是如果系数矩阵A是n阶的可逆矩阵，那么这时对于任意的n维向量$b$，线性方程组$Ax=b$有唯一的解，这个解是$A^{-1} b$，那这就启发去对于不可逆的矩阵或者是对于$A_{m\times n}$的矩阵，我们来定义它的一个逆矩阵，那么这时候逆矩阵我们叫做伪逆或者是叫广义逆 。 定义伪逆的定义来自于奇异值分解 (需先了解奇异值分解的内容)：(1)若$A$可逆，即$r=m=n$，则：$A^{-1}=(U\Sigma V^T)^{-1}=V\Sigma^{-1}U^T=A^+$，注意：由奇异值分解公式$AV=U\Sigma,\ (v_1\,…\,v_r)\in C(A^T),\ (v_{r+1}\,…\,v_n)\in N(A),\ (u_1\,…\,u_r)\in C(A),\ (u_{r+1}\,…\,u_m)\in N(A^T)$ 得：$AV=U\Sigma: C(A^T)\rightarrow C(A)$，同理可得：$A^+U^T=V\Sigma^{+}:C(A)\rightarrow C(A^T)$ (2)$AA^+=(U\Sigma_{m\times n} V^T)(V\Sigma^+_{n\times m}U^T)=U\Sigma_{m\times n}\Sigma^+_{n\times m}U^T=U\begin{pmatrix}I_r&amp;0\\0&amp;0\end{pmatrix}_{m\times m}U^T$ 得出以下3个性质： 对称性：$(AA^+)^T=AA^+$ $AA^+=u_1u_1^T+\,…\,+u_ru_r^T, U=(u_1,\,…\,u_r,\,u_{r+1}\,…\,,u_n)$ $AA^+=R^m$到$C(A)$的正交投影矩阵，$AA^+|_{C(A)}=id, AA^+|_{N(A^T)}=0$ 证明1：$AA^+x=(u_1u_1^T+\,…\,+u_ru_r^T)x=(u_1^Tx)u_1+\,…\,+(u_r^Tx)u_r​$，由奇异值svd分解得到$V=(v_1,\,…\,,v_r)​$是$A^T​$列空间（即$C(A^T)​$）的单位正交特征向量基，而$U=(u_1,\,…\,,u_r)​$是$C(A)​$的单位正交特征向量基，所以$AA^+​$是投影到$C(A)​$的正交投影矩阵（即保留了$C(A)​$的部分），因此$AA^+​$限制在$C(A)​$的变换即变成了恒等变换。而$U​$中$(u_{r+1}\,…\,u_m)​$和$U^T​$中$(u_{r+1}\,…\,u_m)^T​$即属于$N(A^T)​$的基乘以矩阵$\begin{pmatrix}I_r&amp;0\\0&amp;0\end{pmatrix}_{m\times m}​$中右下角的$0​$相当于对属于$N(A^T)​$的部分做了零变换。 证明2：$A^+u_j={1\over \sigma_j}v_j\Rightarrow AA^+u_j=A({1\over\sigma_j}v_j)={1\over \sigma_j}Av_j$ 再根据奇异值分解中$Av_j=\sigma u_j, (1\le j \le r)$ 得$AA^+u_j=u_j(1\le j\le r),\ AA^+u_j=0(r+1\le j \le m)$ 验证：$(AA^+)(AA^+)=U\begin{pmatrix}I_r&amp;0\\0&amp;0\end{pmatrix}_{m\times m}U^TU\begin{pmatrix}I_r&amp;0\\0&amp;0\end{pmatrix}_{m\times m}U^T$，由于从svd分解知道$U$是单位正交特征向量基 ，因此：$U^T=U^{-1}\Rightarrow (AA^+)(AA^+)=U\begin{pmatrix}I_r&amp;0\\0&amp;0\end{pmatrix}_{m\times m}U^T=AA^+$，这正是投影的性质：多次投影结果还是第一次投影结果。 结果：$\forall\ p\in R^m, b=p+e, p\in C(A), e\in N(A^T), AA^+b=p$ (3)$A^+A=(V\Sigma^+_{n\times m}U^T)(U\Sigma_{m\times n} V^T)=V\begin{pmatrix}I_r&amp;0\\0&amp;0\end{pmatrix}_{n\times n}V^T$ 得到以下三个性质（证明同上）： $(A^+A)^T=A^+A$ $A^+A=v_1v_1^T+\,…\,+v_rv_r^T$ $A^+A=R^n$到$C(A^T)$的正交投影矩阵（$A^+A|_{C(A^T)}=id,\quad A^+A|_{N(A)}=0$）: $\forall\ x\in R^n=C(A^T)\bigoplus N(A)),\ x=x_{1,r}+x_{r+1,n}, \ x_{1,r}\in C(A^T),\ x_{r+1,n}\in N(A^T),\\ A^+Ax=A^+A(x_1,\,…\,x_r,x_{r+1},\,…\,x_n)=x_{1,r}$ 为什么称为伪逆、左逆、右逆 例子注：$u_1, u_2,u_3$ 是$R^m$的一组基底那么它是${Av_1\over \sigma_1}$，那么很容易计算出来，是${1\over\sqrt{2}}\begin{pmatrix}1\\1\\0\end{pmatrix}$那$u_2$和$u_3$ 分别是0所对应的特征向量，$u_2$和$u_3$可以看成是三维空间里头，$u_1$的正交补所给出来的单位正交的向量。 特例 Jordan标准形的伪逆推导结论：$J_n^+=J_n^T$，Jordan标准形的伪逆是它自己的转置。 Moore-Penrose伪逆E.H.Moore伪逆 Penrose伪逆注： A可以是mxn的复数矩阵，这样的话(3)(4)里面就变成共轭转置。 Penrose伪逆与E.H.Moore伪逆定义是等价的。 $(1)AXA =A \Rightarrow AXAX=AX\Rightarrow (AX)^N=AX\Rightarrow AX$ 是幂等矩阵，投影矩阵$(2)XAX=X\Rightarrow XAXA=XA\Rightarrow (XA)^N=XA\Rightarrow XA$ 是幂等矩阵，投影矩阵$(3)(AX)^T=AX\Rightarrow AX$ 是对称矩阵$(4)(XA)^T=XA\Rightarrow XA$ 是对称矩阵 通过奇异值分解得到的伪逆矩阵$A^+$，$AA^+: R^m \rightarrow C(A)$，$A^+A:R^n\rightarrow C(A^T)=C(A^+)$，前文已经证明两者都是对称的，所以符合Penrose对伪逆矩阵的定义。对于伪逆唯一性的证明上文图片太小可以放大来看。 伪逆的应用之最小二乘法引言但是我们需要求$e$ 即误差最小的解！但是这时候$A_{m\times n}$不是列满秩不存在逆矩阵，于是自然地想到利用伪逆求解。 伪逆求解正规方程——最佳最小二乘解注：由于$A^+$ 来自于：$A^+U^T=V\Sigma^{+},\ (v_1\,…\,v_r)\in C(A^T),\ (v_{r+1}\,…\,v_n)\in N(A),\ (u_1\,…\,u_r)\in C(A),\ (u_{r+1}\,…\,u_m)\in N(A^T),\\\Sigma^+=\begin{pmatrix}{1\over \sigma_1}\\&amp;{1\over \sigma_2}\\&amp;&amp;.\\&amp;&amp;&amp;.\\&amp;&amp;&amp;&amp;{1\over \sigma_r}\\&amp;&amp;&amp;&amp;&amp;0\end{pmatrix}_{n\times m}\Rightarrow A^+: C(A)\rightarrow C(A^T)$，另外由于 $A^TAx=0, Ax=0$ 同解所以零空间相同。 最佳最小二乘解的四个基本子空间]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[马尔科夫矩阵和正矩阵]]></title>
    <url>%2F2017%2F08%2F06%2FMarkov_matrix%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第9讲：马尔科夫矩阵和正矩阵 引言 马尔科夫链详细参考：Markov chain Markov Matrix正矩阵 马尔科夫矩阵定义 马尔科夫矩阵性质 正马尔科夫矩阵 正马尔科夫矩阵的性质 例子 人口流动模型 正矩阵 谱半径Spectral radius 定义为谱半径是矩阵特征值模的最大值，而非最大特征值，注意：矩阵来自于线性变换（也叫线性算子），因此线性变换也有谱半径，详询wiki: 谱半径Spectral radius。 ##Perron-Frobenius theorem 这个原理应用在统计推断，经济，人口统计学，搜索引擎的基础。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性变换2]]></title>
    <url>%2F2017%2F08%2F05%2Flinear_transformation_2nd_part%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第五讲：线性变换2 前言对于给定的线性变换选取适当的基底使得其矩阵表示尽可能简单，我们引入了线性变换的矩阵表示对于从$n$维的向量空间$V$到$m$维的向量空间$W$的线性变换$\sigma$，我们取定$V$的一组基$v_1$到$v_n$取定W的一组基$w_1$到$w_m$，那线性变换$σ$作用在$v_1$到$v_n$上可以被$w_1,…,w_m$线性表示，表示的系数我们被一个$m×n$的矩阵$A$去描述，那么这样线性变换$σ$就跟这个$m×n$的矩阵$A$一一对应。线性变换的矩阵表示要依赖于我们基底的选取，一般说来如果基做了改变，同一个线性变换它会有不同的矩阵表示，那我们希望找出线性变换与基底选取无关的性质，这样当我们借助矩阵来研究线性变换的这些性质的时候就可以利用好基底下面尽可能简单的矩阵表示。 恒等变换与基变换恒等变换就是不变，那么不变的线性变换对应单位矩阵。 the 9th property of determinant: the determinant of $AB$ is det $A$ times det $B$: $|AB| = |A||B|$ 因此：由于$(\sigma_1\,….\,\sigma_n)$ 和 $(\beta_1\,…\,\beta_n)$ 都是基向量，因此都是列满秩，又是 $n$ 维，所以可逆，再推出$P$可逆。否则 $|\alpha_1\,…\,\alpha_n|\ne|\beta_1\,…\,\beta_n||P|$ 基变换的应用一张256x256的灰度图像 注意：$C^N$是$n$维元素可为复数的基 图像的其中3种基底 小波基好求它的逆，傅里叶基也好求它的逆。如果是$4\times4$纯色图像直接用小波基或者傅里叶基的第一个分量$w_1$和$\xi_1$做基底，表示成$c_1w_1=W\begin{pmatrix}c_1\\0\\0\\0\end{pmatrix}$和$c_1\xi_1=\xi\begin{pmatrix}c_1\\0\\0\\0\end{pmatrix}$。而像素之间变换比较剧烈的图像可用小波基中的 $c(w_3+w_4)$ 和傅里叶基中的 $c\xi_3$ 。 jpeg 图像本身是用系数矩阵$c$表示，那么所谓的压缩和传输图像也是压缩和传输这个矩阵$c$。压缩做的就是用尽可能少的信息（数据）去代表原有的信息（数据），这个过程会丢失一些不重要的信息（数据），对应到矩阵上就是$c$的非0项元素比较少（这个要求用更少数量的基底向量就能接近描述出原来的矩阵，越少越好）。由于 $c=W^{-1}x$ 因此能不能快速计算基底的逆也很重要，而小波基和傅里叶基正符合此特点。 线性变换在不同基下的矩阵 定理：$n$向量空间$V$上的线性变换$\sigma$在$V$的不同基下的矩阵是相似矩阵。 由上图可得：$I_1$ 和 $I_2$ 是恒等变换$(\beta_1\,…\,\beta_n)=I_1(\beta_1\,…\,\beta_n)=(\alpha_1\,…\,\alpha_n)P$$(\alpha_1\,…\,\alpha_n)=I_2(\alpha_1\,…\,\alpha_n)=(\beta_1\,…\,\beta_n)P^{-1}$线性变换复合角度：$\sigma=I_2\,\sigma\,I_1\,\rightarrow\,B=P^{-1}AP$ 同一个线性变换在不同基下的不变性当我们借助于矩阵来研究线性变换的时候，我们希望研究线性变换与基底选取无关的性质。由以上的讨论我们知道这个向量空间$V$到自身的线性变换在不同基下的矩阵表示是互为相似矩阵的。因此，所谓与基底选取无关的性质也就是相似变换下不变的性质，那么这样自然地研究相似不变量是线性代数中很重要的内容。我们知道对于一个矩阵而言特征多项式、特征值、迹、行列式、矩阵的秩等等都是矩阵的相似不变量，这样我们就称一个n维向量空间$V$上线性变换在$V$的一组基下的矩阵$A$，把矩阵表示$A$的特征多项式、特征值、迹行列式等等就叫做这个线性变换的特征多项式、特征值 、迹、行列式。 矩阵分解与基变换给定一个$R^n$到$R^m$的线性变换$σ$，它在$R^n$中的标准基$e_1$到$e_n$和$R^m$的标准基$ẽ_1,…,ẽ_m$下的矩阵是 $A$ ，$σ$作用在$e_1 … e_n$上面就等于$\tilde{e}_1,…, \tilde{e}_m$去乘以矩阵$A$，也就是说$σ$作用在$e_j$上，就等于$A$的第j列，也就是$A$去乘以$e_j$，因此这个线性变换就可以表示成对任何的$n$维向量$v$，那么$σ$作用在$v$上就是矩阵$A$去乘以$V$ ：$$\sigma(e_1\,…\,e_n)=(\tilde{e}_1 \,…\,\tilde{e}_m)A\rightarrow\sigma(e_j)=Ae_j$$ 接下来做基变换，第一个改变输入基，第二个改变输出基，第三个输入输出基都改。 对角化矩阵视为线性变换 由上可得 $\sigma(x_1\,…\,x_n)=(x_1\,…\,x_n)\Lambda=S\Lambda$ ，$x$ 为特征向量基，另外基变换 ${id}_1(S)=S=\{e\}S$$σ$这个线性变换在A的特征向量作为的新基下面，它的矩阵表示是 $\Lambda$ 这个对角阵。而$σ$从 $R^n$ 到 $R^n$在标准基下的矩阵是$A$，$σ$在特征向量基下的矩阵表示是对角阵 $\Lambda$。那么输入$x$这组基，输出$e$这组基，这个恒同变换，它的矩阵表示是 $S$ 。如果输入$e$这组基 ，输出$x$这组基这个恒同变换，它的矩阵表示是$S^{-1}$。 奇异值分解视为线性变换 线性变换的核与像定义 线性变换的零度与秩 线性变换秩的证明 注：$L(\sigma(v_1),\,…\,,\sigma(v_n))$ 符号含义：由 $\sigma(v_1),\,…\,,\sigma(v_n)$ 线性张成。 线性变换的维度公式 单射满射可逆中学学过的单射双射满射 线性变换下的单射（injective），满射（surjective）与逆（inverse） 第一个等价符号证明（反证法）：如果单射无法推出核只有$\{0\}$，那么假设$\exists\,\alpha(\ne0)\in{ker\,\sigma}$ 那么$\sigma(\alpha)=0$，又因为$\sigma(0)=0$, 即$\sigma(\alpha\,or\,0)=0$与单射矛盾。反之，如果$\sigma(v_1)=0, \sigma(v_2)=0$，根据线性变换的定义或者性质得：$\sigma(v_1-v_2)=0\rightarrow v_1-v_2\in ker\,\sigma=\{0\}\rightarrow v_1=v_2\rightarrow \sigma$ 是单射。因此：$\sigma$是单射$\Leftarrow\Rightarrow ker\,\sigma=\{0\}$ 例子： 不变子空间定义 不变子空间的意义 那从这里头我们看到，我们希望把大空间分解成不变子空间的直和，从而能够取出合适的基底，从而使得线性变换在这组基底下的矩阵表示能够成为对角块的形状，那么对于线性变换的研究就转化成它限制在不变子空间上的研究以此为基础，看一下幂零变换的结构。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线性变换1]]></title>
    <url>%2F2017%2F08%2F04%2Flinear_transformation_1st_part%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第四讲：线性变换1 前言历史上英国数学家Arthur Cayley是为了描述线性变换的复合而引入矩阵的乘法，从而使矩阵成为数学的研究对象。线性变换是两个向量空间之间保持线性运算的映射。线性代数就是从其中心问题（求解线性方程组）出发发展起来研究向量空间、线性变换以及研究相关数学问题的数学学科。对有限维向量空间的研究总可以转化成对矩阵的研究，这是线性代数的核心特点。 线性变换的定义性质运算回顾中学阶段学过的函数：$f(x)=2x\quad g(x)=x^{2}\quad l(x)=sin(x)$ 都是一个映射从定义域中的一个数映成值域中的一个数。推广到把向量映射到向量的映射比如f是从 $R^{3}$ 映到 $R^{2}$ 的一个映射：$f:\begin{pmatrix}x\\y\\z\end{pmatrix}\,\rightarrow\,\begin{pmatrix}2x\\3y-z\end{pmatrix}$，我们关心向量空间到向量空间的映射。人们发现平面上的点、空间中的点 、矩阵多项式函数、连续函数等等集合看上去不同但是它们各自的加法和数乘满足同样的性质，于是就引入了向量空间这样的一个抽象的概念来统一地研究向量空间的概念。 向量空间的定义 线性变换的定义 例子 注意，由线性变换的定义 $T:V\,\rightarrow\,W$ 得到 $T(0)=0$ 线性变换的性质 针对第一条证明： 如果 $T(0)\ne0$ 不满足线性变换定义 $T(cx)=cT(x)$，例如： $T(0)=1\,\rightarrow\,T(0)=T(c0)=1\,\ne\,cT(0)=c$ 针对第三条证明：若 $x_{1},\,…\,,x_{n}$ 线性相关，那么存在不全为0的数 $c_{1},\,…\,,c_{n}$ 满足 $c_{1}x_{1}\,+\,…\,+\,c_{n}x_{n}=0$ 即 $T(c_{1}x_{1}\,+\,c_{2}x_{2}\,+\,…\,+\,c_{n}x_{n})=T(0)=c_{1}f(x_{1})\,+\,…\,+\,c_{n}f(x_{n})=0$，即$T(x_{1}),\,…\,,T(x_{n})$ 线性相关。 线性变换的运算加法 数乘 乘积注：线性变换的乘积被定义为线性变换的复合运算 注意：线性变换不满足乘法交换律、消去律，与矩阵乘法类似 逆 幂 多项式 注：由于线性变换不满足乘法交换律，因此$(\sigma\tau)^{m}=\underbrace{(\sigma\tau)(\sigma\tau)\,…\,(\sigma\tau)}_{m个(\sigma\tau)相乘}\ne\sigma^{m}\tau^{m}$ 线性变化的矩阵表示 由于 $T(v_{1})$,$T(v_{2})$, … , $T(v_{3})\,\epsilon\,W$ 这个输出空间, 因此可以进行如下： 例子 线性变换与矩阵之间的关系一一对应 线性变换的乘积与矩阵的乘积 注（极其重要）：这里线性变换的乘积（复合）对应的是矩阵的“左乘”。 线性同构 例：设线性变换$\tau\,:\,R^{3}\rightarrow\,R^{2}$定义为$\tau(x,y,z)=(x+y,y-z)$, 线性变换$\sigma:R^{2}\,\rightarrow\,R^{2}$定义为$\sigma(u,v)=(2u-v,u)$.求线性变换$\sigma\tau:R^{3}\,\rightarrow\,R^{2}$在$R^{3}$与$R^{2}$标准基下的矩阵. 解：注意到$\sigma\tau=\sigma(\tau(x,y,z))=\sigma(x+y, y-z)=(2x+y+z, x+y)$ 因此标准基下线性变化$\sigma(\tau(x\,y\,z)):R^{3}\to\,R^{2}$: $$e_{1}=(1,0,0)^{T}, e_{2}=(0,1,0)^{T}, e_{3}=(0,0,1)^{T}\,\Rightarrow\, I_{3}=(e_{1}\,e_{2}\,e_{3})$$ $\sigma(\tau(e_{1}))=\sigma(\tau(\,(1,0,0)\,)=\begin{pmatrix}2\\1\\\end{pmatrix}\quad\sigma((\tau(e_{2}))=\begin{pmatrix}1\\1\\\end{pmatrix}\quad\sigma(\tau(e_{3}))=\begin{pmatrix}1\\0\\\end{pmatrix}$ $\sigma(\tau(e_{1}\,e_{2}\,e_{3}))=\sigma(\tau(I_{3}))=\underbrace{\begin{pmatrix}2&amp;1&amp;1\\1&amp;1&amp;0\end{pmatrix}}_{C}$ 第一个线性变化$\tau(x,y,z)=(x+y,y-z):R^{3}\,\to\,R^{2}$ : $$\tau(e_{1})=\tau(1,0,0)=(1+0,0+0)=(1,0)$$ $$\tau(e_{2})=\tau(0,1,0)=(0+1,1+0)=(1,1)$$ $$\tau(e_{3})=\tau(0,0,1)=(0+0,0+1)=(0,1)$$ $$\tau(I_{3})=\tau(e_{1}\,e_{2}\,e_{3})=\begin{pmatrix}1&amp;1&amp;0\\0&amp;1&amp;-1\end{pmatrix}=I_{2}\begin{pmatrix}1&amp;1&amp;0\\0&amp;1&amp;-1\end{pmatrix}$$ $$\underbrace{\begin{pmatrix}1&amp;1&amp;0\\0&amp;1&amp;-1\end{pmatrix}}_{A}\begin{pmatrix}x\\y\\z\end{pmatrix}=\begin{pmatrix}x+y\\y-z\end{pmatrix}$$ 第二个线性变化$\sigma(u,v)=(2u-v,u): R^{2}\,\to\,R^{2}$: $$\delta_{1}=(1,0)^{T}, \delta_{2}=(0,1)^{T}\,\Rightarrow\, I_{2}=(\delta_{1}\,\delta_{2})$$ $$\sigma(\delta_{1})=\begin{pmatrix}2\\1\end{pmatrix},\,\sigma(\delta_{2})=\begin{pmatrix}-1\\0\end{pmatrix}\Rightarrow\sigma(\delta_{1}\,\delta_{2})=I_{2}\begin{pmatrix}2&amp;-1\\1&amp;0\end{pmatrix}$$ $$\underbrace{\begin{pmatrix}2&amp;-1\\1&amp;0\end{pmatrix}}_{B}\begin{pmatrix}u\\v\end{pmatrix}=\begin{pmatrix}2u-v\\u\end{pmatrix}$$ 发现$BA=C\,\Rightarrow\,\begin{pmatrix}2&amp;-1\\1&amp;0\end{pmatrix}\begin{pmatrix}1&amp;1&amp;0\\0&amp;1&amp;-1\end{pmatrix}=\begin{pmatrix}2&amp;1&amp;1\\1&amp;1&amp;0\end{pmatrix}$，符合上文所说的线性变换的复合是对应矩阵的左乘。 结论：有限维向量空间上的线性变换$\leftarrow\rightarrow$矩阵]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[奇异值分解singular values decomposition]]></title>
    <url>%2F2017%2F08%2F03%2Fsingular_values_decomposition%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第三讲：奇异值分解 前言对角矩阵是我们最喜欢的一类矩阵，对能够相似于对角阵的矩阵能方便地计算其幂和指数，对不能相似于对角阵的方阵。上节课我们讨论了如何求出其尽可能简单的相似标准形及Jordan标准形以上讨论的都是方阵。那么对m乘n的矩阵我们如何来对它进行对角化呢？ 线性代数中最重要的一类矩阵分解即奇异值分解，从而回答以上的问题。对角矩阵是我们最喜欢的一类矩阵，因为给定一个对角阵立即就可以得到它的特征值，行列式，幂和指数函数等等。对角矩阵的运算跟我们熟悉的数的运算有很多相似之处，而一个n阶的矩阵相似于对角阵当且仅当它存在着n个线性无关的特征向量。特别地，实对称矩阵一定会正交相似于对角阵，也就是说给你一个实对称矩阵，一定存在着正交矩阵$Q$把它的列向量记成$v_1$到$v_n$，它能够满足$Q^TAQ$等于$\lambda$，$\lambda$是一个对角阵，它的对角元是$A$的特征值，那么其中$Q$的列向量$v_i$，它是矩阵$A$的属于特征值，$\lambda_i$的特征向量，也就是满足$Av_i$等于$\lambda_iv_i$。我们现在有个问题是说，如果对于$m \times n$的一个矩阵，我们如何来”对角化”它。那么也就是说在什么意义上，我们能够尽可能地。把$m \times n$的一个矩形的阵向对角阵靠拢，今天我们来讨论矩阵的奇异值分解它是线性代数应用中，最重要的一类矩阵分解。 $AA^T$与$A^TA$的特性$AA^T$与$A^TA$的特征值 $AA^T$与$A^TA$非0特征值集合 $A^TA$与$AA^T$的特征向量 令$u_i:={Av_i \over \sigma_i}\in\,R^m(1 \le i \le r) $，则 $AA^Tu_i=A(A^T\frac{Av_i}{\sigma_i})=A\frac{A^TAv_i}{\sigma_i}=A\frac{\sigma_i^2v_i}{\sigma_i}={\sigma_i}^2{Av_i \over \sigma_i}={\sigma_i}^2u_i$，得出：$AA^Tu_i={\sigma_i}^2u_i$。又因为：${u_i}^T{u_j}=\frac{(Av_i)^T}{\sigma_i}{Av_j \over \sigma_j}={v_i^T(A^TAv_j) \over \sigma_i\sigma_j}=\frac{\sigma_j^2{v_i}^Tv_j}{\sigma_i\sigma_j}={\sigma_j\over \sigma_i}v_i^Tv_j\rightarrow u_i^Tu_j=\begin{cases}0, &amp; i\ne j\\ 1, &amp; i=j\end{cases}$故：$\{u_i|1\le i \le r\}$ 是$AA^T$的单位正交特征向量。 根据假设（$v_1,\,…\,,v_n$是$A^TA$的单位交基，$\sigma_1^2,\,…\,,\sigma_n^2$是$AA^T$的特征值）得：$A^TAv_i=\sigma_i^2v_i(1\le i\le r) \rightarrow v_i^TA^TAv_i=v_i^T\sigma_i^2v_i=\sigma_i^2v_i^Tv_i \rightarrow ||Av_i||^2=\sigma_i^2 \rightarrow|Av_i|=\sigma_i$ 从$AA^T$得出SVD$(1)u_i:={Av_i \over \sigma_i}\in\,R^m(1 \le i \le r) \rightarrow Av_i=\sigma_iu_i\\ (2)A^TAv_i={\sigma_i}^2v_i, (i\le i \le r)\rightarrow A^T{Av_i\over \sigma_i}=\sigma_iv_i\rightarrow A^Tu_i=\sigma_iv_i$ 由上式子得：$U$是$A$列空间的一组单位正交基，$V$是$A^T$的列空间的一组单位正交基。$\sigma_i$是$Av_i$的长度，计$\begin{pmatrix}\sigma_1&amp;&amp;&amp;&amp;\\&amp;.&amp;&amp;&amp;\\&amp;&amp;.&amp;&amp;\\&amp;&amp;&amp;.&amp;\\&amp;&amp;&amp;&amp;\sigma_r\end{pmatrix}$为$\Sigma$，得：$A_{m\times n}V_{n\times r}=U_{m\times r}\Sigma_{r\times r}\rightarrow A_{m\times n}=U_{m\times r}\Sigma_{r\times r} {V^{-1}}_{r\times n}\\=U_{m\times r}\Sigma_{r\times r} {V^{T}}_{r\times n}$ 向量形式：$A=\sum_{i=1}^r \sigma_i u_i{v_i}^T$ SVD形式 例题 求$u_3$两种方法： 方法1：$AA^Tu_3=\begin{pmatrix}1&amp;0\\0&amp;1\\1&amp;-1\end{pmatrix}\begin{pmatrix}1&amp;0&amp;1\\0&amp;1&amp;-1\end{pmatrix}u_3=\begin{pmatrix}1&amp;0&amp;1\\0&amp;1&amp;-1\\1&amp;-1&amp;2\end{pmatrix}u_3=0u_3\rightarrow u_3={1\over\sqrt{3}}\begin{pmatrix}1\\ -1\\ -1\end{pmatrix}$ 方法2：$u_j:=\begin{pmatrix}x\\y\\z\end{pmatrix}, \sum_{i=1}^{r=3}u_iu_j=0 (i\ne j), ||u_j||^2=1\rightarrow u_{j=3}={1\over\sqrt{3}}\begin{pmatrix}1\\ -1\\ -1\end{pmatrix}$ svd几何意义 svd应用svd与矩阵的四个基本子空间 svd与图像压缩 奇异值与特征值关系 奇异值与奇异矩阵]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[傅里叶级数]]></title>
    <url>%2F2017%2F08%2F02%2FFourier_series%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第10讲：傅里叶级数 引言 傅里叶级数Fourier series定义定义1设$f(x)$是周期为$2\pi$的有限个分段（piecewise）的连续函数（ continuous function）（即在$[\pi,-\pi]$中只有有限个点不连续，且不连续点的左右极限存在），那么它的傅里叶级数是 $F={a_0\over 2}+\sum\limits_{k=1}^{\infty}(\ a_kcos(kx)+b_ksin(kx)\ ), a_k={1\over \pi}\int_{-\pi}^{\pi}f(x)cos(kx)dx, b_k={1\over \pi}\int_{-\pi}^{\pi}f(x)sin(kx)dx,k=0,1,\ldots$，这个级数又称为傅里叶级数的实形式。 $f(x)$举例如下的$(1)$，而 $(2)$ 在周期内的不连续点处无极限。 定义2$f(x)$如上，它的傅里叶级数的复形式是$F=\sum\limits_{k=-\infty}^{+\infty}c_ke^{ikx}, c_k={1\over 2\pi}\int_{-\pi}^{\pi}f(x)e^{-ikx}dx$. 推导如下： 在定义1中，使用欧拉公式：$e^{ix}=cosx+isinx\Rightarrow cosx={e^{ix}+e^{-ix}\over 2},\ sinx={e^{ix}-e^{-ix}\over 2i}$ ，定义1中的傅里叶级数变成$F={a_0\over 2}+\sum\limits_{k=1}^{\infty}[{a_k\over 2}(e^{ikx}+e^{-ikx})-{ib_k\over 2}(e^{ikx}-e^{-ikx})]={a_0\over 2}+\sum\limits_{k=1}^{\infty}({a_k-ib_k\over 2}e^{ikx}+{a_k+ib_k\over 2}e^{-ikx}).$ 其中$a_k-ib_k={1\over 2\pi}\int_{-\pi}^{\pi}f(x)(e^{ikx}+e^{-ikx})dx-{i\over 2\pi}\int_{-\pi}^{\pi}f(x)\frac{e^{ikx}-e^{-ikx}}{i}dx ={1\over \pi}\int_{-\pi}^{\pi}e^{-ikx}dx,\ a_k+ib_k={1\over \pi}\int_{-\pi}^{\pi}e^{ikx}dx​$ , 令$c_k={a_k-ib_k\over 2}={1\over 2\pi}\int_{-\pi}^{\pi}f(x)e^{-ikx}dx,k=1,2,\ldots\quad c_{-k}={a_k+ib_k\over 2}={1\over 2\pi}\int_{-\pi}^{\pi}f(x)e^{ikx}dx,k=1,2,\ldots​$ 这样就得到定义2。注意：正如泰勒级数，这里并没有断言$f(x)​$等于它的傅里叶级数。 定理设$f(x)$是周期为$2\pi$的周期函数，$f(x)$和$f’(x)$均在$[-\pi, \pi]$上是分段连续的，则$f(x)$的傅里叶级数收敛，且在任意连续点$x=a$等于$f(a)$，在不连续点$x=a$等于${1\over 2}[lim_{x\rightarrow a^{+}}f(x)+lim_{x\rightarrow a^{-}}f(x)]$。 内积空间inner product space设$V$是一个向量空间（线性空间）（$R$或$C$上），$V$上的一个内积是这样一个函数 (-,-) : $V\times V\rightarrow R\ or\ C$ 满足： $\forall u\in V, (u,u)\ge0$，且若$(u,u)=0\rightarrow u=0$ $(c_1u+c_2v,w)=c_1(u,w)+c_2(v,w), u,v,w\in V, c_1,c_2\in R\ or\ C$ $\overline{(u,v)}=(v,u)$ 共轭对称 注：没有假设$v$是有限维的。第一条：$u$跟自己的内积必须是一个实数且是一个正数，或者说更确切地是一个非负数，如果$u$跟自己的内积是等于0的，那么就可以确定$u$就是$0$向量。第二条：两个向量的线性组合跟另一个向量的内积相当于两个向量跟另一个向量先作内积再做线性组合。第三条：$u$和$v$的内积与$v$和$u$的内积是一个共轭的关系，如果这个函数是定义在$V\times V\rightarrow R$，那么这个内积函数是个对称的，$u,v$的内积与$v,u$的内积是一样的，如果定义在复数上，那么就差一个共轭。 令$||u||=\sqrt{(u,u)}$，若$||u||=1$，则$u$ 是一个单位向量。任何一个向量$u\ne 0\rightarrow {v\over ||v||}$是一个单位向量，关于范数($||·||$)，这里范数是长度。 例 $V=R^2,u=\begin{pmatrix}a_1\\a_2\end{pmatrix}, v=\begin{pmatrix}b_1\\b_2\end{pmatrix},(u,v)=u^T v=a_1b_1+a_2b_2$ 是一个内积，$||u||=\sqrt{(a_1^2+a_2^2)}$。若$V=C^2$，$u,v\in C, (u,v)=u^T\bar{v}=a_1\overline{b_1}+a_2\overline{b_2}$ 。 $C[a,b]$是定义在区间$[a,b]$上的全体连续实函数构成的向量空间。定义连续函数的内积为$(f,g)=\int_{a}^{b}f(x)g(x)dx$ 。验证这个式子：$f(x)\in C[a,b], (f,f)\ge 0$ ，即 $(f,f)=\int_{a}^{b}{f(x)}^2dx=\int_{a}^{b}{|f(x)|}^2dx\ge 0$。若$(f,f)=0$，即$\int_{a}^{b}{|f(x)|}^2dx=0$，令$F(t)=\int_{a}^{t}{|f(x)|}^2dx, a\le t \le b$，则$F(t)=0, F(t)$可导，$F’(t)={|f(t)|}^2=0$，即$f(t)=0,t\in [a,b]$。在这里函数的长度的平方定义为函数与自身的内积，即$||f(x)||^2=(f(x),f(x))=\int_{a}^{b}f(x)f(x)dx$。 在例2中，若$C[a,b]$是$[a,b]$上的连续复函数的向量空间，则内积定义为：$(f,g)=\int_{a}^{b}f(x)\overline{g(x)}dx$。 标准正交系orthonormal system总结：若f(x)在区间$[a,b]$存在傅里叶级数，那么f(x)的傅里叶级数是f(x)在标准正交系$\{\frac{1}{\sqrt{2\pi}}, \frac{1}{\sqrt{\pi}}sinx, \frac{1}{\sqrt{\pi}}cosx, \frac{1}{\sqrt{\pi}}sin2x, \frac{1}{\sqrt{\pi}}cos2x, \ldots\}$下的投影。 周期函数的傅里叶级数对傅里叶级数的实数形式$F={a_0\over 2}+\sum\limits_{k=1}^{\infty}(\ a_kcos(kx)+b_ksin(kx)\ ), a_k={1\over \pi}\int_{-\pi}^{\pi}f(x)cos(kx)dx, b_k={1\over \pi}\int_{-\pi}^{\pi}f(x)sin(kx)dx,k=0,1,\ldots$进行变量代换，令 $x={\pi\over L}t, k=n$ 得：$ dx={\pi\over L}dt,\ t=\cases{L, x=\pi\\ -L, x=-\pi}\Rightarrow f(t)={a_0\over 2}+\sum\limits_{n=1}^{\infty}[a_ncos({n\pi t\over L})+b_nsin({n\pi t\over L})],\ a_n={1\over L}\int_{-L}^{L}f(t)cos({n\pi t\over L})dt, \\b_n={1\over L}\int_{-L}^{L}f(t)sin({n\pi t\over L})dt, n=0,1,\ldots$，对应的复数形式为： 投影注：$e^{ikx}=cos(kx)+isin(kx)\rightarrow (e^{ikx},e^{ikx})=2L, L$为半周期的绝对值，另外根据复函数的向量空间的内积定义为：$(f,g)=\int_{a}^{b}f(x)\overline{g(x)}dx \rightarrow (f(x),e^{ikx})$在周期 $[-\pi,\pi]$ 下为 $\int_{-\pi}^{\pi}f(x)e^{-iks}dx$ 。 关于傅里叶变换的注记Fourier series傅里叶级数和Fourier transformation傅里叶变换是傅里叶分析的主要部分。设$f(t)$周期$T=2L$，则$f(t)$的傅里叶级数展开为$f(t)=\sum\limits_{k=-\infty}^{\infty}c_ke^{\frac{ik\pi}{L}t},c_k={1\over 2L}\int_{-L}^{L}f(t)e^{\frac{-ik\pi}{L}t}dt$ ($c_k$是$f(t)$在$e^{\frac{ik\pi}{L}t}$上的投影)。现在考虑定义在$(-\infty, +\infty)$上的非周期函数$f(t)$，它有傅里叶级数展开形式吗？ 给定$L&gt;0$，定义$f_L(t)=\cases{f(t), |t|&lt;L\\0 , \quad\ |t| \ge L}$。假设$L\rightarrow \infty$时，$f_L(t)$（一致）趋近于$f(t)$。函数 $f_L(t)$ 能被周期延拓，即令$F_L(t)=\cases{f(t), -L&lt; t \le L\\ F_L(t+2L), T=2L}$ 则 $F_L(t)$有傅里叶级数。 当 $-L&lt;t&lt;L,f(t)=f_L(t)=F_L(t)=\sum\limits_{k=-\infty}^{\infty}c_k(L)e^{\frac{ik\pi}{L}t},c_k(L)={1\over 2L}\int_{-L}^{L}f_L(t)e^{\frac{-ik\pi}{L}t}dt$ 因为 $f_L(t)=0, |t|&gt;L \rightarrow c_k(L)={1\over 2L}\int_{-L}^{L}f_L(t)e^{\frac{-ik\pi}{L}t}dt={1\over 2L}\int_{-\infty}^{\infty}f_L(t)e^{\frac{-ik\pi}{L}t}dt$ 由于 $k\rightarrow\infty$ 同时 $L\rightarrow \infty$ ，所以等式右边的指数项未知，因此做变量代换，令 $\tilde{f}_L(w)=\int_{-\infty}^{\infty}f_L(t)e^{-iwt}dt$，令 $w_k={k\pi\over L}$ 则$c_k(L)={1\over 2L}\tilde{f}(\frac{k\pi}{L})={1\over 2L}\tilde{f}({w_k})={1\over 2\pi}\tilde{f}({w_k})(w_{k+1}-w_k)$ 那么得到傅里叶展开的新形式：$f_L(t)=F_L(t)={1\over 2\pi}\sum\limits_{-\infty}^{+\infty}\tilde{f}_L(w_k)e^{iw_kt}\Delta w_k, \tilde{f}_L(w)=\int_{-\infty}^{+\infty}f_L(t)e^{-iw_kt}dt, \Delta w_k=w_{k+1}-w_{k}={\pi\over L}$。当$L\rightarrow +\infty, \Delta w\rightarrow 0$，等式左边$f_L(t)$（一致）趋近于$f(t)$，右边就趋近于一个积分形式：$ f(t)={1\over 2\pi}\int_{-\infty}^{+\infty}\tilde{f}_L(w)e^{iwt}dw$， 称$\tilde{f}(w)$是$f(t)$的傅里叶变换，$f(t)$是$\tilde{f}(w$)的逆傅里叶变换。 $f(t)$实际上是关于时间函数的$sin\ cos$之间叠加出来的，那么$\tilde{f}(ω)$是关于这些频率叠加出来的，它是频率的函数。讲复矩阵的时候将会回到这个傅里叶变换，会考虑傅里叶变换的离散形式，那么$f(x)$或者$f(t)$就被一个向量替换，$\tilde{f}(ω)$也被一个向量替换，它们之间互逆的这种傅里叶变换或者逆傅里叶变换的关系，实际上就是通过一个傅里叶矩阵进行互相转换的，以及相应的快速的傅里叶变换。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[positive definite and least squares]]></title>
    <url>%2F2017%2F08%2F01%2Fpositive_definite_and_least_square%2F</url>
    <content type="text"><![CDATA[positive definiteWhen a symmetric matrix $A$ has one of these five properties, it has them all and $A$ is positive definite: all n eigenvalue are positive. all n principal minors(n upper left determinants) are positive. all n pivots are positive. $x^{T}Ax$ is positive except when $x = 0$ (this is usually the definition of positive definiteness and the energy-based definition). $A$ equals $R^{T}R$ for a matrix $R$ with independent columns. Let us prove the fifth rule. If $A = R^{T}R$, then $$\begin{eqnarray}x^{T}Ax&amp;=&amp;x^{T}R^{T}Rx \nonumber\\&amp;=&amp;(x^{T}R^{T})Rx \nonumber\\&amp;=&amp;(Rx)^{T}Rx \nonumber\\&amp;=&amp;|Rx| \nonumber\\&amp;\ge&amp;0 \nonumber\end{eqnarray}$$ And the columns of $R$ are also independent, so $|Rx|=x^{T}Ax&gt;0$, except when $x$=0 and thus $A$ is positive definite. $A^{T}A$$A_{m\times n}$ is almost certainly not symmetric, but $A^{T}A$ is square (n by n) and symmetric. We can easily get the following equations through left multiplying $A^{T}A$ by $x^{T}$ and right multiplying $A^{T}A$ by $x$: $$\begin{eqnarray}x^{T}A{^TA}x&amp;=&amp;x^{T}(A{^TA})x\nonumber\\&amp;=&amp;(x^{T}A^{T})Ax\nonumber\\&amp;=&amp;(Ax)^{T}(Ax)\nonumber\\&amp;=&amp;|Ax|\nonumber\\&amp;\ge&amp;0\nonumber\end{eqnarray}$$ If $A_{m\times\,n}$ has rank $n$ (independent columns), then except when $x = 0$, $Ax=|Ax|=x^{T}(A{^TA})x&gt;0$ and thus $A^{T}A$ is positive definite. And vice versus. Besides, $A^{T}A$ is invertible only if $A$ has rank $n$ (independent columns). To prove this, we assume $Ax=0$, then: $$\begin{eqnarray}Ax&amp;=&amp;0\nonumber\\(Ax)^{T}(Ax)&amp;=&amp;0\nonumber\\(x^{T}A{^T})(Ax)&amp;=&amp;0\nonumber\\x^{T}A{^T}(Ax)&amp;=&amp;x^{T}0\nonumber\\(A{^TA})x&amp;=&amp;0\nonumber\end{eqnarray}$$ From the above equations, we know solutions of $Ax=0$ are also solutions of $(A{^TA})x=0$. Because $A_{m\times\,n}$ has a full set of column rank (independent columns), $Ax=0$ only has a zero solution as well as $(A{^T}A)x=0$. Furthermore, if $A{^T}A$ is invertible, then $A_{m\times\,n}$ has rank $n$ (independent columns). We also notice that if $A$ is square and invertible, then $A{^T}A$ is invertible. Overall, if all columns of $A_{m\times\,n}$ are mutual independent, then $(A{^T}A)$ is invertible and positive definite as well, and vice versus. least squareWe have learned that least square comes from projection :$$b-p=e\Rightarrow\,A^{T}(b-A\hat{x})=0\Rightarrow\,A^{T}A\hat{x}=A^{T}b$$Consequently, only if $A^{T}A$ is invertible, then we can use linear regression to find approximate solutions $\hat{x}=(A^{T}A)^{-1}A^{T}b$ to unsolvable systems of linear equations. According to the reasoning before, we know as long as all columns of $A_{m\times\,n}$ are mutual independent, then $A{^T}A$ is invertible. At the same time we ought to notice that the columns of $A$ are guaranteed to be independent if they are orthoganal and even orthonormal. In another prospective, if $A^{T}A$ is positive definite, then $A_{m\times\,n}$ has rank $n$ (independent columns) and thus $A^{T}A$ is invertible. Overall, if $A^{T}A$ is positive definite or invertible, then we can find approximate solutions of least square.]]></content>
      <categories>
        <category>English</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[正定矩阵]]></title>
    <url>%2F2017%2F08%2F01%2Fpositive_definite_matrix%2F</url>
    <content type="text"><![CDATA[笔记源自：清华大学公开课：线性代数2——第一讲：正定矩阵 引言 矩阵特征值的正负在求解微分方程和差分方程时，会影响解是否收敛，例如上图如果$\lambda_i &lt; 0$那么$e^{\lambda_i t}$ 随着$t\rightarrow \infty, e^{\lambda_it}\rightarrow0$ 主子式 实对称矩阵A正定的充要条件下列6项条件，满足任意一项即可判定实对称矩阵$A$为正定矩阵： 证明$(1)\Rightarrow(2):$ 对实对称矩阵$A$，那么存在正交阵$Q$，使得$AQ=Q\Lambda \rightarrow A=Q\Lambda Q^T$，其中$\Lambda=diag(\lambda_1,\,…\,,\lambda_n)$。于是对于任意非零向量$x$，有$x^TAx=x^TQ\Lambda Q^Tx=y^T \Lambda y=\lambda_1 {y_1}^2+\,…\,+\lambda_n {y_n}^2&gt;0, y=Q^Tx=(y_1,\,…\,,y_n) \ne\vec{0}$ $(2)\Rightarrow(1):​$ 设$Ax=\lambda x(x\ne0)​$ 则$0&lt;x^TAx=x^T\lambda x=\lambda||x||^2​$，因此所有$\lambda_i&gt;0​$。 $(2)\Rightarrow(3):$ 由于行列式等于矩阵特征值的乘积，故$(2)\Rightarrow(1)\Rightarrow (3)det A=\lambda_1\,…\,\lambda_n&gt;0$ ： $(2)\, 0&lt;\begin{pmatrix}x_k^T&amp;0\end{pmatrix} \begin{pmatrix}A_k&amp;*\\*&amp;*\end{pmatrix}\begin{pmatrix}x_k\\0\end{pmatrix}={x_k}^T A_k x_k = {x_k}^T \begin{pmatrix} \lambda_1&amp;\\&amp;\ddots\\&amp;&amp;\lambda_k \end{pmatrix} x,\, (1 \le k \le n) \\\Rightarrow (1) \lambda_i &gt; 0,(1\le i \le k, 1 \le k \le n) \Rightarrow (3) detA_k&gt;0, (1 \le k \le n)$ $(3)\Rightarrow(4)$：顺序主子式与主元有直接联系，因为第k个主元$d_k={det A_k \over det A_{k-1}}$，所以$(3) \Rightarrow (4)\,d_k &gt; 0$，其中$A_k$是第$k$个顺序主子矩阵（the k-th leading principal sub-matrix）。 $(4) \Rightarrow (2)$：由对称矩阵的Gauss消元法得$A=LDL^T$且对角阵$D=diag(d_1,\,…\,d_n)$ 的对角元为A的主元，$L$是下三角矩阵，$L^T$ 是上三角矩阵，而且根据分解结果知道$L$的主对角线上全元素为1，也即$L^T$的主元全为1，即$L^T$行列式为1且是方阵，那么这俩都可逆。因为$(4):d_1,\,…\,,d_n$大于0，那么到：$x\ne 0\Rightarrow y=L^Tx\ne 0\Rightarrow x^TAx=x^TLDL^Tx=y^TDy=d_1y_1^2+…+d_ny_n^2&gt;0$ 。 可逆矩阵齐次方程只有零解 $(2)\Rightarrow(5)$：$A=LDL^T=L\sqrt{D}\sqrt{D}L^T=(\sqrt{D}L^T)^T(\sqrt{D}L^T)$，此时可取$R=\sqrt{D}L^T$，因为$\sqrt{D}, L^T$ 都可逆且都是方阵，由于$(2)\Rightarrow(3)\Rightarrow(4)$ ，因此$\sqrt{D}&gt;0$，且有上面推导得$|L^T|&gt;0$， 可逆矩阵乘积还是可逆。 根据行列式性质：$ |A||B|=|AB|$, 当$A,B$ 均可逆，那么$|A|&gt;0, |B|&gt;0 \rightarrow |AB|&gt;0$, 所以$AB$也可逆。 或者：$A=Q\Lambda Q^T=Q\sqrt{\Lambda}\sqrt{\Lambda}Q^T=(\sqrt{\Lambda}Q^T)(\sqrt{\Lambda}Q^T)$，此时可取 $R=\sqrt{\Lambda}Q^T$ ，同理可得。 $(5)\Rightarrow(2)$：$A=R^TR\Rightarrow x^TAx=x^TR^TRx=(Rx)^TRx=||Rx||^2 \ge 0$且$R$是列满秩，除了$x=0$之外，其余 $x^TAx=||Rx||^2 &gt; 0$，即$(5)\Rightarrow(2)$ $(6)\Leftarrow\Rightarrow(2)$: 典型例子 正定矩阵的性质如果$A,B$是正定矩阵，那么$A+B$也是正定矩阵 如果$A$为正定矩阵，则存在矩阵$C$，满足$A=C^2$ 如果$A$为正定矩阵，则矩阵$A$的幂也是正定的 如果$A$为正定矩阵，矩阵$C$，那么$B=C^TAC$也是正定的 注：其实B称为A的合同矩阵 半正定矩阵的判别条件 二次型定义 注意：这里证明里面 ${A-A^T\over 2}$ 是反对称矩阵，利用反对称矩阵性质，所以 $x^T{A-A^T\over 2}x=0$ 。二次型与判定正定矩阵的第二条准则密切相关。 例子 对角形 二次型化成对角形 注：由于实对称矩阵$A$可以与二次型一一对应，因此，可以借助实对称矩阵研究二次型。 主轴定理principal axis theorem 有心二次型central_conic 三维空间中的二次曲面-6类基本的二次曲面$R^3$种的二次曲面的方程形如:$a_{11}x^2+a_{22}y^2+a_{33}z^2+2a_{12}xy+2a_{13}xz+2a_{23}yz+b_{1}x+b_{2}y+b_{3}z+c=0$. 注：由于二次型可以与实对称对称矩阵一一对应，二次型里面又包括二次曲面，所以实对称矩阵可以跟二次曲面对应起来。 二次型的分类 二次型与特征值 二次型的一个应用——求二次型的几何形状 把二次型的部分去化成对角形的标准型，相应的这个一次项也作了变换，于是再做配方然后去跟基本的形状做比较得出这个曲面的几何形状，这是二次型的一个应用。 合同congruent前言 注：非退化矩阵即满秩矩阵 定义 例子 主轴定理与合同 合同的性质 证明:矩阵$A$左乘可逆矩阵$C^T$相当于做初等行变换，右乘以可逆矩阵$C$相当于做初等列变换，因此根据消元法知道并不改变矩阵$A$的秩。对称性保持证明在于二次型定义可以看到。 1.利用初等变换不改变矩阵的秩，因为可逆矩阵可以表示为初等矩阵的乘积，而A乘初等矩阵相当于对A作初等变换，所以A的秩不变-。这个方法包括了可逆矩阵左乘A，右乘A，或是左右同时乘A 2.利用 r(AB) 惯性定理Sylvester’s law of inertia的证明 惯性定理的应用 正负定矩阵在函数极值中的应用以二元函数$f(x,y)$为例：设$(x_0,y_0)$是二元函数$f(x,y)$的一个稳定点，即：$\frac{\partial f}{\partial x}(x_0,y_0)={\partial{f}\over \partial{y}}(x_0,y_0)=0$。如果$f(x,y)$在$(x_0,y_0)$的领域里有三阶偏导数，则$f(x,y)$在$(x_0,y_0)$可展开成Talor级数： 黑塞Hessian矩阵黑塞矩阵（Hessian Matrix），又译作海森矩阵、海瑟矩阵、海塞矩阵等，是一个多元函数的二阶偏导数构成的方阵，描述了函数的局部曲率。黑塞矩阵最早于19世纪由德国数学家Ludwig Otto Hesse提出，并以其名字命名。黑塞矩阵常用于牛顿法解决优化问题，利用黑塞矩阵可判定多元函数的极值问题。在工程实际问题的优化设计中，所列的目标函数往往很复杂，为了使问题简化，常常将目标函数在某点邻域展开成泰勒多项式来逼近原函数，此时函数在某点泰勒展开式的矩阵形式中会涉及到黑塞矩阵。]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linear_algebra</tag>
        <tag>线性代数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vim无插件使用]]></title>
    <url>%2F2016%2F01%2F20%2Fvim_without_widgets%2F</url>
    <content type="text"><![CDATA[本文根据使用经验，会持续更新。 vim的四种模式 一般模式：normal模式。可以移动光标，删除字符或整行，也可复制、粘贴文件数据。打开vim就是进入这个模式，3个模式的切换也是在这里中转。 编辑模式：一般模式下按下i I o O a A r R s S 任何一个进入该模式。可以编辑文件内容，按Esc回到一般模式。 i I是insert（在光标所在字符前和行首） o O是open新行（在光标所在行的下面另起一新行和在光标所在行的上面另起一行开始插入 a A是append（在在光标所在字符后和在光标所在你行的行尾） s S 是删除（光标所在的字符并开始插入和光标所在行并开始插入），即substitute替换。 r R是replace光标所在的字符和变成替换模式 命令行模式：一般模式下按下: / ？任何一个进入该模式（下文会介绍这些符号的含义）。可以查找数据操作，读取、保存、大量替换字符、离开vim、显示行号等操作，按Esc回到一般模式。 可视模式：一般模式下按下v V ctr+v 进入可视模式，相当于高亮选取文本后的普通模式，即在该模式下进行任意选择特定区域且被选择的区域高亮显示，v选择单位：一个字符； V 又称为可视行模式，选择单位：行；ctr+v又称为可视块模式，选择的单位：方块；这三者都有用，详细看下文。 移动normal模式下： w → 到下一个单词的开头 e → 到下一个单词的结尾 （单词默认是以空格分隔的）W → 到下一个字符串的开头 E → 到下一个字符串的结尾 (字符串指的是数字、字母、下划线组成的字符串)B → 到前一个字符串的首字符上 b → “命令则将光标移动到前一个word的首字符上。 默认上来说，一个单词由字母，数字和下划线组成 如果你认为单词是由blank字符分隔符，那么你需要使用大写的E和W（陈皓: 注） 0 → 数字零，到行头^ → 到本行第一个不是blank字符的位置（所谓blank字符就是空格，tab，换行，回车等）$ → 到本行行尾g_ → 到本行最后一个不是blank字符的位置% → 到光标所在这对括号的另外一个gg → 首行G → 最后一行h j k l (强例推荐使用其移动光标，但不必需) →你也可以使用光标键 (←↓↑→). 注: j 向下伸，k是向上伸 高频使用场景1： 修改行中某个变量名 先移把光标移动：w and b 、W and B （或者如果本行太长可用下文的搜索功能）到目的单词 高频使用场景2： 修改缩进，跳到行头^ 高频使用场景3： 查看函数或类的完整或者变量作用域% 高频使用场景4： 切分屏幕之后，跳转不同窗口：ctrl+w+(h or j or k or l) 高频使用场景5： 左下上右移动（h、j、k、l） 高频使用场景6： 删除到末尾:d$ 删除到开头: d^ 标记简记：标记是为了更好地查找，normal模式下： mx mean: mark x, x is mark name;&#39;x mean: go to the position of x mark 高频使用场景1： 在函数中看到调用其他函数，你想去看怎么定义的，你看完之后要回来，那么先标记一下，然后在跳回来。 语法相关的跳转normal模式下： gd 意思： go to definition 先按 [ 再按 ctrl+d 跳转到#define处 语言支持不太良好 先按 [ 再按 ctrl+i 跳转到函数、变量和#define 语言支持不太良好 快速翻页normal模式下： 伙伴1 伙伴2 ctr + d page down ctr + u page up ctr + f page former ctr + b page back 动作操作指令normal模式下： 伙伴1 伙伴2 d delete a character and copy to clipboard D 从光标所在位置一直删除到行尾 y copy to clipboard Y 复制一行(=yy) s substitue a character S 替换光标所在行 r replace a character R 不常用，表示进入替换模式 c change a character C 不常用，表示修改光标所在位置一直到行尾，与S呈现效果一样 p paste after the cursor P 黏贴在光标位置之前（如果是黏贴一整行，则黏贴到上一行） u undo a operation U 一次性撤销对一整行的所有操作 x cut a character X 不常用， 向左剪切，即退格：删除光标的左边那个字符 * 向下搜索当前光标所在的单词，找到就跳到下一个单词 # 向上搜索当前光标所在的单词，找到就跳到上一个单词 /word 向下全文搜索单词word，跳到匹配的第一个单词，如果多个，继续向下查找按n键（顺着命令本来方向），向上找按N键。 ?word 向上全文搜索单词word，跳到匹配的第一个单词，如果多个，继续向上查找按n键（顺着命令本来方向），向下找按N键。 a append after the cursor A是附加在光标所在行的行尾） i insert before the cursor I插入在光标所在行的行首 o 在光标所在行的下面另起一新行，open the new world？ O在光标所在行的上面另起一行开始插入 v 进入visual模式，用来选择区域（可跨行），用来配合后续的其他操作（增删改查） v 进入visual行模式，用来选择一些行，用来配合后续的其他操作（增删改查） f find a character after the cursor F 向光标位置之前查找一个字符 t till a character tx和fx相同，区别是跳到字符x前 T Tx 和Fx相同，区别是跳到字符x后 单独成型. 重复刚才的操作~ 转换大小写 可以对变量首字母改变大小写 可以结合下文提供的命令的选择一个字符串（变量），然后再改变整个字符串（变量）的大小写。比如：宏定义 = 自动格式化 对当前行用== （连按=两次）, 或对多行用n==（n是自然数）表示自动缩进从当前行起的下面n行 或者进入可视行模式选择一些行后再=进行格式化，相当于一般IDE里的code format。 使用gg=G可对整篇代码进行排版。 撤销和恢复 u undo撤销上一步的操作，命令可以组合，例如Nu N是任意一个整数，表示撤销N步操作，以下类同。 U 恢复当前行（即一次撤销对当前行的全部操作） ctr+r control+redo 恢复上一步被撤销的操作 CTRL-R 回退前一个命令 文本替换normal 模式下输入替换命令： :[range]s/pattern/string/[flags] pattern 就是要被替換掉的字串，可以用 regexp 來表示。 string 將 pattern 由 string 所取代。 [range] 有以下一些取值： [range] 含义 无 默认为光标所在的行 . 光标所在当前的行 N 第N行 $ 最后一行 &#39;a 标记a所在的行（之前要使用ma做过标记） .+1 当前光标所在行的下面一行 $-1 倒数第二行，可以对某一行加减某个数值来确定取得相对的行 22,33 第22～33行 1,$ 第1行 到 最后一行 1,. 第1行 到 当前行 .,$ 当前行 到 最后一行 &#39;a,&#39;b 标记a所在的行 到 标记b所在的行（之前要使用ma和mb做过标记） % 所有行（与 1,$ 等价） ?str? 从当前位置向上搜索，找到的第一个str所在的行 （其中str可以是任何字符串或者正则表达式） /str/ 从当前位置向下搜索，找到的第一个str所在的行（其中str可以是任何字符串或者正则表达式） 注意，上面的所有用于range的表示方法都可以通过 +、- 操作来设置相对偏移量。 [flags]有以下一些取值： flags 含义 g 对指定范围内的所有匹配项（global）进行替换 c 在替换前请求用户确认（confirm） e 忽略执行过程中的错误 i ignore 不分大小写 无 只对指定范围内的第一个匹配项进行替换 注意：上面的所有flags都可以组合起来使用，比如 gc 表示对指定范围内的 所有匹配项进行替换，并且在每一次替换之前都会请用户确认。 例子替换某些行的内容 :10,20s/from/to/g 对第10行到第20行的内容进行替换。 :1,$s/from/to/g 对第一行到最后一行的内容进行替换（即全部文本） :1,.s/from/to/g 对第一行到当前行的内容进行替换。 :.,$s/from/to/g 对当前行到最后一行的内容进行替换。 :&#39;a,&#39;bs/from/to/g 对标记a和b之间的行（含a和b所在的行）进行替换，其中a和b是之前用m命令所做的标记。 替换所有行的内容：:%s/from/to/g 动作的重复normal模式下，任意一个动作都可以重复 注：N是数字 数字：Nyy从当前行算起向下拷贝N行、Ndd从当前行算起向下删除N行、Ngg跳到第N行、dNw删除从当前光标开始到第N个单词前（不包含空白，即删除N-1个单词)、yNe拷贝从当前光标到第N个单词末尾（注意： yy=1yy dd=1dd）、d$删除到本行末尾 重复前一个命令： .N （N表示重复的次数） 区块选择注：中括号内容为可选项 normal模式下：[ctr + ] v + (h or j or k or l) 高频使用场景1: [ctr + ] v 选中某些行的行头之后 再按= 效果：代码格式自动调整 高频使用场景2: [ctr + ] v 选中某些行的行头之后 再按I再按注释的符号（比如：//）最后按ESC 效果：选中的这些行全部注释了 多行快速注释 高频使用场景3: [ctr + ] v 选中某些行的行头之后 再按A再按注释的内容 最后按 ESC（比如：//这是测试代码） 效果：选中的这些行的行尾全部注释上//这是测试代码 多行快速注释 高频使用场景4: [ctr + ] v 选中某些行的行头的注释（比如：//）之后 再按d 最后按ESC 效果：选中的这些行全部注释删除了 多行快速删除注释 高频使用场景5: [ctr + ] v 选中某些区块之后，再按上文动作的按键实现区域操作 组合的强大操作光标所在的一个单词normal模式下： 动作 + 移动 [+重复次数]前面已经已经大量使用组合，这里继续： 动作操作指令+范围 效果 cw or c1 or c1w change from current cursor to word end caw change whole word including current cursor dw or d1 or d1w delete from current cursor to word end daw delete whole word including current cursor yw or y1 or y1w copy from current cursor to word end yaw copy whole word including current cursor dtword delete until before the next ‘word’ dfword delete until after the next ‘word’ 范围+动作操作指令 效果 bve 或 BvE + c/d/y 操作一个变量或字符串 上表都是高频使用场景 自动补全在insert模式下直接按： 最常用的补全 12ctrl + n ctrl + p 智能补全 1ctrl + x //进入补全模式 整行补全 CTRL-X CTRL-L 根据当前文件里关键字补全 CTRL-X CTRL-N 根据字典补全 CTRL-X CTRL-K 根据同义词字典补全 CTRL-X CTRL-T 根据头文件内关键字补全 CTRL-X CTRL-I 根据标签补全 CTRL-X CTRL-] 补全文件名 CTRL-X CTRL-F 补全宏定义 CTRL-X CTRL-D 补全vim命令 CTRL-X CTRL-V 用户自定义补全方式 CTRL-X CTRL-U 拼写建议 CTRL-X CTRL-S //例如：一个英文单词 折叠normal模式下： 123zo (折+open)zi (折+indent)zc (折+close) 切分屏幕切分命令，normal模式下，输入 vs(说明：vertically split 纵向切分屏幕） sp(说明：split 横向切分屏幕，即默认的切分方式） 屏幕相互跳转 ctr + w 再按 h或j或k或l 解释：h: left , j : down , k : up, l : right 调整切分窗口的大小 ctrl+w 在按 + 或 - 或 = ，当然在按 + 或 - 或 = 之前先按一个数字，改变窗口高度，= 是均分的意思。。 在normal模式下 输入：resize -N 或 :resize +N 明确指定窗口减少或增加N行 ctrl+w 在按 &lt; 或 &gt; 或 = ，当然在按 &lt; 或 &gt; 或 = 之前先按一个数字，改变窗口宽度，= 是均分的意思。 有时候预览大文件，感觉切分的屏幕太小，ctrl+w + T 移动当前窗口至新的标签页。 tab窗口vim 从 vim7 开始加入了多标签切换的功能， 相当于多窗口. 之前的版本虽然也有多文件编辑功能， 但是总之不如这个方便啦。 用法normal模式下： :tabnew [++opt选项] ［＋cmd］ 文件 建立对指定文件新的tab :tabc 关闭当前的tab or :q :tabo 关闭其他的tab :tabs 查看所有打开的tab :tabp 前一个previous tab window :tabn 后一个next tab window 标准模式下： gt , gT 可以直接在tab之间切换。 还有很多他命令， :help table 吧。 目录normal模式下： :Te 以tab窗口形式显示当前目录 然后可进行切换目录、打开某个文件 :!ls 这种是vim调用shell命令的方式:!ls + shell_command,但不是以tab窗口的形式显示当前目录。 成对符号的内容操作以下命令可以对标点内的内容进行操作： ci&#39; ci&quot; ci( ci[ ci{ ci&lt; 分别change这些配对标点符号中的文本内容 di&#39; di&quot; di(或dib di[ di{或diB di&lt; 分别删除这些配对标点符号中的文本内容 yi&#39; yi&quot; yi( yi[ yi{ yi&lt; 分别复制这些配对标点符号中的文本内容 vi&#39; vi&quot; vi( vi[ vi{ vi&lt; 分别选中这些配对标点符号中的文本内容 cit dit yit vit 分别操作一对标签之间的内容，编辑html很好用 另外如果把上面的 i 改成 a 可以同时操作配对标点和配对标点内的内容，举个例子： 比如要操作的文本：111”222”333，将光标移到”222”的任何一个字符处输入命令 di” ,文本会变成： 111””333 若输入命令 da” ,文本会变成： 111333 剪贴板1. 简单复制和粘贴vim提供12个剪贴板，它们的名字分别为vim有11个粘贴板，分别是0、1、2、…、9、a、“。如果开启了系统剪贴板，则会另外多出两个+和*。使用:reg命令，可以查看各个粘贴板里的内容。 在vim中简单用y 只是复制到 &quot; 的粘贴板里，同样用p 粘贴的也是这个粘贴板里的内容。 2. 复制和粘贴到指定剪贴板要将vim的内容复制到某个粘贴板，进入正常模式后，选择要复制的内容，然后按 &quot;Ny 完成复制，其中N为粘贴板号（注意是按一下双引号然后按粘贴板号最后按y），例如要把内容复制到粘贴板a，选中内容后按”ay就可以了。 要将vim某个粘贴板里的内容粘贴进来，需要退出编辑模式，在正常模式按&quot;Np，其中N为粘贴板号。比如，可以按&quot;5p将5号粘贴板里的内容粘贴进来，也可以按&quot;+p将系统全局粘贴板里的内容粘贴进来。 3. 系统剪贴板查看vim支持的剪切板，normal模式下输入：reg 和系统剪贴板的交互又应该怎么用呢？遇到问题一般第一个寻找的是帮助文档，剪切板即是 Clipboard。通过:h clipboard 查看帮助 星号和加号+粘贴板是系统粘贴板。在windows系统下， 和 + 剪贴板是相同的。对于 X11 系统， 剪贴板存放选中或者高亮的内容， + 剪贴板存放复制或剪贴的内容。打开clipboard选项，可以访问 + 剪贴板；打开xterm_clipboard，可以访问 剪贴板。 * 剪贴板的一个作用是，在vim的一个窗口选中的内容，可以在vim的另一个窗口取出。 复制到系统剪贴板 example： &quot;*y &quot;+y &quot;+Nyy 复制N行到系统剪切板 解释： 命令 含义 {Visual}”+y copy the selected text into the system clipboard “+y{motion} copy the text specified by {motion} into the system clipboard :[range]yank+ copy the text specified by [range] into the system clipboard 剪切到系统剪贴板 example： “+dd 从系统剪贴板粘贴到vim normal模式下： &quot;*p &quot;+p :put+ 含义： Ex command puts contents of system clipboard on a new line 插入模式下： &lt;C-r&gt;+ 含义： From insert mode (or commandline mode) “+p比 Ctrl-v 命令更好，它可以更快更可靠地处理大块文本的粘贴，也能够避免粘贴大量文本时，发生每行行首的自动缩进累积，因为Ctrl-v是通过系统缓存的stream处理，一行一行地处理粘贴的文本。 vim编码Vim 可以很好的编辑各种字符编码的文件，这当然包括UCS-2、UTF-8 等流行的 Unicode 编码方式。 四个字符编码选项，encoding、fileencoding、fileencodings、termencoding (这些选项可能的取值请参考 Vim 在线帮助 :help encoding-names，它们的意义如下: encoding: Vim 内部使用的字符编码方式 包括 Vim 的 buffer (缓冲区)、菜单文本、消息文本等。默认是根据你的locale选择.用户手册上建议只在 .vimrc 中改变它的值，事实上似乎也只有在.vimrc 中改变它的值才有意义。你可以用另外一种编码来编辑和保存文件，如你的vim的encoding为utf-8,所编辑的文件采用cp936编码,vim会自动将读入的文件转成utf-8(vim的能读懂的方式），而当你写入文件时,又会自动转回成cp936（文件的保存编码). fileencoding: Vim 中当前编辑的文件的字符编码方式 Vim 保存文件时也会将文件保存为这种字符编码方式 (不管是否新文件都如此)。 fileencodings: Vim会自动探测编码设置项 启动时会按照它所列出的字符编码方式逐一探测即将打开的文件的字符编码方式，并且将 fileencoding 设置为最终探测到的字符编码方式。因此最好将Unicode 编码方式放到这个列表的最前面，将拉丁语系编码方式 latin1 放到最后面。 termencoding: Vim 所工作的终端 (或者 Windows 的 Console 窗口) 的字符编码方式 如果vim所在的term与vim编码相同，则无需设置。如其不然，你可以用vim的termencoding选项将自动转换成term的编码.这个选项在 Windows 下对我们常用的 GUI 模式的 gVim 无效，而对 Console 模式的Vim 而言就是 Windows 控制台的代码页，并且通常我们不需要改变它。 好了，解释完了这一堆容易让新手犯糊涂的参数，我们来看看 Vim 的多字符编码方式支持是如何工作的。 Vim 启动，根据 .vimrc 中设置的 encoding 的值来设置 buffer、菜单文本、消息文的字符编码方式。 读取需要编辑的文件，根据 fileencodings 中列出的字符编码方式逐一探测该文件编码方式。并设置 fileencoding 为探测到的，看起来是正确的 (注1) 字符编码方式。 对比 fileencoding 和 encoding 的值，若不同则调用 iconv 将文件内容转换为encoding 所描述的字符编码方式，并且把转换后的内容放到为此文件开辟的 buffer 里，此时我们就可以开始编辑这个文件了。注意，完成这一步动作需要调用外部的 iconv.dll(注2)，你需要保证这个文件存在于 $VIMRUNTIME 或者其他列在 PATH 环境变量中的目录里。 编辑完成后保存文件时，再次对比 fileencoding 和 encoding 的值。若不同，再次调用 iconv 将即将保存的 buffer 中的文本转换为 fileencoding 所描述的字符编码方式，并保存到指定的文件中。同样，这需要调用 iconv.dll由于 Unicode 能够包含几乎所有的语言的字符，而且 Unicode 的 UTF-8 编码方式又是非常具有性价比的编码方式 (空间消耗比 UCS-2 小)，因此建议 encoding 的值设置为utf-8。这么做的另一个理由是 encoding 设置为 utf-8 时，Vim 自动探测文件的编码方式会更准确 (或许这个理由才是主要的 ;)。我们在中文 Windows 里编辑的文件，为了兼顾与其他软件的兼容性，文件编码还是设置为 GB2312/GBK 比较合适，因此 fileencoding 建议设置为 chinese (chinese 是个别名，在 Unix 里表示 gb2312，在 Windows 里表示cp936，也就是 GBK 的代码页)。 对于fedora来说，vim的设置一般放在/etc/vimrc文件中，不过，建议不要修改它。可以修改~/.vimrc文件（默认不存在，可以自己新建一个），写入所希望的设置。 我的.vimrc文件如下: 1234:set encoding=utf-8:set fileencodings=ucs-bom,utf-8,cp936:set fileencoding=gb2312:set termencoding=utf-8 其中，fileencoding配置可以设置utf-8，但是我的mp3好像不支持utf-8编码，所以干脆，我就设置为gb2312了。现在搞定了，不管是vi中还是mp3上都可以显示无乱码的.txt文件了。 个人的配置本人无插件使用过程中的配置很短，写在vim的配置文件.vimrc里， 配置是使用vim script进行配置的，它有自己的一套语法，详细请点击vim Script 1234567891011set number;display numberset mouse=a; setting smart mouseset hlsearch ;high light searchset tabstop=4 ; setting tab width 4 lettersset shiftwidth=4; setting new line incident widthset noexpandtab; tab doesn't expand to space;set list ;display manipulator, example： \n \t \r ......set encoding=utf-8set fileencodings=ucs-bom,utf-8,cp936set fileencoding=gb2312set termencoding=utf-8 前进和后退功能流行的文本编辑器通常都有前进和后退功能，可以在文件中曾经浏览过的位置之间来回移动（联想到浏览器），在 vim 中使用 Ctrl-O 执行后退，使用 Ctrl-I 执行前进，相关帮助： :help CTRL-O :help CTRL-I :help jump-motions vim比较文件启动方法首先保证系统中的diff命令是可用的。Vim的diff模式是依赖于diff命令的。 1vimdiff file1 file2 [file3 [file4]] 或者1vim -d file1 file2 [file3 [file4]] 窗口比较局部于当前标签页中。你不能看到某窗口和别的标签页中的窗口的差异。这样，可以同时打开多组比较窗口，每组差异在单独的标签页中。Vim 将为每个文件打开一个窗口，并且就像使用 -O 参数一样，使用垂直分割。如果你要水平分割，加上 -o 参数:1vimdiff -o file1 file2 [file3 [file4]] 如果已在 Vim 中，你可以用三种方式进入比较模式，只介绍一种：1:diffs[plit] &#123;filename&#125; 对 {filename} 开一个新窗口。当前的和新开的窗口将设定和”vimdiff” 一样的参数。要垂直分割窗口，在前面加上 :vertical 。例如:1:vert diffsplit another_filename 跳转到差异有两条命令可用于在跳转到差异文所在的位置: [c 反向跳转至上一处更改的开始。计数前缀使之重复执行相应次。 ]c 正向跳转至下一个更改的开始。计数前缀使之重复执行相应次。如果不存在光标可以跳转到的更改，将产生错误。 合并比较目的就是合并差异，直接使用以下自带命令或者麻烦的办法：手动从一个窗口拷贝至另一个窗口。 123456789101112131415161718:[range]diffg[et] [bufspec] 用另一个缓冲区来修改当前的缓冲区，消除不同之处。除非只有另外一 个比较模式下的缓冲区， [bufspec] 必须存在并指定那个缓冲区。 如果 [bufspec] 指定的是当前缓冲区，则为空动作。[range] 可以参考下面。:[range]diffpu[t] [bufspec] 用当前缓冲区来修改另一个缓冲区，消除不同之处。[count]do 同 ":diffget"，但没有范围。"o" 表示 "obtain" (不能用 "dg"，因为那可能是 "dgg" 的开始！)。dp 同 ":diffput"，但没有范围。注意 不适用于可视模式。 给出的 [count] 用作 ":diffput" 的 [bufspec] 参数。当没有给定 [range] 时，受影响的仅是当前光标所处位置或其紧上方的差异文本。当指定 [range] 时，Vim 试图仅改动它指定的行。不过，当有被删除的行时，这不总有效。参数 [bufspec] 可以是缓冲区的序号，匹配缓冲区名称或缓冲区名称的一部分的模式。例如: :diffget 使用另一个进入比较模式的缓冲区 :diffget 3 使用 3 号缓冲区 :diffget v2 使用名字同 "v2" 匹配的缓冲区，并进入比较模式(例如，"file.c.v2") 更新比较和撤销修改比较基于缓冲区的内容。因而，如果在载入文件后你做过改动，这些改动也将参加比较。不过，你也许要不时地使用 :diffupdate[!]。因为并非所有的改动的结果都能自动更新。包含! 时，Vim 会检查文件是否被外部改变而需要重新载入。对每个被改变的文件给出提示。 如果希望撤销修改，可以和平常用vim编辑一样，直接进入normal模式下按u但是要注意一定要将光标移动到需要撤销修改的文件窗口中。 上下文的展开和查看比较和合并文件的时候经常需要结合上下文来确定最终要采取的操作。Vimdiff 缺省是会把不同之处上下各 6 行的文本都显示出来以供参考。其他的相同的文本行被自动折叠。如果希望修改缺省的上下文行数，可以这样设置： 1:set diffopt=context:3 多个文件的退出在比较和合并告一段落之后，可以用下列命令对多个文件同时进行操作。 比如同时退出：:qa （quit all） 如果希望保存全部文件：:wa （write all） 或者是两者的合并命令，保存全部文件，然后退出：:wqa （write, then quit all） 如果在退出的时候不希望保存任何操作的结果：:qa! （force to quit all） vimdiff 详细请参考 vim下 :help diff vimdiff doc vim命令行的保存、离开等命令： :w 将编辑的数据写入硬盘文件中。 :w! 若文件属性为“只读”，强制写入该文件。但能否写入还由对该文件的文件权限有关。 :q保存后离开。若为“:wq！”则强制保存后离开。 :w[文件名] 将编辑的数据保存为另一个文件。 :r[文件名] 在编辑的数据中读入另一个文件的内容加到光标所在行后面。 :n1,n2 w[文件名] 将n1行到n2行的内容保存到另一个文件。 :!command 暂时离开vi到命令行模式下执行command的显示结果。 ZZ 若文件未改动，则直接离开；若已改动则保存后离开。 set num/nonum 显示/取消行号。 VIM的宏宏的使用非常强大，前往vim 中，宏的使用 完整版命令本文只提供个人使用过程中积累的高频场景，完整版请点击此处，或查阅 vim manual 玩游戏来熟能生巧用进废退，所以多用才是王道，这里推荐一个游戏：通过键盘输入控制人物角色冒险的游戏，玩游戏的过程中熟悉VIM命令: vim-adventures 参考 官方文档 vim doc 中文 freewater 博客 Thinking In Linux]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[决策树学习]]></title>
    <url>%2F2015%2F05%2F01%2Fdecision_tree%2F</url>
    <content type="text"><![CDATA[决策树学习决策树学习通常包含三个方面：特征选择、决策树生成和决策树剪枝。决策树学习思想主要来源于：Quinlan在1986年提出的ID算法、在1993年提出的C4.5算法和Breiman等人在1984年提出的CART算法。 特征选择为了解释清楚各个数学概念，引入例子 表5.1 贷款申请样本数据表（来自李航《统计方法》） ​ 上表有15个样本数据组成的贷款申请训练数据D。数据包括贷款申请人的4个特征：年龄、有工作与否、有房子与否、信贷情况，其中最后一列类别的意思是：是否同意发放贷款，这个就是决策树最后要给出的结论，即目标属性——是否发放贷款，即决策树最末端的叶子节点只分成2类：同意发放贷款与不同意发放贷款。 信息熵（entropy）​ 引入概念，对于第一个要用到的概念：信息熵在另外一篇博客——数据压缩与信息熵中详细解释了信息熵为什么度量的是不确定性，下文也不再赘述，直接引用。 ​ 设D为按照目标类别（或称目标属性）对训练数据（即样本数据）进行的划分，则D的信息熵（information entropy）表示为：$$info(D)=-\sum\limits_{i=1}^{m}p_ilog_2(p_i)$$ ​ 其中pi表示第i个类别在整个训练数据中出现的概率，可以用属于此类别元素的数量除以训练数据（即样本数据）总数量作为估计。 具体问题具体分析 在上表中目标类别：是否发放贷款，将9个发放归为一类，剩余6个不发放归为一类，这样进行分类的信息熵为： $$H(D)=-\frac{9}{15}log_2\frac{9}{15}-\frac{6}{15}log_2\frac{6}{15}=0.971$$ 注：这个根据目标类别分类得出的信息熵，在样本给出的情况下就已经知晓，根据概率统计，也称经验熵。 ​ 现在我们假设将训练数据D按属性A进行划分，则按A属性进行分裂出的v个子集（即树中的v个分支），这些子集按目标类别（发放与不发放两类）进行分类所对应的熵的期望（即：按属性A划分出不同子集的信息熵的平均值）： $$info_A(D)=\sum\limits_{j=1}^{v}\frac{|D_j|}{|D|}info(D_j)$$ 注：这个实际上是经验条件熵，因为确认是在A属性划分出子集的前提下再按照目标类别分类得出的熵的期望，见下文信息增益计算就可以一目了然。 信息增益（information gain）为上述两者的差值： $$gain(A)=info(D)-info_A(D)$$ 具体问题具体分析 按照年龄属性（记为A1）划分：青年（D1表示），中年（D2表示），老年（D3表示） $$\begin{align}g(D, A_1) &amp;= H(D) - [\frac{5}{15}H(D_1) + \frac{5}{15}H(D_2) + \frac{5}{15}H(D_3)] \\ &amp;= 0.971 - [ \frac{5}{15}(-\frac{2}{5}log_2\frac{2}{5}-\frac{3}{5}log_2\frac{3}{5})+\frac{5}{15}(-\frac{3}{5}log_2\frac{3}{5} - \frac{2}{5}log_2\frac{2}{5}) + \frac{5}{15}(-\frac{4}{5}log_2\frac{4}{5} - \frac{1}{5}log_2\frac{1}{5})] \\ &amp;= 0.971 - 0.888 \\ &amp;= 0.083 \end{align}$$ 按照是否有工作（记为A2）划分：有工作（D1表示），无工作（D2表示） $$\begin{align}g(D, A_2) &amp;= H(D) - [\frac{5}{15}H(D_1) + \frac{5}{15}H(D_2) ] \\ &amp;= 0.971 - [ \frac{5}{15}\times 0+\frac{10}{15}(-\frac{4}{10}log_2\frac{4}{10} - \frac{6}{10}log_2\frac{6}{10})] \\ &amp;= 0.324 \end{align}$$ 按照是否有自己房子（记为A3）划分：有自己房子（D1表示），无自己房子（D2表示） $$\begin{align}g(D, A_3) &amp;= 0.971 - [ \frac{6}{15}\times 0+\frac{9}{15}(-\frac{3}{9}log_2\frac{3}{9} - \frac{6}{9}log_2\frac{6}{9})] \\ &amp;= 0.971 - 0.551 \\ &amp;= 0.420 \end{align}$$ 同理，根据最后一个属性：信贷情况算出其信息增益： $$g(D, A_4) = 0.971 - 0.608 = 0.363$$ 所以可以看出信息增益度量的是：信息熵的降低量，这个降低是经过某个属性对原数据进行划分得出的。信息熵的降低，即确定性的提高，进一步讲，就是类别的数量在下降，那么确定为哪一类的可能性就提高，这样就更容易分类了。ID3算法就是基于信息增益来衡量属性（即特征）划分数据的能力，进而为特征（即属性）选择提供原则。 增益比率（gain ratio） 信息增益选择方法有一个很大的缺陷，它总是会倾向于选择属性值多的属性，如果我们在上面的数据记录中加一个姓名属性，假设15条记录中的每个人姓名不同，那么信息增益就会选择姓名作为最佳属性，因为按姓名分裂后，每个组只包含一条记录，而每个记录只属于一类（要么发放要么不发放），因此不确定性最低，即纯度最高，（注：为什么最高呢？大家可以根据导数计算一下，最大值的情况，这里不赘述）以姓名作为测试分裂的结点下面有15个分支。但是这样的分类没有意义，它没有任何泛化能力。增益比率对此进行了改进，它引入一个分裂信息： $$SplitInfo_R(D)=-\sum\limits_{j=1}^{k}\frac{|D_j|}{D}\times log_2(\frac{|D_j|}{D})$$ 注：分裂信息即按照某个属性划分的信息熵，而本文前面叙述的熵全部是按照目标属性进行分类的信息熵。 增益比率定义为信息增益与分裂信息的比率： $$GainRatio(R)=\frac{Gain(R)}{SplitInfo_R(D)}$$ 我们找GainRatio最大的属性作为最佳分裂属性。如果一个属性的取值很多，那么SplitInfoR(D)会大，从而使GainRatio(R)变小。不过增益比率也有缺点，SplitInfo(D)可能取0，此时没有计算意义；且当SplitInfo(D)趋向于0时，GainRatio(R)的值变得不可信，改进的措施就是在分母加一个平滑，这里加一个所有分裂信息的平均值： $$GainRatio(R)=\frac{Gain(R)}{\overline{SplitInfo(D)}+SplitInfo_R(D)}$$ C4.5算法就是按照信息增益比来计算各属性的分类能力，进而为特征（即属性）选择提供原则。 基尼指数（Gini coefficient） 定义（基尼指数）：在分类问题中，假设有K个类，样本点属于第K类的概率为p(k)，则概率分布的基尼指数定义为 $$[Gini(p)=\sum\limits_{k=1}^{K}p_k(1-p_k)=1-\sum\limits_{k=1}^{K}p_k^2]$$ 对于2分类问题，若样本属于第一类的概率是p，则概率分布的基尼指数为： $$Gini(p)=2p(1-p)$$ 对于给定的样本集合D的基尼指数为： $$Gini(D)=1-\sum\limits_{k=1}^{K}\left(\frac{|C_k|}{|D|}\right)^2$$ 这里，C(k)是D中属于第k类的样本子集，K是类的个数。 如果样本集合D根据特征A是否取某一可能值α被分割成D1和D2两部分，即 $$D_1 = \{(x, y) \in D| A(x)=a\}, D_2=D - D_1$$ 则在特征A的条件下，集合D的基尼指数定义为 $$Gini(D, A) = \frac{|D_1|}{|D|}Gini(D_1) + \frac{|D_2|}{D}Gini(D_2)$$ 基尼指数Gini(D)表示集合D的不确定性，基尼指数Gini(D, A)表示经A=α分割后集合D的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。 ID3算法 信息增益算法（来自李航《统计方法》） ID3算法（来自李航《统计方法》） 示例（来自李航《统计方法》） C4.5生成算法（来自李航《统计方法》） CART生成算法（来自李航《统计方法》） 示例]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Perception Learning Algorithm]]></title>
    <url>%2F2015%2F04%2F24%2Fperception_learning_algorithm%2F</url>
    <content type="text"><![CDATA[PLA(Perception Learning Algorithm) 适用于二维及高维的线性可划分问题。问题的答案只有同意或者不同意。 例子银行可以根据顾客的个人信息来判断是否给顾客发放信用卡。将顾客抽象为一个向量$X$，包括姓名、年龄、年收入、负债数等。同时设定各个属性所占的权重向量为$W$，对于正相关的属性设置相对较高的权重，如年收入，对于负相关的属性设置较低的权重，如负债数。$y$表示是否想该用户发放了信用卡。通过求$X$和$W$的内积减去一个阀值threshold，若为正则同意发放信用卡，否则不发放信用卡。我们假设存在着一个从$X$到$Y$的映射$f$，PLA算法就是用来模拟这个映射，使得求出的函数与$f$尽可能的相似，起码在已知的数据集(即样本上)上一致。 PLA算法即用来求向量$W$，使得在已知的数据中机器做出的判断与现实数据相同。当$X$为二维向量时，相当于在平面上画出一条直线将所有的点分成两部分，一部分同意发送，另一部分的不同意。内积可以表示成：$$\begin{eqnarray}h(x) &amp;=&amp; sign((\sum\limits_{i=1}^{d}W_i X_i)-threshold)\\&amp;=&amp; sign((\sum\limits_{i=1}^{d}W_i X_i)+\underbrace{(-threshold)}_{W_0}\times\underbrace{(+1)}_{X_0})\\&amp;=&amp; sign(\sum\limits_{i=0}^{d}W_i X_i)\\&amp;=&amp; sign(W^TX)\end{eqnarray}$$ 其中$X_0=1，W_0=-threshold$ $y_s$的值域：$\{+1，-1\}$，($y_s$ 表示样本中$y$的值，用于输入到算法进行调整) 结合文中例子：$y_s=1$ 表示在给定的样本数据中，给该用户发放了信用卡，$y_s= -1$表示未发放。 PLA先假定$W_0$为向量$\vec{0}$，然后找到一个不满足条件的点，调整$W$的值，依次进行迭代所有样本数据使得最终可以将两部分完全分开。 W的调整方案错误驱动调整 解释一下ppt的内容，出现错误分2种情况： 在给定的已知数据中向该用户发放了数据，即$y_s(i)$样本中第$i$个数据为$+1$，但算法给出的结果是不发放（$h(X_i) &lt;0$），说明两个向量的内积为负，需要调整$W$向量使得两条向量更接近，此时令调整系数为样本的$y_s(i)$，则调整后的$W_{t+1}= W_t + y_s(i)X_i$，$W$的下标$t, t+1$表示调整的次数，示意图: 在给定的已知数据中向该用户发放了数据，即$y_s(i)$样本中第$i$个数据为$-1$，但算法给出的结果是不发放（$h(X_i) &gt; 0$），说明两个向量的内积为正，需要调整$W$向量使得两条向量更远离，此时令调整系数为样本的$y_s(i)$，则调整后的$W_{t+1}= W_t + y_s(i)X_i$，示意图: 注意：2种不同情况的调整的表达式都一样 对于线性可分的数据集，PLA算法是可收敛的 两个向量的内积增大说明： 两个向量夹角越小 或者向量的长度增大 老师的ppt上 $||W_{t+1}||^2 \le ||W_t||^2 + max\{1 \le i \le n\ \ |\ \ ||y_i X_i||^2\}$ 其中，$y_i$的值域 $\{+1, -1\}$ 因此 $||W_{t+1}||^2 \le ||W_t||^2 + max\{1 \le i \le n\ \ |\ \ ||X_i||^2\}$ 这说明每次调整后，向量的长度增加有限。不妨 带入上一公式得到： $$\begin{eqnarray}\frac{||W_{t+1}||^2}{||W_{t}||^2} &amp;\le&amp; \frac{||W_{t}||^2 + max||y_n x_n||^2}{||W_{t}||^2} \\&amp;=&amp; \frac{||W_{t}||^2 + max||x_n||^2}{||W_{t}||^2} \\&amp;=&amp;1+ \frac{R^2}{||W(t)||^2}\end{eqnarray}$$ 因此$W_t$最终是收敛的，到此已经证明了PLA算法最终可以停止。 算法需要调整的次数由上述过程可以得到以下两个不等式： $$\begin{eqnarray}W_f^t W_t &amp;=&amp; W_f^t(W_{t-1}+y_s(t-1)X_{t-1}) \\&amp;=&amp; W_f^tW_{t-1}+y_s(t-1)W_f^tX(t-1) \\&amp;\ge&amp; W_f^tW_{t-1}+min(yW_f^tX) \\&amp;\ge&amp; W_f^tW_{t-2}+2\,min(yW_f^tX) \\&amp;\cdots&amp; \\&amp;\ge&amp; W_f^tW_0 + t\,min(yW_f^tX) \\&amp;=&amp; t\,min(yW_f^tX)\end{eqnarray}$$ $$\begin{eqnarray}||W_t||^2 &amp;\le&amp; ||W_{t-1}||^2+max(||X||^2)\\&amp;\le&amp; ||W_{t-2}||^2+2\,max(||X||^2)\\&amp;\cdots&amp; \\&amp;\le&amp; ||W_0||^2+t\,max(||X||^2)\\&amp;=&amp; t\, max(||X||^2)\end{eqnarray}$$ 那么来看这个式子：$\frac{W_f^t W_t}{||W_f^t||\ ||W_t||}\ge \frac{t\ min(yW_f^tX)}{||W_f^t||\sqrt{t\,(max(||X||))^2}}=\sqrt{t}\, \frac{min(yW_f^tX)}{||W_f^t||max(||X||)} $ 再根据余弦值最大为1，可以得到$\frac{W_f^tW_t}{||W_f^t||\ ||W_t||}\le 1$，于是我们得到调整次数：$t\le \frac{||W_f^t||(max(||X||))^2}{(min(yW_f^tX))^2}={R^2 \over \rho^2}$. PLA的优缺点 一方面，我事先肯定不知道$W_f^t$，另一方面为了应对可能出现的噪声。那么怎么衡量当前得到的直线能够满足要求呢？我们只能在每一步的时候都判断一下，调整后的$W_{t+1}$是否比上一次的$W_t$能够线性可分更多的数据，于是有了下面的改进算法Pocket PLA，PocketPLA比PLA在调整的时候多做一步：判断当前改正犯的错是否比之前更小，也就是贪心选择。 Pocket PLA 参考 HappyAngel DreamerMonkey ppt全部来自台大《机器学习基石》课堂]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[动态规划-最长公共子序列]]></title>
    <url>%2F2014%2F06%2F01%2Flongest_common_sub-sequence%2F</url>
    <content type="text"><![CDATA[算法总体思想动态规划（Dynamic Programming）是通过组合子问题的解而解决整个问题的。分治是指将问题划分成一些独立的子问题，递归地求解各子问题，然后合并子问题的解而得到原始问题的解，与此不同，动态规划适用于子问题不是独立的情况，也就是各个子问题包含公共的子问题。在这种情况下，采用分治法会做许多不必要的工作，即重复地求解公共地子问题。动态规划算法对每个子问题只求解一次，将其结果保存在一张表中，从而避免每次遇到各个子问题时重新计算答案。 动态规划算法的基本要素最优子结构 矩阵连乘计算次序问题的最优解包含着其子问题的最优解。这种性质称为最优子结构性质。 在分析问题的最优子结构性质时，所用的方法具有普遍性：首先假设由问题的最优解导出的子问题的解不是最优的，然后再设法说明在这个假设下可构造出比原问题最优解更好的解，从而导致矛盾。 利用问题的最优子结构性质，以自底向上的方式递归地从子问题的最优解逐步构造出整个问题的最优解。最优子结构是问题能用动态规划算法求解的前提。 注意：同一个问题可以有多种方式刻划它的最优子结构，有些表示方法的求解速度更快（空间占用小，问题的维度低） 重叠子问题 递归算法求解问题时，每次产生的子问题并不总是新问题，有些子问题被反复计算多次。这种性质称为子问题的重叠性质。 动态规划算法，对每一个子问题只解一次，而后将其解保存在一个表格中，当再次需要解此子问题时，只是简单地用常数时间查看一下结果。 通常不同的子问题个数随问题的大小呈多项式增长。因此用动态规划算法只需要多项式时间，从而获得较高的解题效率。 问题举例最长公共子序列(LCS) 若给定序列$X=\{x_1,x_2,…,x_m\}$，则另一序列$Z=\{z_1,z_2,…,z_k\}$，是 $X$ 的子序列是指存在一个严格递增下标序列$\{i_1,i_2,…,i_k\}$使得对于所有$j=1,2,…,k$ 有：$z_j=x_{i}$。例如，序列 $Z=\{B，C，D，B\}$ 是序列 $X=\{A，B，C，B，D，A，B\}$ 的子序列，相应的递增下标序列为$\{2，3，5，7\}$。 给定2个序列 $X$ 和 $Y$，当另一序列 $Z$ 既是 $X$ 的子序列又是 $Y$ 的子序列时，称 $Z$ 是序列 $X$ 和 $Y$ 的公共子序列。 给定2个序列$X=\{x_1,x_2,…,x_m\}$和 $Y=\{y_1,y_2,…,y_n\}$，找出 $X$ 和 $Y$ 的最长公共子序列。 最长公共子序列的结构(LCS)设序列$X=\{x_1,x_2,…,x_m\}$和 $Y=\{y_1,y_2,…,y_n\}$的最长公共子序列为 $Z=\{z_1,z_2,…,z_k\}$ ，则 若$x_m=y_n$，则$z_k=x_m=y_n$，且 $z_{k-1}$ 是 $\{x_1,\ldots, x_{m-1}\}$ 和 $\{y_1, \ldots, y_{n-1}\}$ 的最长公共子序列。 若 $x_m≠y_n$ 且 $z_k≠x_m, z_k=y_n$，则 $Z$ 是 $\{x_1,\ldots, x_{m-1}\}$ 和 $Y$ 的最长公共子序列。 若 $x_m≠y_n$且 $z_k≠y_n, z_k=x_m$，则 $Z$ 是 $X$ 和 $\{y_1, \ldots, y_{n-1}\}$ 的最长公共子序列。 由此可见，2个序列的最长公共子序列包含了这2个序列的前缀的最长公共子序列。因此，最长公共子序列问题具有最优子结构性质。 LCS时间复杂度求解LCS问题，不能使用暴力搜索方法。一个长度为n的序列拥有 2的n次方个子序列，它的时间复杂度是指数阶，而且还是两个序列求最长公共子序列。 子问题的递归结构由最长公共子序列问题的最优子结构性质建立子问题最优值的递归关系。用$c[i][j]$记录序列和的最长公共子序列的长度。 其中，$X[i]=\{x_1,x_2,…,x_i\}$；$Y[j]=\{y_1,y_2,…,y_j\}$。当 $i=0$ 或 $j=0$ 时，空序列是 $X[i]$ 和 $Y[j]$ 的最长公共子序列。故此时 $c[i][j]=0$ 。其他情况下，由最优子结构性质可建立递归关系如下： $c[i][j]=\cases{0,\quad i=0,j=0 \\ c[i-1][j-1]+1\quad i,j&gt;0;x_i=y_j \\ max\{c[i][j-1],c[i-1][j]\}\quad i,j&gt;0;x_i\ne y_j}$ 计算最优值（伪代码）12345678910111213141516AlgorithmlcsLength(x,y,b)mßx.length-1;nßy.length-1;c[i][0]=0;c[0][i]=0;for(int i= 1; i&lt;= m;i++) for(int j = 1; j &lt;= n; j++) if(x[i]==y[j]) c[i][j]=c[i-1][j-1]+1; b[i][j]=1; else if(c[i-1][j]&gt;=c[i][j-1]) c[i][j]=c[i-1][j]; b[i][j]=2; else c[i][j]=c[i][j-1]; b[i][j]=3; 源代码实现（测试通过）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485#include&lt;iostream&gt;#define LEN_ARR_A 20#define LEN_ARR_B 12using namespace std;/* * brief : calculate longest common substring of two string and * mark path of getting the longest common substring * parameter : lenComStr : length of common substring * solutionPath : mark of path of solution * * note： dynamic programming :caculate longest common substring between X(n) * and Y(m-1),likey to caculate longest common substring between X(n) * and Y(m-1) or between X(n-1) and Y(m). X(0) and Y(0) are not referenced! * * * return : null */template&lt;class Type&gt;void LCSLength(size_t lenStrA, size_t lenStrB, Type *strA, Type *strB, size_t ** lenComStr, size_t ** solutionPath)&#123; for(size_t i = 0; i &lt; lenStrA; ++i) lenComStr[i][0] = 0; for(size_t i = 0; i &lt; lenStrB; ++i) lenComStr[0][i] = 0; for(size_t i = 1; i &lt; lenStrA; ++i) for(size_t j = 1; j &lt; lenStrB; ++j)&#123; if(strA[i] == strB[j])&#123; lenComStr[i][j] = lenComStr[i -1][j - 1] + 1; //global solution depends on local solution solutionPath[i][j] = 1; //mark path of getting the longest common substring &#125; else if(lenComStr[i - 1][j] &gt;= lenComStr[i][j - 1])&#123; lenComStr[i][j] = lenComStr[i - 1][j]; //global solution depends on local solution solutionPath[i][j] = 2; &#125;else&#123; lenComStr[i][j] = lenComStr[i][j - 1]; //global solution depends on local solution solutionPath[i][j] = 3; &#125; &#125;&#125;/* * brief : output longest common substring of two string * * parameter : i is beginning index of char array * j is end index of char array * solutionPath is the path of solution * * note： value of solutionPath[i][j] has 3 states * * return : null */ template&lt;class Type&gt;void LCS(size_t i, size_t j, Type * strA, size_t ** solutionPath)&#123; if(i == -1 || j == -1) return; if(solutionPath[i][j] == 1)&#123; LCS(i - 1, j - 1, strA, solutionPath); cout &lt;&lt; strA[i]; &#125;else if(solutionPath[i][j] == 2) LCS(i - 1, j, strA, solutionPath); else&#123; LCS(i, j - 1, strA, solutionPath); &#125;&#125;int main()&#123; char strA[LEN_ARR_A] = &#123; '0','B', 'D', 'C', 'A', 'B', 'A'&#125;; char strB[LEN_ARR_B] = &#123; '0', 'A', 'B', 'C', 'B', 'D', 'A', 'B'&#125;; size_t ** lenComStr = new size_t*[LEN_ARR_A]; size_t ** solutionPath = new size_t*[LEN_ARR_A]; for(size_t i = 0; i &lt; LEN_ARR_A; ++i) &#123; lenComStr[i] = new size_t[LEN_ARR_B]; solutionPath[i] = new size_t[LEN_ARR_B]; &#125; LCSLength(LEN_ARR_A, LEN_ARR_B, strA, strB, lenComStr, solutionPath); LCS(LEN_ARR_A - 1, LEN_ARR_B - 1, strA, solutionPath); return 0;&#125;]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[含括号的运算表达式求解-栈的基础应用3]]></title>
    <url>%2F2013%2F05%2F13%2Fstack_application_on_expression-resolution-with-braces%2F</url>
    <content type="text"><![CDATA[本文根据严蔚敏老师数据结构（c语言版） 写的程序 如有需要先去看视频 如有错误不当之处，欢迎指出，以免害人害己。 例子Exp = a b + ( c – d / e ) f ​ 前缀式：+ a b - c / d e f ​ 中缀式：a b + c–d / e f ​ 后缀式：a b cd e /-f + 相同点数字都是按原式子排列的：因此操作数就按顺序入栈就好了 不同点1:后缀式中运算符的顺序，正好就是求解的顺序 2:每个运算符和它之前出现且紧靠它的2个操作数构成一个最小表达式 关键：就是由原表达式求得后缀式 应用步骤 Step1： 先设立两个栈，一个运算符栈，另一个后缀式栈。 Step2：在表达式前后头尾加入=号，表示运算表达式开始和结束，因此在运算符中，=号优先级最低。 Step3：若当前字符是操作数，则直接发送给后缀式栈。符合上面提到的共同点：数字按原表达式从左自右的顺序。 Step4：左括号的优先级高于左括号前的运算符，左括号后的运算符优先级高于左括号，这样才能起到隔离的作用，则右括号前的运算符高于右括号，这样才能起到括号隔离内层表达式的作用！ Step5：若当前运算符的优先级高于栈顶的运算符，则进运算符栈，否则退出运算符栈的栈顶运算符与从操作数栈栈顶取出的两个操作数运算结果作为新的操作数压入操作数栈，然后再把当前运算符入运算符栈。 源代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206#include&lt;stdio.h&gt; #include&lt;malloc.h&gt;//malloc()#include&lt;process.h&gt;//exit();// 函数结果状态代码#define TRUE 1#define FALSE 0#define OK 1#define ERROR 0#define INFEASIBLE -1#define STACK_INIT_SIZE 100 // 存储空间初始分配量#define STACKINCREMENT 2 // 存储空间分配增量typedef int Status; // Status是函数的类型,其值是函数结果状态代码，如OK等typedef char SElemType;struct SqStack&#123; SElemType *base; // 在栈构造之前和销毁之后，base的值为NULL SElemType *top; // 栈顶指针 int stacksize; // 当前已分配的存储空间，以元素为单位&#125;;Status InitStack(SqStack &amp;S)&#123; // 构造一个空栈S if(!(S.base=(SElemType *)malloc(STACK_INIT_SIZE*sizeof(SElemType)))) exit(-1); // 存储分配失败 S.top=S.base; S.stacksize=STACK_INIT_SIZE; return OK;&#125;Status DestroyStack(SqStack &amp;S)&#123; // 销毁栈S，S不再存在 free(S.base); S.base=NULL; S.top=NULL; S.stacksize=0; return OK;&#125;Status ClearStack(SqStack &amp;S)&#123; // 把S置为空栈 S.top=S.base; return OK;&#125;Status StackEmpty(SqStack S)&#123; // 若栈S为空栈，则返回TRUE，否则返回FALSE if(S.top==S.base) return TRUE; else return FALSE;&#125;int StackLength(SqStack S)&#123; // 返回S的元素个数，即栈的长度 return S.top-S.base;&#125;Status GetTop(SqStack S,SElemType &amp;e)&#123; // 若栈不空，则用e返回S的栈顶元素，并返回OK；否则返回ERROR if(S.top&gt;S.base) &#123; e=*(S.top-1); return OK; &#125; else return ERROR;&#125;Status Push(SqStack &amp;S,SElemType e)&#123; // 插入元素e为新的栈顶元素 if(S.top-S.base&gt;=S.stacksize) // 栈满，追加存储空间 &#123; S.base=(SElemType *)realloc(S.base,(S.stacksize+STACKINCREMENT)*sizeof(SElemType)); if(!S.base) exit(-1); // 存储分配失败 S.top=S.base+S.stacksize; S.stacksize+=STACKINCREMENT; &#125; *(S.top)++=e; return OK;&#125;Status Pop(SqStack &amp;S,SElemType &amp;e)&#123; // 若栈不空，则删除S的栈顶元素，用e返回其值，并返回OK；否则返回ERROR if(S.top==S.base) return ERROR; e=*--S.top; return OK;&#125;Status StackTraverse(SqStack S,Status(*visit)(SElemType))&#123; // 从栈底到栈顶依次对栈中每个元素调用函数visit()。 // 一旦visit()失败，则操作失败 while(S.top&gt;S.base) visit(*S.base++); printf("\n"); return OK;&#125;SElemType Precede(SElemType a, SElemType b) &#123; //判断运算符优先级 int i, j; char Table[8][8] = &#123; &#123;' ','+','-','*','/','(',')','#'&#125;, &#123;'+','&gt;','&gt;','&lt;','&lt;','&lt;','&gt;','&gt;'&#125;, &#123;'-','&gt;','&gt;','&lt;','&lt;','&lt;','&gt;','&gt;'&#125;, &#123;'*','&gt;','&gt;','&gt;','&gt;','&lt;','&gt;','&gt;'&#125;, &#123;'/','&gt;','&gt;','&gt;','&gt;','&lt;','&gt;','&gt;'&#125;, &#123;'(','&lt;','&lt;','&lt;','&lt;','&lt;','=',' '&#125;, &#123;')','&gt;','&gt;','&gt;','&gt;',' ','&gt;','&gt;'&#125;, &#123;'#','&lt;','&lt;','&lt;','&lt;','&lt;',' ','='&#125; &#125;; //优先级表格 for(i=0; i&lt;8; i++) if(Table[0][i]==a) //寻找运算符a break; for(j=0; j&lt;8; j++) //寻找运算符 if(Table[j][0]==b) break; return Table[j][i];&#125;Status In(SElemType c)&#123; // 判断c是否为运算符 switch(c) &#123; case'+': case'-': case'*': case'/': case'(': case')': case'#':return TRUE; default:return FALSE; &#125;&#125;SElemType Operate(SElemType a,SElemType theta,SElemType b)&#123; SElemType c; a=a-48; b=b-48; switch(theta) &#123; case'+':c=a+b+48; break; case'-':c=a-b+48; break; case'*':c=a*b+48; break; case'/':c=a/b+48; &#125; return c;&#125;SElemType EvaluateExpression() // 算法3.4&#123; // 算术表达式求值的算符优先算法。设OPTR和OPND分别为运算符栈和运算数栈 SqStack OPTR,OPND; SElemType a,b,c,x,theta; InitStack(OPTR); Push(OPTR,'#'); InitStack(OPND); c=getchar(); GetTop(OPTR,x); while(c!='#'||x!='#') &#123; if(In(c)) // 是7种运算符之一 switch(Precede(c,x)) &#123; case'&lt;':Push(OPTR,c); // 栈顶元素优先权低 c=getchar(); break; case'=':Pop(OPTR,x); // 脱括号并接收下一字符 c=getchar(); break; case'&gt;':Pop(OPTR,theta); // 退栈并将运算结果入栈 Pop(OPND,b); Pop(OPND,a); Push(OPND,Operate(a,theta,b)); break; &#125; else if(c&gt;='0'&amp;&amp;c&lt;='9') // c是操作数 &#123; Push(OPND,c); c=getchar(); &#125; else // c是非法字符 &#123; printf("非法字符\n"); exit(-1); &#125; GetTop(OPTR,x); &#125; GetTop(OPND,x); return x;&#125;int main()&#123; printf("请输入算术表达式（中间值及最终结果要在0～9之间），并以#结束\n"); printf("%c\n",EvaluateExpression()); return 0;&#125;]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[行编辑器/容错缓冲区-栈的基础应用2]]></title>
    <url>%2F2013%2F05%2F12%2Fstack_application_on_line-editor_or_buff_zone%2F</url>
    <content type="text"><![CDATA[功能接收用户的从终端输入程序或数据，并存入用户的数据区。由于用户在终端上输入难免出现差错，因此，若在行编辑程序中，“每接收一个字符即存入用户数据区”显然是不合理的。较好的做法，设立一个缓冲区，用以接收用户输入的每一行字符，然后逐行存入用户数据区。允许用户输入出错，并在发现有误时可以及时更正。例如，当用户发现刚刚键入的一个字符是错的时，补进一个退格符#，以表示前一个字符无效；如果发现当前键入的行内差错较多或难以补救，就可以键入一个退行符号@，以表示当前行中的字符均无效。 例子​ 从终端接收这样两行字符： ​ whi##ilr#e(s#*s) ​ outcha@putchar(*s#++) ​ 实际有效的是下列两行： ​ while(*s) ​ putchar(*s++) ​ 为此，我们可以设立一个缓冲区，结构为栈，每当用户从终端接受了一个字符之后现做如下判别：如果他既不是退格符，也不是退行符，则将该字符压入栈中，如果是退格符，则从栈顶删去一个字符；如果是退行符，则将字符栈清空。 源代码1234567891011121314151617181920212223242526272829void LineEdit()&#123; //利用字符栈s，从终端接收一行并送至调用过程的数据区。算法3.2 SqStack s; char ch,c; InitStack(s); printf("请输入一个文本文件,^Z结束输入:\n"); ch=getchar(); while(ch!=EOF) &#123;// EOF为^Z键，全文结束符 while(ch!=EOF&amp;&amp;ch!='\n') &#123; switch(ch) &#123; case '#':Pop(s,c); break; // 仅当栈非空时退栈 case '@':ClearStack(s); break; // 重置s为空栈 default :Push(s,ch); // 有效字符进栈 &#125; ch=getchar(); // 从终端接收下一个字符 &#125; StackTraverse(s,copy); // 将从栈底到栈顶的栈内字符传送至文件 ClearStack(s); // 重置s为空栈 fputc('\n',fp); if(ch!=EOF) ch=getchar(); &#125; DestroyStack(s);&#125;]]></content>
      <categories>
        <category>中文</category>
      </categories>
      <tags>
        <tag>Data Structure</tag>
        <tag>数据结构</tag>
      </tags>
  </entry>
</search>
